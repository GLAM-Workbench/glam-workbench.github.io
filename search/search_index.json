{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the wonderful world of GLAM data! \u00b6 Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand. What is GLAM data? \u00b6 When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets. What can I do with GLAM data? \u00b6 Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data. Do I need to be able to code? \u00b6 No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code. What is Jupyter? \u00b6 ~~More detail required~~ Where do I start? \u00b6 ~~More detail required~~ Other GLAM related notebooks \u00b6 Awesome Jupyter GLAM Getting started with ODate","title":"Home"},{"location":"#welcome-to-the-wonderful-world-of-glam-data","text":"Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand.","title":"Welcome to the wonderful world of GLAM data!"},{"location":"#what-is-glam-data","text":"When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets.","title":"What is GLAM data?"},{"location":"#what-can-i-do-with-glam-data","text":"Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data.","title":"What can I do with GLAM data?"},{"location":"#do-i-need-to-be-able-to-code","text":"No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code.","title":"Do I need to be able to code?"},{"location":"#what-is-jupyter","text":"~~More detail required~~","title":"What is Jupyter?"},{"location":"#where-do-i-start","text":"~~More detail required~~","title":"Where do I start?"},{"location":"#other-glam-related-notebooks","text":"Awesome Jupyter GLAM Getting started with ODate","title":"Other GLAM related notebooks"},{"location":"about/","text":"This is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions. It's aimed at researchers in the humanities, but will include examples and tutorials of more general interest. Over the past decade I've created and shared a wide variety of digital tools, examples, tutorials, and datasets. Some like QueryPic and TroveHarvester are fairly polished and well documented. Others are just fragments of code. All of them are intended to support research into our rich cultural collections. But even though something like the TroveHarvester is pretty easy to use, it does require a bit of set-up, and I've been very aware that this can be a barrier to people starting their explorations. I created the dhistory site many years ago to provide the foundation for a digital workbench, but I couldn't quite achieve what I wanted \u2014 tools that were easy to use and required minimal setup, but also tools that exposed their own workings, that inspired novice users to question and to tinker. So here we are. My plan is to use Jupyter , GitHub, and Binder to bring together all those tools, examples, tutorials, and datasets in a way that supports people's explorations through digital GLAM collections. I'm really excited, for example, that I can create a notebook that provides a deconstructed (or perhaps see-through) version of QueryPic \u2014 that enables you to build, step by step, the same sort of visualisations, while learning about how it works. And at the end you can download the results as a CSV for further analysis. I love the way that Jupyter notebooks combine learning with real, live, digital tools and methods. You don't have to read a tutorial then go away and try to follow the instructions on your own. It's all together. Live code. Real research. Active learning. Like most of my projects this is in itself an experiment. I'm still learning what's possible and what works. But I'm hopeful. If you think this project is worthwhile, you might like to support me on Patreon .","title":"Some background"},{"location":"archway/","text":"Archway \u00b6 Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 Harvesting a records search \u00b6 This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 10,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 10,000 \u2014 the notebook provides some examples of how you might do this. Download from GitHub View using NBViewer Run live on Binder Harvesting functions \u00b6 Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset. Download from GitHub View using NBViewer Run live on Binder Who's responsible? \u00b6 Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode","title":"Archway"},{"location":"archway/#archway","text":"Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Archway"},{"location":"archway/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"archway/#harvesting-a-records-search","text":"This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 10,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 10,000 \u2014 the notebook provides some examples of how you might do this. Download from GitHub View using NBViewer Run live on Binder","title":"Harvesting a records search"},{"location":"archway/#harvesting-functions","text":"Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset. Download from GitHub View using NBViewer Run live on Binder","title":"Harvesting functions"},{"location":"archway/#whos-responsible","text":"Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode","title":"Who's responsible?"},{"location":"csv-explorer/","text":"GLAM CSV Explorer \u00b6 Cultural institutions are making collection data available as machine readable downloads. But how can researchers explore the shape and meaning of this data? How do they know what types of questions they can ask? This notebook provides a quick overview of CSV-formatted data files, particularly those created by GLAM institutions (galleries, libraries, archives, and museums). You can select one of more than 400 GLAM datasets shared on data.gov.au or upload your own. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"GLAM CSV Explorer"},{"location":"csv-explorer/#glam-csv-explorer","text":"Cultural institutions are making collection data available as machine readable downloads. But how can researchers explore the shape and meaning of this data? How do they know what types of questions they can ask? This notebook provides a quick overview of CSV-formatted data files, particularly those created by GLAM institutions (galleries, libraries, archives, and museums). You can select one of more than 400 GLAM datasets shared on data.gov.au or upload your own. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"GLAM CSV Explorer"},{"location":"digitalnz/","text":"DigitalNZ aggregates collections from across New Zealand and makes the aggregated metadata available through an API . You'll need an API key to work with DigitalNZ data. Tips, tools, and examples \u00b6 Getting some top-level data from the DigitalNZ API \u00b6 This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Download from GitHub View using NBViewer Run live on Binder Find results by country in DigitalNZ \u00b6 Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Download from GitHub View using NBViewer Run live on Binder Visualise a search in Papers Past \u00b6 Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Download from GitHub View using NBViewer Run live on Binder Harvest data from Papers Past \u00b6 This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles. Download from GitHub View using NBViewer Run live on Binder","title":"DigitalNZ"},{"location":"digitalnz/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"digitalnz/#getting-some-top-level-data-from-the-digitalnz-api","text":"This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Download from GitHub View using NBViewer Run live on Binder","title":"Getting some top-level data from the DigitalNZ API"},{"location":"digitalnz/#find-results-by-country-in-digitalnz","text":"Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Download from GitHub View using NBViewer Run live on Binder","title":"Find results by country in DigitalNZ"},{"location":"digitalnz/#visualise-a-search-in-papers-past","text":"Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Download from GitHub View using NBViewer Run live on Binder","title":"Visualise a search in Papers Past"},{"location":"digitalnz/#harvest-data-from-papers-past","text":"This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles. Download from GitHub View using NBViewer Run live on Binder","title":"Harvest data from Papers Past"},{"location":"facial-detection/","text":"Finding faces in the Tribune negatives \u00b6 Exploring mages from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Finding faces in the Tribune collection \u00b6 Using OpenCV for basic facial detection. Finding all the faces in the Tribune collection \u00b6 This notebook runs a facial detection script across the whole Tribune collection. It saves cropped versions of all the detected faces, and creates a data file recording the number of faces detected per image. Exploring the face data \u00b6 This notebook plays around with the data generated by running the facial detection script over the whole Tribune collection. Make all the faces into one big image \u00b6 Having extracted lots and lots of faces from the Tribune photos, I thought I'd combine them in one big image. Interesting apps \u00b6 Focus on faces \u00b6 A little app that fades from faces to photos...","title":"Facial detection"},{"location":"facial-detection/#finding-faces-in-the-tribune-negatives","text":"Exploring mages from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Finding faces in the Tribune negatives"},{"location":"facial-detection/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"facial-detection/#finding-faces-in-the-tribune-collection","text":"Using OpenCV for basic facial detection.","title":"Finding faces in the Tribune collection"},{"location":"facial-detection/#finding-all-the-faces-in-the-tribune-collection","text":"This notebook runs a facial detection script across the whole Tribune collection. It saves cropped versions of all the detected faces, and creates a data file recording the number of faces detected per image.","title":"Finding all the faces in the Tribune collection"},{"location":"facial-detection/#exploring-the-face-data","text":"This notebook plays around with the data generated by running the facial detection script over the whole Tribune collection.","title":"Exploring the face data"},{"location":"facial-detection/#make-all-the-faces-into-one-big-image","text":"Having extracted lots and lots of faces from the Tribune photos, I thought I'd combine them in one big image.","title":"Make all the faces into one big image"},{"location":"facial-detection/#interesting-apps","text":"","title":"Interesting apps"},{"location":"facial-detection/#focus-on-faces","text":"A little app that fades from faces to photos...","title":"Focus on faces"},{"location":"getting-started/","text":"Using Jupyter notebooks \u00b6 Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing. Viewing notebooks on NBViewer \u00b6 The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar. Running notebooks live on Binder \u00b6 Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session. Running notebooks in \u2018app mode\u2019 \u00b6 Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required. Running notebooks on your own computer \u00b6 If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Getting started"},{"location":"getting-started/#using-jupyter-notebooks","text":"Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing.","title":"Using Jupyter notebooks"},{"location":"getting-started/#viewing-notebooks-on-nbviewer","text":"The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar.","title":"Viewing notebooks on NBViewer"},{"location":"getting-started/#running-notebooks-live-on-binder","text":"Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session.","title":"Running notebooks live on Binder"},{"location":"getting-started/#running-notebooks-in-app-mode","text":"Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required.","title":"Running notebooks in &lsquo;app mode&rsquo;"},{"location":"getting-started/#running-notebooks-on-your-own-computer","text":"If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Running notebooks on your own computer"},{"location":"glam-data-portals/","text":"GLAM data from government portals \u00b6 This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. You can visualise the contents of the CSV datasets I've harvested by using the GLAM CSV Explorer . Tools, tips, and examples \u00b6 Harvesting GLAM data from government portals \u00b6 This notebook attempts to harvest the details of GLAM datasets from state and national data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. It also attempts some analysis of the results. Harvest GLAM datasets from data.gov.au \u00b6 This is a quick attempt to harvest datasets published by GLAM institutions using the new data.gov.au API. To create the list of organisations, I searched the organisations on the data.gov.au site for 'library', 'archives', 'records', and 'museum'. Data \u00b6 Results (July 2019) \u00b6 Human readable list of all GLAM datasets harvested from data.gov.au GLAM datasets from data.gov.au \u2013 all formats (CSV) GLAM datasets from data.gov.au \u2013 CSVs only (CSV) Datasets by format: CSV 586 XML 81 JSON 74 XLSX 61 DOCX 34 HTML 33 PLAIN 14 ZIP 14 API 9 GEOJSON 8 DATA 6 OTHER 4 PDF 4 KML 3 RSS 2 JPEG 2 RDF 1 HMTL 1 WFS 1 TXT 1 APP 1 JAVASCRIPT 1 CSS 1 WMS 1 PAGE 1 Datasets by organisation: State Library of Queensland 204 Queensland State Archives 172 State Library of Western Australia 147 State Library of South Australia 140 Libraries Tasmania 71 State Records 41 PROV Public Record Office 33 South Australian Museum 33 State Library of New South Wales 21 NSW State Archives 19 History Trust of South Australia 19 State Records Office of Western Australia 7 State Library of NSW 6 Western Australian Museum 6 National Library of Australia 5 State Library of Victoria 5 Australian Museum 4 Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS) 3 National Archives of Australia 3 Museum of Applied Arts and Sciences 3 Mount Gambier Library 2 Tasmanian Museum and Art Gallery 2 National Portrait Gallery 2 Datasets by licence: Creative Commons Attribution 250 Creative Commons Attribution 3.0 Australia 244 Creative Commons Attribution 4.0 237 Creative Commons Attribution 4.0 International 146 Creative Commons Attribution 2.5 Australia 32 Creative Commons Attribution-NonCommercial 10 Other (Open) 5 notspecified 5 Creative Commons Attribution Share-Alike 4.0 3 Creative Commons Attribution 3.0 3 Creative Commons Attribution Non-Commercial 4.0 2 Custom (Other) 1 Results (April 2018) \u00b6 Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Gov data portals"},{"location":"glam-data-portals/#glam-data-from-government-portals","text":"This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. You can visualise the contents of the CSV datasets I've harvested by using the GLAM CSV Explorer .","title":"GLAM data from government portals"},{"location":"glam-data-portals/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"glam-data-portals/#harvesting-glam-data-from-government-portals","text":"This notebook attempts to harvest the details of GLAM datasets from state and national data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. It also attempts some analysis of the results.","title":"Harvesting GLAM data from government portals"},{"location":"glam-data-portals/#harvest-glam-datasets-from-datagovau","text":"This is a quick attempt to harvest datasets published by GLAM institutions using the new data.gov.au API. To create the list of organisations, I searched the organisations on the data.gov.au site for 'library', 'archives', 'records', and 'museum'.","title":"Harvest GLAM datasets from data.gov.au"},{"location":"glam-data-portals/#data","text":"","title":"Data"},{"location":"glam-data-portals/#results-july-2019","text":"Human readable list of all GLAM datasets harvested from data.gov.au GLAM datasets from data.gov.au \u2013 all formats (CSV) GLAM datasets from data.gov.au \u2013 CSVs only (CSV) Datasets by format: CSV 586 XML 81 JSON 74 XLSX 61 DOCX 34 HTML 33 PLAIN 14 ZIP 14 API 9 GEOJSON 8 DATA 6 OTHER 4 PDF 4 KML 3 RSS 2 JPEG 2 RDF 1 HMTL 1 WFS 1 TXT 1 APP 1 JAVASCRIPT 1 CSS 1 WMS 1 PAGE 1 Datasets by organisation: State Library of Queensland 204 Queensland State Archives 172 State Library of Western Australia 147 State Library of South Australia 140 Libraries Tasmania 71 State Records 41 PROV Public Record Office 33 South Australian Museum 33 State Library of New South Wales 21 NSW State Archives 19 History Trust of South Australia 19 State Records Office of Western Australia 7 State Library of NSW 6 Western Australian Museum 6 National Library of Australia 5 State Library of Victoria 5 Australian Museum 4 Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS) 3 National Archives of Australia 3 Museum of Applied Arts and Sciences 3 Mount Gambier Library 2 Tasmanian Museum and Art Gallery 2 National Portrait Gallery 2 Datasets by licence: Creative Commons Attribution 250 Creative Commons Attribution 3.0 Australia 244 Creative Commons Attribution 4.0 237 Creative Commons Attribution 4.0 International 146 Creative Commons Attribution 2.5 Australia 32 Creative Commons Attribution-NonCommercial 10 Other (Open) 5 notspecified 5 Creative Commons Attribution Share-Alike 4.0 3 Creative Commons Attribution 3.0 3 Creative Commons Attribution Non-Commercial 4.0 2 Custom (Other) 1","title":"Results (July 2019)"},{"location":"glam-data-portals/#results-april-2018","text":"Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Results (April 2018)"},{"location":"hansard/","text":"The proceedings of Australia's Commonwealth Parliament are recorded in Hansard, which is available online through the Parliamentary Library's ParlInfo database. This repository includes Jupyter notebooks to harvest and explore XML formatted versions of Hansard. You can access a full harvest of the XML files for both houses between 1901 and 1980 from this repository. The XML files are made available on the Australian Parliament website under a CC-BY-NC-ND licence. Tools, tips, and examples \u00b6 Harvesting Commonwealth Hansard \u00b6 Results in ParlInfo are generated from well-structured XML files which can be downloaded individually from the web interface \u2013 one XML file for each sitting day. This notebook shows you how to download all the XML files for large scale analysis. It's an updated version of the code I used to harvest Hansard in 2016.","title":"Commonwealth Hansard"},{"location":"hansard/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"hansard/#harvesting-commonwealth-hansard","text":"Results in ParlInfo are generated from well-structured XML files which can be downloaded individually from the web interface \u2013 one XML file for each sitting day. This notebook shows you how to download all the XML files for large scale analysis. It's an updated version of the code I used to harvest Hansard in 2016.","title":"Harvesting Commonwealth Hansard"},{"location":"image-tagging/","text":"Experimenting with automated image tagging in the Tribune negatives \u00b6 Exploring images from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Basic image recognition with Tribune photos \u00b6 Trying Inception-v3's pre-trained model on some Tribune photos. Tensorflow for poets \u00b6 Notebook to accompany the Tensorflow for poets tutorial Training a classification model for the Tribune \u00b6 Using the 'Tensorflow for poets' example to create our own model. Classifying all the Tribune images \u00b6 Using the simple model we created in the previous notebook, let's attempt to classify all the images in the Tribune collection.","title":"Image tagging"},{"location":"image-tagging/#experimenting-with-automated-image-tagging-in-the-tribune-negatives","text":"Exploring images from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Experimenting with automated image tagging in the Tribune negatives"},{"location":"image-tagging/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"image-tagging/#basic-image-recognition-with-tribune-photos","text":"Trying Inception-v3's pre-trained model on some Tribune photos.","title":"Basic image recognition with Tribune photos"},{"location":"image-tagging/#tensorflow-for-poets","text":"Notebook to accompany the Tensorflow for poets tutorial","title":"Tensorflow for poets"},{"location":"image-tagging/#training-a-classification-model-for-the-tribune","text":"Using the 'Tensorflow for poets' example to create our own model.","title":"Training a classification model for the Tribune"},{"location":"image-tagging/#classifying-all-the-tribune-images","text":"Using the simple model we created in the previous notebook, let's attempt to classify all the images in the Tribune collection.","title":"Classifying all the Tribune images"},{"location":"lac/","text":"Library Archives Canada \u00b6 Unfortunately Library Archives Canada doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 LAC naturalisation database, 1915-1946 \u2014 harvest by country \u00b6 This notebook helps you extract the records of people from a specific country from the LAC 1915-1946 naturalisations database . There are numerous limitations and problems, including the fact that you can only get the first 2000 results. Women and children are not returned in a search by country, so the notebook makes an attempt to find and add them to the initial harvest. Here's an example harvest for 'China' .","title":"Library Archives Canada"},{"location":"lac/#library-archives-canada","text":"Unfortunately Library Archives Canada doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Library Archives Canada"},{"location":"lac/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"lac/#lac-naturalisation-database-1915-1946-harvest-by-country","text":"This notebook helps you extract the records of people from a specific country from the LAC 1915-1946 naturalisations database . There are numerous limitations and problems, including the fact that you can only get the first 2000 results. Women and children are not returned in a search by country, so the notebook makes an attempt to find and add them to the initial harvest. Here's an example harvest for 'China' .","title":"LAC naturalisation database, 1915-1946 \u2014 harvest by country"},{"location":"naa-asio/","text":"Series in the National Archives of Australia with content recorded by the Australian Security Intelligence Organisation (ASIO) \u00b6 This repository includes item-level metadata from all 18 series in the National Archives of Australia that are listed on RecordSearch as including content recorded by ASIO. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory. Tools, tips, and examples \u00b6 Harvesting series \u00b6 Code to harvest item-level data and digitised images from series. Analyse a series \u00b6 Some ways of exploring and visualising the item level data. Summarise series data \u00b6 Summarise the data from all harvested series. Summarise series \u00b6 Summarise the data from a single series. Data \u00b6 Harvest May 2018 \u00b6 To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"ASIO archives"},{"location":"naa-asio/#series-in-the-national-archives-of-australia-with-content-recorded-by-the-australian-security-intelligence-organisation-asio","text":"This repository includes item-level metadata from all 18 series in the National Archives of Australia that are listed on RecordSearch as including content recorded by ASIO. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory.","title":"Series in the National Archives of Australia with content recorded by the Australian Security Intelligence Organisation (ASIO)"},{"location":"naa-asio/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"naa-asio/#harvesting-series","text":"Code to harvest item-level data and digitised images from series.","title":"Harvesting series"},{"location":"naa-asio/#analyse-a-series","text":"Some ways of exploring and visualising the item level data.","title":"Analyse a series"},{"location":"naa-asio/#summarise-series-data","text":"Summarise the data from all harvested series.","title":"Summarise series data"},{"location":"naa-asio/#summarise-series","text":"Summarise the data from a single series.","title":"Summarise series"},{"location":"naa-asio/#data","text":"","title":"Data"},{"location":"naa-asio/#harvest-may-2018","text":"To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"Harvest May 2018"},{"location":"naa-wap/","text":"Series in the National Archives of Australia relating to the White Australia Policy \u00b6 This repository includes item-level metadata from 23 series harvested from the National Archives online database RecordSearch. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory. Tools, tips, and examples \u00b6 Harvesting series \u00b6 Code to harvest item-level data and digitised images from series. Analyse a series \u00b6 Some ways of exploring and visualising the item level data. Summarise series data \u00b6 Summarise the data from all harvested series. Summarise series \u00b6 Summarise the data from a single series. Data \u00b6 Harvest May 2018 \u00b6 To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"White Australia Policy archives"},{"location":"naa-wap/#series-in-the-national-archives-of-australia-relating-to-the-white-australia-policy","text":"This repository includes item-level metadata from 23 series harvested from the National Archives online database RecordSearch. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory.","title":"Series in the National Archives of Australia relating to the White Australia Policy"},{"location":"naa-wap/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"naa-wap/#harvesting-series","text":"Code to harvest item-level data and digitised images from series.","title":"Harvesting series"},{"location":"naa-wap/#analyse-a-series","text":"Some ways of exploring and visualising the item level data.","title":"Analyse a series"},{"location":"naa-wap/#summarise-series-data","text":"Summarise the data from all harvested series.","title":"Summarise series data"},{"location":"naa-wap/#summarise-series","text":"Summarise the data from a single series.","title":"Summarise series"},{"location":"naa-wap/#data","text":"","title":"Data"},{"location":"naa-wap/#harvest-may-2018","text":"To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"Harvest May 2018"},{"location":"nma/","text":"National Museum of Australia collection \u00b6 The National Museum of Australia provides access to its collection data through an API . As well as collection items, data is available for parties, places, media, and more. You'll need an API key for serious exploration. Tips, tools, and examples \u00b6 Harvest records from the NMA API \u00b6 If you're going to do any large-scale analysis of the NMA collection data, you probably want to harvest it all using the API and save it locally. This notebook helps you do just that. It harvests records in the simple JSON format and saves them as they are to a file-based database using TinyDB . Download from GitHub View using NBViewer Run live on Binder Exploring object records \u00b6 In this notebook we'll have a preliminary poke around in the object data harvested from the NMA Collection API. I'll focus here on the basic shape/stats of the data, with a deeper dive into the additionalType and extent fields. Can we find the biggest object in the collection? Download from GitHub View using NBViewer Run live on Binder Explore collection objects over time \u00b6 In this notebook we'll explore the temporal dimensions of the object data. When were objects created, collected, or used? To do that we'll extract the nested temporal data, see what's there, and create a few charts. Download from GitHub View using NBViewer Run live on Binder Explore places associated with collection objects \u00b6 In this notebook we'll explore the spatial dimensions of the object data. Where were objects created or collected? To do that we'll extract the nested spatial data, see what's there, and create a few maps. Download from GitHub View using NBViewer Run live on Binder","title":"National Museum of Australia"},{"location":"nma/#national-museum-of-australia-collection","text":"The National Museum of Australia provides access to its collection data through an API . As well as collection items, data is available for parties, places, media, and more. You'll need an API key for serious exploration.","title":"National Museum of Australia collection"},{"location":"nma/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"nma/#harvest-records-from-the-nma-api","text":"If you're going to do any large-scale analysis of the NMA collection data, you probably want to harvest it all using the API and save it locally. This notebook helps you do just that. It harvests records in the simple JSON format and saves them as they are to a file-based database using TinyDB . Download from GitHub View using NBViewer Run live on Binder","title":"Harvest records from the NMA API"},{"location":"nma/#exploring-object-records","text":"In this notebook we'll have a preliminary poke around in the object data harvested from the NMA Collection API. I'll focus here on the basic shape/stats of the data, with a deeper dive into the additionalType and extent fields. Can we find the biggest object in the collection? Download from GitHub View using NBViewer Run live on Binder","title":"Exploring object records"},{"location":"nma/#explore-collection-objects-over-time","text":"In this notebook we'll explore the temporal dimensions of the object data. When were objects created, collected, or used? To do that we'll extract the nested temporal data, see what's there, and create a few charts. Download from GitHub View using NBViewer Run live on Binder","title":"Explore collection objects over time"},{"location":"nma/#explore-places-associated-with-collection-objects","text":"In this notebook we'll explore the spatial dimensions of the object data. Where were objects created or collected? To do that we'll extract the nested spatial data, see what's there, and create a few maps. Download from GitHub View using NBViewer Run live on Binder","title":"Explore places associated with collection objects"},{"location":"nsw-state-archives/","text":"NSW State Archives \u00b6 A collection of notebooks for working with collection data from the NSW State Archives . State Archives content in copyright is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence . See their copyright page for more information. Tools, tips, and examples \u00b6 Get details of online indexes \u00b6 This notebook scrapes details of available indexes from the NSW State Archives A to Z list of online indexes . It saves the results as a CSV formatted file. Download from GitHub View using NBViewer Run live on Binder Harvest online indexes \u00b6 This notebook harvests data from all of NSW State Archives online indexes , saving the data as a collection of easily downloadable CSV files. Download from GitHub View using NBViewer Run live on Binder Summarise index details \u00b6 This notebook counts the number of rows in each index and calculates the total for the whole repository. It formats the results in nice HTML and Markdown tables for easy browsing. Download from GitHub View using NBViewer Run live on Binder NSW State Archives Index Explorer \u00b6 NSW State Archives provides a lot of rich descriptive data in its online indexes. But there's so much data it can be hard to understand what's actually in each index. This notebook tries to help by generating an overview of an index, summarising the contents of each field. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode Data \u00b6 NSW State Archives Online Indexes \u00b6 Harvested: July 2019 This is a repository of data harvested from NSW State Archives online indexes. Data from each index has been extracted from the web interface and saved as a single CSV-formatted file for easy download. View repository on GitHub Currently: 64 indexes harvested with 1,499,259 rows of data. Title Status Number of rows Download data View at NSWSA More info Assisted Immigrants Fully digitised 191688 CSV file Browse index More info Australian Railway Supply Detachment Fully digitised 65 CSV file Browse index More info Bankruptcy Index Not digitised 28880 CSV file Browse index More info Bench of Magistrates cases, 1788-1820 Not digitised 4442 CSV file Browse index More info Botanic Gardens and Government Domains Employees Index Not digitised 916 CSV file Browse index More info Bubonic Plague Index Fully digitised 592 CSV file Browse index More info CSreLand Not digitised 10849 CSV file Browse index More info Child Care and Protection Not digitised 21980 CSV file Browse index More info Closer Settlement Transfer Registers, NRS 8082 Not digitised 4957 CSV file Browse index More info Closer and Soldier Settlement Transfer Files Not digitised 9656 CSV file Browse index More info Colonial Secretary Main series of letters received,1826-1982 Not digitised 7638 CSV file Browse index More info Convict Index Not digitised 141854 CSV file Browse index More info Convicts Applications to Marry 1825-51 Not digitised 8456 CSV file Browse index More info Coroners Inquests 1796-1824 Not digitised 808 CSV file Browse index More info Court of Civil Jurisdiction index Not digitised 2876 CSV file Browse index More info Crew (and Passenger) Lists, 1828-1841 Fully digitised 2560 CSV file Browse index More info Criminal Court Records index 1788-1833 Not digitised 5028 CSV file Browse index More info Criminal Indictments, 1863-1919 Not digitised 15701 CSV file Browse index More info Deceased Estates Not digitised 257524 CSV file Browse index More info Depasturing Licenses Not digitised 7449 CSV file Browse index More info Devonshire Street Cemetery Reinterment Index, 1901 Not digitised 9559 CSV file Browse index More info Divorce Index Not digitised 21239 CSV file Browse index More info Early Convict Index Fully digitised 12933 CSV file Browse index More info FieldBooks Not digitised 813 CSV file Browse index More info Government Architect Not digitised 2373 CSV file Browse index More info Government Asylums for the Infirm and Destitute Not digitised 10264 CSV file Browse index More info Governor\u2019s Court Case Papers, 1815-1824 Not digitised 3789 CSV file Browse index More info Index on Occupants on Aboriginal Reserves, 1875 to 1904 Not digitised 80 CSV file Browse index More info Index to 1841 Census Not digitised 9355 CSV file Browse index More info Index to Closer Settlement Promotion Not digitised 4354 CSV file Browse index More info Index to Court of Claims Not digitised 1051 CSV file Browse index More info Index to Deposition Registers Not digitised 65790 CSV file Browse index More info Index to Early Probate Records Not digitised 1627 CSV file Browse index More info Index to Gaol Photographs Fully digitised 48171 CSV file Browse index More info Index to Intestate Estate Case Papers Not digitised 22520 CSV file Browse index More info Index to Miscellaneous Immigrants Not digitised 8821 CSV file Browse index More info Index to Quarter Sessions cases, 1824-37 Not digitised 6232 CSV file Browse index More info Index to Registers of Firms Not digitised 45683 CSV file Browse index More info Index to Squatters and Graziers Not digitised 9003 CSV file Browse index More info Index to Vessels Arrived, 1837 - 1925 Not digitised 120083 CSV file Browse index More info Index to convict exiles, 1846-50 Not digitised 3036 CSV file Browse index More info Index to the Unassisted Arrivals NSW 1842-1855 Not digitised 135792 CSV file Browse index More info Indigenous Colonial Court Cases 1788-1838 Not digitised 66 CSV file Browse index More info Insolvency Index Not digitised 23108 CSV file Browse index More info King\u2019s and Queen\u2019s Counsel Appointments Fully digitised 2083 CSV file Browse index More info LandGrants Not digitised 5627 CSV file Browse index More info List of Maps and Plans (and Supplement) Not digitised 5455 CSV file Browse index More info NSW Chemists and Druggists Not digitised 2967 CSV file Browse index More info NSW Government Employees Granted Military Leave, 1914-1918 Not digitised 13735 CSV file Browse index More info NSW Govt Railways and Tramways - Roll of Honour - 1914-1919 Not digitised 1214 CSV file Browse index More info Naturalisation Not digitised 9860 CSV file Browse index More info Nominal Roll of the First Railway Section (AIF) Not digitised 417 CSV file Browse index More info Publicans Licenses Not digitised 18457 CSV file Browse index More info Railway Employment Records Not digitised 763 CSV file Browse index More info Register of Auriferous Leases Not digitised 53076 CSV file Browse index More info Registers of Nurses Not digitised 26665 CSV file Browse index More info Registers of Police Not digitised 11319 CSV file Browse index More info Registers of Settlement Purchases Not digitised 9776 CSV file Browse index More info Returned Soldier Settlement Loan Files Not digitised 7642 CSV file Browse index More info Returned Soldiers Settlement Misc files 1916-25 Not digitised 1050 CSV file Browse index More info Schools Not digitised 21246 CSV file Browse index More info Surveyor General - Letters received 1822-55 Not digitised 157 CSV file Browse index More info Teachers Rolls Not digitised 14867 CSV file Browse index More info Unemployed in Sydney 1866 Fully digitised 3222 CSV file Browse index More info","title":"NSW State Archives"},{"location":"nsw-state-archives/#nsw-state-archives","text":"A collection of notebooks for working with collection data from the NSW State Archives . State Archives content in copyright is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence . See their copyright page for more information.","title":"NSW State Archives"},{"location":"nsw-state-archives/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"nsw-state-archives/#get-details-of-online-indexes","text":"This notebook scrapes details of available indexes from the NSW State Archives A to Z list of online indexes . It saves the results as a CSV formatted file. Download from GitHub View using NBViewer Run live on Binder","title":"Get details of online indexes"},{"location":"nsw-state-archives/#harvest-online-indexes","text":"This notebook harvests data from all of NSW State Archives online indexes , saving the data as a collection of easily downloadable CSV files. Download from GitHub View using NBViewer Run live on Binder","title":"Harvest online indexes"},{"location":"nsw-state-archives/#summarise-index-details","text":"This notebook counts the number of rows in each index and calculates the total for the whole repository. It formats the results in nice HTML and Markdown tables for easy browsing. Download from GitHub View using NBViewer Run live on Binder","title":"Summarise index details"},{"location":"nsw-state-archives/#nsw-state-archives-index-explorer","text":"NSW State Archives provides a lot of rich descriptive data in its online indexes. But there's so much data it can be hard to understand what's actually in each index. This notebook tries to help by generating an overview of an index, summarising the contents of each field. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode","title":"NSW State Archives Index Explorer"},{"location":"nsw-state-archives/#data","text":"","title":"Data"},{"location":"nsw-state-archives/#nsw-state-archives-online-indexes","text":"Harvested: July 2019 This is a repository of data harvested from NSW State Archives online indexes. Data from each index has been extracted from the web interface and saved as a single CSV-formatted file for easy download. View repository on GitHub Currently: 64 indexes harvested with 1,499,259 rows of data. Title Status Number of rows Download data View at NSWSA More info Assisted Immigrants Fully digitised 191688 CSV file Browse index More info Australian Railway Supply Detachment Fully digitised 65 CSV file Browse index More info Bankruptcy Index Not digitised 28880 CSV file Browse index More info Bench of Magistrates cases, 1788-1820 Not digitised 4442 CSV file Browse index More info Botanic Gardens and Government Domains Employees Index Not digitised 916 CSV file Browse index More info Bubonic Plague Index Fully digitised 592 CSV file Browse index More info CSreLand Not digitised 10849 CSV file Browse index More info Child Care and Protection Not digitised 21980 CSV file Browse index More info Closer Settlement Transfer Registers, NRS 8082 Not digitised 4957 CSV file Browse index More info Closer and Soldier Settlement Transfer Files Not digitised 9656 CSV file Browse index More info Colonial Secretary Main series of letters received,1826-1982 Not digitised 7638 CSV file Browse index More info Convict Index Not digitised 141854 CSV file Browse index More info Convicts Applications to Marry 1825-51 Not digitised 8456 CSV file Browse index More info Coroners Inquests 1796-1824 Not digitised 808 CSV file Browse index More info Court of Civil Jurisdiction index Not digitised 2876 CSV file Browse index More info Crew (and Passenger) Lists, 1828-1841 Fully digitised 2560 CSV file Browse index More info Criminal Court Records index 1788-1833 Not digitised 5028 CSV file Browse index More info Criminal Indictments, 1863-1919 Not digitised 15701 CSV file Browse index More info Deceased Estates Not digitised 257524 CSV file Browse index More info Depasturing Licenses Not digitised 7449 CSV file Browse index More info Devonshire Street Cemetery Reinterment Index, 1901 Not digitised 9559 CSV file Browse index More info Divorce Index Not digitised 21239 CSV file Browse index More info Early Convict Index Fully digitised 12933 CSV file Browse index More info FieldBooks Not digitised 813 CSV file Browse index More info Government Architect Not digitised 2373 CSV file Browse index More info Government Asylums for the Infirm and Destitute Not digitised 10264 CSV file Browse index More info Governor\u2019s Court Case Papers, 1815-1824 Not digitised 3789 CSV file Browse index More info Index on Occupants on Aboriginal Reserves, 1875 to 1904 Not digitised 80 CSV file Browse index More info Index to 1841 Census Not digitised 9355 CSV file Browse index More info Index to Closer Settlement Promotion Not digitised 4354 CSV file Browse index More info Index to Court of Claims Not digitised 1051 CSV file Browse index More info Index to Deposition Registers Not digitised 65790 CSV file Browse index More info Index to Early Probate Records Not digitised 1627 CSV file Browse index More info Index to Gaol Photographs Fully digitised 48171 CSV file Browse index More info Index to Intestate Estate Case Papers Not digitised 22520 CSV file Browse index More info Index to Miscellaneous Immigrants Not digitised 8821 CSV file Browse index More info Index to Quarter Sessions cases, 1824-37 Not digitised 6232 CSV file Browse index More info Index to Registers of Firms Not digitised 45683 CSV file Browse index More info Index to Squatters and Graziers Not digitised 9003 CSV file Browse index More info Index to Vessels Arrived, 1837 - 1925 Not digitised 120083 CSV file Browse index More info Index to convict exiles, 1846-50 Not digitised 3036 CSV file Browse index More info Index to the Unassisted Arrivals NSW 1842-1855 Not digitised 135792 CSV file Browse index More info Indigenous Colonial Court Cases 1788-1838 Not digitised 66 CSV file Browse index More info Insolvency Index Not digitised 23108 CSV file Browse index More info King\u2019s and Queen\u2019s Counsel Appointments Fully digitised 2083 CSV file Browse index More info LandGrants Not digitised 5627 CSV file Browse index More info List of Maps and Plans (and Supplement) Not digitised 5455 CSV file Browse index More info NSW Chemists and Druggists Not digitised 2967 CSV file Browse index More info NSW Government Employees Granted Military Leave, 1914-1918 Not digitised 13735 CSV file Browse index More info NSW Govt Railways and Tramways - Roll of Honour - 1914-1919 Not digitised 1214 CSV file Browse index More info Naturalisation Not digitised 9860 CSV file Browse index More info Nominal Roll of the First Railway Section (AIF) Not digitised 417 CSV file Browse index More info Publicans Licenses Not digitised 18457 CSV file Browse index More info Railway Employment Records Not digitised 763 CSV file Browse index More info Register of Auriferous Leases Not digitised 53076 CSV file Browse index More info Registers of Nurses Not digitised 26665 CSV file Browse index More info Registers of Police Not digitised 11319 CSV file Browse index More info Registers of Settlement Purchases Not digitised 9776 CSV file Browse index More info Returned Soldier Settlement Loan Files Not digitised 7642 CSV file Browse index More info Returned Soldiers Settlement Misc files 1916-25 Not digitised 1050 CSV file Browse index More info Schools Not digitised 21246 CSV file Browse index More info Surveyor General - Letters received 1822-55 Not digitised 157 CSV file Browse index More info Teachers Rolls Not digitised 14867 CSV file Browse index More info Unemployed in Sydney 1866 Fully digitised 3222 CSV file Browse index More info","title":"NSW State Archives Online Indexes"},{"location":"pm-transcripts/","text":"The Department of Prime Minister and Cabinet provides transcripts of more than 20,000 speeches, media releases, and interviews by Australian Prime Ministers. These transcripts can be searched online , and the underlying XML files can be downloaded using a simple API. This repository includes Jupyter notebooks for harvesting, indexing, analysing, and aggregating the transcripts. A full harvest of the XML files from the PM Transcripts site is available if you don't want to do it yourself. The XML files are made available by the Department of Prime Minister and Cabinet under a Creative Commons Attribution 3.0 Australia Licence. Tools, tips, and examples \u00b6 Harvest transcripts \u00b6 Harvest all the XML transcripts from the PMs Transcripts site. Create an index to the harvested files \u00b6 The XML files contain embedded metadata that includes the name of the prime minister, and the title and date of the transcript. This notebook extracts that metadata from the harvested files and creates a CSV formatted spreadsheet for easy analysis. It also demonstrates some ways of summarising and visualising the metadata. Aggregate transcripts \u00b6 Depending on how you want to analyse them, it can be useful to group the transcripts by prime minister. This notebook aggregates the transcripts in two ways: by extracting the text content of each XML file and combining them into one big text file, and by zipping up the original XML files. Data \u00b6 Index to transcripts \u00b6 CSV formatted file containing metadata extracted from the XML transcripts. The fields are: id \u2013 transcript id date \u2013 release date title pm \u2013 prime minister's name release_type \u2013 type of transcript (speech, interview, media release etc) subjects \u2013 subjects (not used very often) pdf \u2013 url for PDF version (if there is one) Note that the release_type and subjects fields are not used consistently. See the create an index to the harvested files for more analysis of the metadata. XML repository \u00b6 Harvested: 11 July 2019 All of the harvested XML files are available from this repository . In addition to the original XML files, there is: a single zip file for each prime minister containing all their XML transcripts; a single text file for each prime minister containing the text extracted from all of their XML transcripts.","title":"PMs Transcripts"},{"location":"pm-transcripts/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"pm-transcripts/#harvest-transcripts","text":"Harvest all the XML transcripts from the PMs Transcripts site.","title":"Harvest transcripts"},{"location":"pm-transcripts/#create-an-index-to-the-harvested-files","text":"The XML files contain embedded metadata that includes the name of the prime minister, and the title and date of the transcript. This notebook extracts that metadata from the harvested files and creates a CSV formatted spreadsheet for easy analysis. It also demonstrates some ways of summarising and visualising the metadata.","title":"Create an index to the harvested files"},{"location":"pm-transcripts/#aggregate-transcripts","text":"Depending on how you want to analyse them, it can be useful to group the transcripts by prime minister. This notebook aggregates the transcripts in two ways: by extracting the text content of each XML file and combining them into one big text file, and by zipping up the original XML files.","title":"Aggregate transcripts"},{"location":"pm-transcripts/#data","text":"","title":"Data"},{"location":"pm-transcripts/#index-to-transcripts","text":"CSV formatted file containing metadata extracted from the XML transcripts. The fields are: id \u2013 transcript id date \u2013 release date title pm \u2013 prime minister's name release_type \u2013 type of transcript (speech, interview, media release etc) subjects \u2013 subjects (not used very often) pdf \u2013 url for PDF version (if there is one) Note that the release_type and subjects fields are not used consistently. See the create an index to the harvested files for more analysis of the metadata.","title":"Index to transcripts"},{"location":"pm-transcripts/#xml-repository","text":"Harvested: 11 July 2019 All of the harvested XML files are available from this repository . In addition to the original XML files, there is: a single zip file for each prime minister containing all their XML transcripts; a single text file for each prime minister containing the text extracted from all of their XML transcripts.","title":"XML repository"},{"location":"qsa/","text":"Queensland State Archives \u00b6 Tools, tips, and examples \u00b6 Naturalisations, 1851-1904 \u2014 add series information \u00b6 The Naturalisations, 1851 to 1904 index is available from the Queensland Government data portal. It's not clear, however, that the index collates name entries from a number of different series, with a separate row for each name reference. This means that there can be multiple rows referring to the naturalisation of a single individual. This is obviously important to keep in mind if you're trying to analyse aggregate data relating to naturalisations in Queensland. This notebook adds series information to the original index so that you can filter the data by series.","title":"Queensland State Archives"},{"location":"qsa/#queensland-state-archives","text":"","title":"Queensland State Archives"},{"location":"qsa/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"qsa/#naturalisations-1851-1904-add-series-information","text":"The Naturalisations, 1851 to 1904 index is available from the Queensland Government data portal. It's not clear, however, that the index collates name entries from a number of different series, with a separate row for each name reference. This means that there can be multiple rows referring to the naturalisation of a single individual. This is obviously important to keep in mind if you're trying to analyse aggregate data relating to naturalisations in Queensland. This notebook adds series information to the original index so that you can filter the data by series.","title":"Naturalisations, 1851-1904 \u2014 add series information"},{"location":"quick-start/","text":"\u00b6 Is this thing on? \u00b6 Perhaps you've followed a link to a Jupyter notebook and want to try running the code. But when you click a cell, or hit Shift+Enter , nothing happens. Why? Notebooks can be viewed as static (read-only) documents, or run as live (interactive) resources. Static versions are just web pages, there's no computing environment sitting underneath for the code to run in. Both GitHub and NBViewer display static versions of Jupyter notebooks. But don't worry, it's easy to shift from a static view to a live environment using Binder. Opening a notebook in Binder \u00b6 Binder is a cloud-based service that creates customised computing environments for Jupyter notebooks to run in. You give Binder a list of the software you need and it spins up a server ready to go. If you're viewing the static version of a notebook in GitHub or NBViewer Running notebooks \u00b6 Sharing notebooks using NBViewer \u00b6 Sharing notebooks using Binder \u00b6 What is a Jupyter notebook? \u00b6 Distinguish between the notebook format and the software used for viewing or running. Why are they useful? \u00b6 reproducibility exploration learning tool and tutorial What do you need to get started? \u00b6 Nothing -- Binder & cloud services Is this thing on? (viewing vs running) \u00b6 Static views vs running a notebook live NBViewer \u00b6 Links to Binder, GitHub etc. It's alive! \u00b6 Options for running notebooks. Classic vs Lab Separate sections for: Binder Tinker Swan DIY nteract Atom / Hydrogen Binder \u00b6 Tinker \u00b6 Swan \u00b6 DIY \u00b6 App Mode and Voila \u00b6 Inside a notebook \u00b6 Types of cells Running a cell","title":"Quick start"},{"location":"quick-start/#is-this-thing-on","text":"Perhaps you've followed a link to a Jupyter notebook and want to try running the code. But when you click a cell, or hit Shift+Enter , nothing happens. Why? Notebooks can be viewed as static (read-only) documents, or run as live (interactive) resources. Static versions are just web pages, there's no computing environment sitting underneath for the code to run in. Both GitHub and NBViewer display static versions of Jupyter notebooks. But don't worry, it's easy to shift from a static view to a live environment using Binder.","title":"Is this thing on?"},{"location":"quick-start/#opening-a-notebook-in-binder","text":"Binder is a cloud-based service that creates customised computing environments for Jupyter notebooks to run in. You give Binder a list of the software you need and it spins up a server ready to go. If you're viewing the static version of a notebook in GitHub or NBViewer","title":"Opening a notebook in Binder"},{"location":"quick-start/#running-notebooks","text":"","title":"Running notebooks"},{"location":"quick-start/#sharing-notebooks-using-nbviewer","text":"","title":"Sharing notebooks using NBViewer"},{"location":"quick-start/#sharing-notebooks-using-binder","text":"","title":"Sharing notebooks using Binder"},{"location":"quick-start/#what-is-a-jupyter-notebook","text":"Distinguish between the notebook format and the software used for viewing or running.","title":"What is a Jupyter notebook?"},{"location":"quick-start/#why-are-they-useful","text":"reproducibility exploration learning tool and tutorial","title":"Why are they useful?"},{"location":"quick-start/#what-do-you-need-to-get-started","text":"Nothing -- Binder & cloud services","title":"What do you need to get started?"},{"location":"quick-start/#is-this-thing-on-viewing-vs-running","text":"Static views vs running a notebook live","title":"Is this thing on? (viewing vs running)"},{"location":"quick-start/#nbviewer","text":"Links to Binder, GitHub etc.","title":"NBViewer"},{"location":"quick-start/#its-alive","text":"Options for running notebooks. Classic vs Lab Separate sections for: Binder Tinker Swan DIY nteract Atom / Hydrogen","title":"It's alive!"},{"location":"quick-start/#binder","text":"","title":"Binder"},{"location":"quick-start/#tinker","text":"","title":"Tinker"},{"location":"quick-start/#swan","text":"","title":"Swan"},{"location":"quick-start/#diy","text":"","title":"DIY"},{"location":"quick-start/#app-mode-and-voila","text":"","title":"App Mode and Voila"},{"location":"quick-start/#inside-a-notebook","text":"Types of cells Running a cell","title":"Inside a notebook"},{"location":"records-of-resistance/","text":"Tribune negatives \u00b6 Exploring metadata harvested from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Tribune collection metadata magic \u00b6 This notebook helps you explore metadata (and images) from the State Library of NSW's Tribune collection. Topics and descriptions \u00b6 Some more examples of analysing text descriptions and topic tags. Playing with places \u00b6 Cleaning and analysing place data created by students.","title":"Tribune metadata"},{"location":"records-of-resistance/#tribune-negatives","text":"Exploring metadata harvested from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Tribune negatives"},{"location":"records-of-resistance/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"records-of-resistance/#tribune-collection-metadata-magic","text":"This notebook helps you explore metadata (and images) from the State Library of NSW's Tribune collection.","title":"Tribune collection metadata magic"},{"location":"records-of-resistance/#topics-and-descriptions","text":"Some more examples of analysing text descriptions and topic tags.","title":"Topics and descriptions"},{"location":"records-of-resistance/#playing-with-places","text":"Cleaning and analysing place data created by students.","title":"Playing with places"},{"location":"recordsearch/","text":"RecordSearch \u00b6 RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping. Tools, tips, and examples \u00b6 Harvesting a series \u00b6 Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well. Harvest files with the access status of 'closed' \u00b6 The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how. Harvesting functions from the RecordSearch interface \u00b6 This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point. How many of the functions are actually used? \u00b6 In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used. Harvest agencies associated with all functions \u00b6 This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy. Useful apps \u00b6 These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button. Download the contents of a digitised file \u00b6 RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link. Get a list of agencies associated with a function \u00b6 RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function. Who's responsible \u00b6 The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function. DFAT Cable Finder \u00b6 If you ever need to find a file in the National Archives of Australia that contains a specific numbered cable from the Department of Foreign Affairs this is the tool for you! Just give it a cable number and it will look in the series listed below for a file that might contain the cable. For each possible match it returns a link to the file as well as a bit of information about it.","title":"RecordSearch"},{"location":"recordsearch/#recordsearch","text":"RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping.","title":"RecordSearch"},{"location":"recordsearch/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"recordsearch/#harvesting-a-series","text":"Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well.","title":"Harvesting a series"},{"location":"recordsearch/#harvest-files-with-the-access-status-of-closed","text":"The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how.","title":"Harvest files with the access status of 'closed'"},{"location":"recordsearch/#harvesting-functions-from-the-recordsearch-interface","text":"This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point.","title":"Harvesting functions from the RecordSearch interface"},{"location":"recordsearch/#how-many-of-the-functions-are-actually-used","text":"In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used.","title":"How many of the functions are actually used?"},{"location":"recordsearch/#harvest-agencies-associated-with-all-functions","text":"This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy.","title":"Harvest agencies associated with all functions"},{"location":"recordsearch/#useful-apps","text":"These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button.","title":"Useful apps"},{"location":"recordsearch/#download-the-contents-of-a-digitised-file","text":"RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link.","title":"Download the contents of a digitised file"},{"location":"recordsearch/#get-a-list-of-agencies-associated-with-a-function","text":"RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function.","title":"Get a list of agencies associated with a function"},{"location":"recordsearch/#whos-responsible","text":"The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function.","title":"Who's responsible"},{"location":"recordsearch/#dfat-cable-finder","text":"If you ever need to find a file in the National Archives of Australia that contains a specific numbered cable from the Department of Foreign Affairs this is the tool for you! Just give it a cable number and it will look in the series listed below for a file that might contain the cable. For each possible match it returns a link to the file as well as a bit of information about it.","title":"DFAT Cable Finder"},{"location":"suggest-a-topic/","text":"Suggest a topic \u00b6 Suggestions for new institutions, data sources, and topics are welcome. Just add your suggestion to the GitHub issue tracker . Suggest a topic","title":"Suggest a topic"},{"location":"suggest-a-topic/#suggest-a-topic","text":"Suggestions for new institutions, data sources, and topics are welcome. Just add your suggestion to the GitHub issue tracker . Suggest a topic","title":"Suggest a topic"},{"location":"tepapa/","text":"Te Papa collections API \u00b6 Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration. Tips, tools, and examples \u00b6 Exploring the Te Papa collection API \u00b6 This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available. Mapping Te Papa's collections \u00b6 This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Te Papa"},{"location":"tepapa/#te-papa-collections-api","text":"Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration.","title":"Te Papa collections API"},{"location":"tepapa/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"tepapa/#exploring-the-te-papa-collection-api","text":"This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available.","title":"Exploring the Te Papa collection API"},{"location":"tepapa/#mapping-te-papas-collections","text":"This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Mapping Te Papa's collections"},{"location":"trove-books/","text":"Trove's 'book' zone includes books (of course), but also ephemera (like pamphlets and leaflets) and theses. You can access metadata from the book zone through the Trove API. Tips, tools, and examples \u00b6 Harvesting the text of digitised books (and ephemera) \u00b6 This notebook harvests metadata and OCRd text from digitised works in Trove's book zone. Results of the harvest are available below. Metadata for Trove digitised works \u00b6 In poking around to try and find a way of automating the download of OCR text from Trove's digitised books, I discovered that there's lots of useful metadata embedded in the page of a digitised work. Most of this metadata isn't available through the Trove API. Getting the text of Trove books from the Internet Archive \u00b6 Previously I've harvested the text of books digitised by the National Library of Australia and made available through Trove. But it occured to me it might be possible to get the full text of other books in Trove by making use of the links to the Open Library. Data and text \u00b6 OCRd text from Trove books (and ephemera) \u00b6 Harvested: 15 April 2019 I've harvested 9,738 text files from digitised books using the notebook above. You can browse the collection in CloudStor , or download the complete set as a zip file (400mb) . CSV formatted list of books with OCRd text \u00b6 Harvested: 15 April 2019 This file provides metadata of 9,738 works in the Trove book zone that have OCRd text for download. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: children \u2013 pipe-separated ids of any child works contributors \u2013 pipe-separated names of contributors date \u2013 publication date form \u2013 work format fulltext_url \u2013 link to the digitised version pages \u2013 number of pages parent \u2013 id of parent work (if any) text_downloaded \u2013 file name of the downloaded OCR text text_file \u2013 True/False is there any OCRd text title \u2013 title of the work trove_id \u2013 unique identifier url \u2013 link to the metadata record in Trove volume \u2013 volume/part number OCRd text from the Internet Archive of 'Australian' books listed in Trove \u00b6 I've harvested 1,513 text files from the Internet Archive of 'Australian' books listed in Trove using the notebook above. Trove's 'Australian content' filter was used to try to limit the results to books published in, or about, Australia. However, this is not always accurate and some of the harvested works don't seem to have an Australian connection. You can browse the collection in CloudStor . CSV formatted list of 'Australian' books in Trove with full text versions in the Internet Archive \u00b6 Harvested: 24 May 2019 This file includes metadata of 1,511 'Australian' books listed in Trove that have freely available text versions in the Internet Archive. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: creators \u2013 pipe-separated list of creators date \u2013 publication date ia_formats \u2013 pipe-separated list of file formats available from the Internet Archive (these can be downloaded from the IA) ia_id \u2013 Internet Archive identifier ia_url \u2013 link to more information in the Internet Archive ol_id \u2013 Open Library identifier publisher \u2013 publisher text_filename \u2013 name of the downloaded text file title \u2013 title of the book trove_url \u2013 link to more information in Trove version_id \u2013 Trove version identifier work_id \u2013 Trove work identifier","title":"Trove books"},{"location":"trove-books/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-books/#harvesting-the-text-of-digitised-books-and-ephemera","text":"This notebook harvests metadata and OCRd text from digitised works in Trove's book zone. Results of the harvest are available below.","title":"Harvesting the text of digitised books (and ephemera)"},{"location":"trove-books/#metadata-for-trove-digitised-works","text":"In poking around to try and find a way of automating the download of OCR text from Trove's digitised books, I discovered that there's lots of useful metadata embedded in the page of a digitised work. Most of this metadata isn't available through the Trove API.","title":"Metadata for Trove digitised works"},{"location":"trove-books/#getting-the-text-of-trove-books-from-the-internet-archive","text":"Previously I've harvested the text of books digitised by the National Library of Australia and made available through Trove. But it occured to me it might be possible to get the full text of other books in Trove by making use of the links to the Open Library.","title":"Getting the text of Trove books from the Internet Archive"},{"location":"trove-books/#data-and-text","text":"","title":"Data and text"},{"location":"trove-books/#ocrd-text-from-trove-books-and-ephemera","text":"Harvested: 15 April 2019 I've harvested 9,738 text files from digitised books using the notebook above. You can browse the collection in CloudStor , or download the complete set as a zip file (400mb) .","title":"OCRd text from Trove books (and ephemera)"},{"location":"trove-books/#csv-formatted-list-of-books-with-ocrd-text","text":"Harvested: 15 April 2019 This file provides metadata of 9,738 works in the Trove book zone that have OCRd text for download. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: children \u2013 pipe-separated ids of any child works contributors \u2013 pipe-separated names of contributors date \u2013 publication date form \u2013 work format fulltext_url \u2013 link to the digitised version pages \u2013 number of pages parent \u2013 id of parent work (if any) text_downloaded \u2013 file name of the downloaded OCR text text_file \u2013 True/False is there any OCRd text title \u2013 title of the work trove_id \u2013 unique identifier url \u2013 link to the metadata record in Trove volume \u2013 volume/part number","title":"CSV formatted list of books with OCRd text"},{"location":"trove-books/#ocrd-text-from-the-internet-archive-of-australian-books-listed-in-trove","text":"I've harvested 1,513 text files from the Internet Archive of 'Australian' books listed in Trove using the notebook above. Trove's 'Australian content' filter was used to try to limit the results to books published in, or about, Australia. However, this is not always accurate and some of the harvested works don't seem to have an Australian connection. You can browse the collection in CloudStor .","title":"OCRd text from the Internet Archive of 'Australian' books listed in Trove"},{"location":"trove-books/#csv-formatted-list-of-australian-books-in-trove-with-full-text-versions-in-the-internet-archive","text":"Harvested: 24 May 2019 This file includes metadata of 1,511 'Australian' books listed in Trove that have freely available text versions in the Internet Archive. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: creators \u2013 pipe-separated list of creators date \u2013 publication date ia_formats \u2013 pipe-separated list of file formats available from the Internet Archive (these can be downloaded from the IA) ia_id \u2013 Internet Archive identifier ia_url \u2013 link to more information in the Internet Archive ol_id \u2013 Open Library identifier publisher \u2013 publisher text_filename \u2013 name of the downloaded text file title \u2013 title of the book trove_url \u2013 link to more information in Trove version_id \u2013 Trove version identifier work_id \u2013 Trove work identifier","title":"CSV formatted list of 'Australian' books in Trove with full text versions in the Internet Archive"},{"location":"trove-harvester/","text":"Download large quantities of digitised newspaper articles from Trove using the Trove Harvester tool. Tools, tips, and examples \u00b6 Using TroveHarvester to get newspaper articles in bulk \u00b6 An introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Download from GitHub View using NBViewer Run live on Binder Trove Harvester web app \u00b6 A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove. Download from GitHub View using NBViewer Run live on Binder in Appmode Exploring your TroveHarvester data \u00b6 This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Download from GitHub View using NBViewer Run live on Binder Exploring harvested text files \u00b6 This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction) Download from GitHub View using NBViewer Run live on Binder","title":"Trove newspaper harvester"},{"location":"trove-harvester/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"trove-harvester/#using-troveharvester-to-get-newspaper-articles-in-bulk","text":"An introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Download from GitHub View using NBViewer Run live on Binder","title":"Using TroveHarvester to get newspaper articles in bulk"},{"location":"trove-harvester/#trove-harvester-web-app","text":"A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"Trove Harvester web app"},{"location":"trove-harvester/#exploring-your-troveharvester-data","text":"This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Download from GitHub View using NBViewer Run live on Binder","title":"Exploring your TroveHarvester data"},{"location":"trove-harvester/#exploring-harvested-text-files","text":"This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction) Download from GitHub View using NBViewer Run live on Binder","title":"Exploring harvested text files"},{"location":"trove-journals/","text":"Trove's 'journals' zone includes journals and journal articles, as well as other research outputs and things like press releases. You can access metadata from the journal zone through the Trove API, but to get text and images you need to use some screen scraping. Tips, tools, and examples \u00b6 Create a list of Trove's digitised journals \u00b6 Everyone know's about Trove's newspapers, but there is also a growing collection of digitised journals available in the journals zone. They're not easy to find, however, which is why I created the Trove Titles web app. This notebook uses the Trove API to harvest metadata relating to digitised journals \u2013 or more accurately, journals that are freely available online in a digital form. This includes some born digital publications that are available to view in formats like PDF and MOBI, but excludes some digital journals that have access restrictions. Download from GitHub View using NBViewer Run live on Binder Get OCRd text from a digitised journal in Trove \u00b6 Many of the digitised journals available in Trove make OCRd text available for download \u2013 one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them? This notebook shows how to extract issue data from a digitised journal and download OCRd text for each issue. Download from GitHub View using NBViewer Run live on Binder Get covers (or any other pages) from a digitised journal in Trove \u00b6 In another notebook, I showed how to get issue metadata and OCRd texts from a digitised journal in Trove. It's also possible to download page images and PDFs. This notebook shows how to download all the cover images from a specified journal. With some minor modifications you could download any page, or range of pages. Download from GitHub View using NBViewer Run live on Binder Download the OCRd text for ALL the digitised journals in Trove! \u00b6 Using the code and data from the previous two notebooks, you can download the OCRd text from every digitised journal. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder. Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details. I repeat, you probably don't want to do this yourself. The point of this notebook is really to document the methodology used to create the repository. Download from GitHub View using NBViewer Run live on Binder Harvest parliament press releases from Trove \u00b6 Trove includes more than 370,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them all in Trove by searching for nuc:\"APAR:PR\" in the journals zone. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo. Download from GitHub View using NBViewer Run live on Binder Harvesting data from the Bulletin \u00b6 This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Bulletin . It also shows how you can get the front cover images (or any other page). Download from GitHub View using NBViewer Run live on Binder Finding editorial cartoons in the Bulletin \u00b6 In another notebook I showed how you could download all the front pages of The Bulletin (and other journals) as images. Amongst the front pages you'll find a number of full page editorial cartoons under The Bulletin's masthead. But you'll also find that many of the front pages are advertising wrap arounds. The full page editorial cartoons were a consistent feature of The Bulletin for many decades, but they moved around between pages one and eleven. That makes them hard to find. I wanted to try and assemble a collection of all the editorial cartoons, but how? Download from GitHub View using NBViewer Run live on Binder Harvesting data from Home \u00b6 This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Home . It also shows how you can get the front cover images (or any other page). Download from GitHub View using NBViewer Run live on Binder Topic Modelling of Australian Parliamentary Press Releases by Adel Rahmani \u00b6 This notebook explores the Politicians talking about 'immigrants' and 'refugees' collection of press releases (see below). Adel notes: 'I was curious about the contents of the press releases, however, at more than 12,000 documents the collection is too overwhelming to read through, so I thought I'd get the computer to do it for me, and use topic modelling to poke aroung the corpus.' View using NBViewer Data and text \u00b6 CSV formatted list of journals available from Trove in digital form \u00b6 Harvested: 5 July 2019 This file provides metadata of 2,381 journals that are available from Trove in a digital form. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove CSV formatted list of journals with OCRd text \u00b6 Harvested: 5 July 2019 This file provides metadata of 384 digitised journals in Trove that have OCRd text for download. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove issues \u2013 the number of available issues issues_with_text \u2013 the number of issues that OCRd text could be downloaded from directory \u2013 the directory in which the files from this journal have been saved (relative to the output directory) OCRd text from Trove digitised journals \u00b6 Harvested: 5 July 2019 Using the notebook above I harvested metadata and OCRd text from Trove's digitised journals. 384 journals had OCRd text available for download OCRd text was downloaded from 30,462 journal issues About 7gb of text was downloaded The complete collection of text files for all the journals can be browsed and downloaded using this repository on CloudStor . Editorial cartoons from The Bulletin, 1886 to 1952 \u00b6 Harvested: 9 May 2019 Using the notebook above I downloaded at least one full page editorial cartoon for every issue of The Bulletin from 4 September 1886 to 17 September 1952. In total there are 3,471 images (approximately 60gb). The complete collection can be downloaded from CloudStor . The names of each image file provide useful contextual metadata. For example, the file name 19330412-2774-nla.obj-606969767-7.jpg tells you: 19330412 \u2013 the cartoon was published on 12 April 1933 2774 \u2013 it was published in issue number 2774 nla.obj-606969767 \u2013 the Trove identifier for the issue, can be used to make a url eg https://nla.gov.au/nla.obj-606969767 7 \u2013 on page 7 To make it easier to browse the images, I've compiled them into a series of PDFs \u2013 one PDF for each decade. The PDFs include lower resolution versions of the images together with their publication details and a link to Trove. They're all available from DropBox : 1886 to 1889 (45mb PDF) 1890 to 1899 (139mb PDF) 1900 to 1909 (147mb PDF) 1910 to 1919 (153mb PDF) 1920 to 1929 (159mb PDF) 1930 to 1939 (151mb PDF) 1940 to 1949 (146mb PDF) 1950 to 1952 (42mb PDF) Politicians talking about 'immigrants' and 'refugees' \u00b6 Using the notebook above I harvested parliamentary press releases that included any of the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', or 'boat arrivals'. A total of 12,619 text files were harvested. You can browse the files on CloudStor , or download the complete dataset as a zip file (43mb) .","title":"Trove journals"},{"location":"trove-journals/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-journals/#create-a-list-of-troves-digitised-journals","text":"Everyone know's about Trove's newspapers, but there is also a growing collection of digitised journals available in the journals zone. They're not easy to find, however, which is why I created the Trove Titles web app. This notebook uses the Trove API to harvest metadata relating to digitised journals \u2013 or more accurately, journals that are freely available online in a digital form. This includes some born digital publications that are available to view in formats like PDF and MOBI, but excludes some digital journals that have access restrictions. Download from GitHub View using NBViewer Run live on Binder","title":"Create a list of Trove's digitised journals"},{"location":"trove-journals/#get-ocrd-text-from-a-digitised-journal-in-trove","text":"Many of the digitised journals available in Trove make OCRd text available for download \u2013 one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them? This notebook shows how to extract issue data from a digitised journal and download OCRd text for each issue. Download from GitHub View using NBViewer Run live on Binder","title":"Get OCRd text from a digitised journal in Trove"},{"location":"trove-journals/#get-covers-or-any-other-pages-from-a-digitised-journal-in-trove","text":"In another notebook, I showed how to get issue metadata and OCRd texts from a digitised journal in Trove. It's also possible to download page images and PDFs. This notebook shows how to download all the cover images from a specified journal. With some minor modifications you could download any page, or range of pages. Download from GitHub View using NBViewer Run live on Binder","title":"Get covers (or any other pages) from a digitised journal in Trove"},{"location":"trove-journals/#download-the-ocrd-text-for-all-the-digitised-journals-in-trove","text":"Using the code and data from the previous two notebooks, you can download the OCRd text from every digitised journal. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder. Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details. I repeat, you probably don't want to do this yourself. The point of this notebook is really to document the methodology used to create the repository. Download from GitHub View using NBViewer Run live on Binder","title":"Download the OCRd text for ALL the digitised journals in Trove!"},{"location":"trove-journals/#harvest-parliament-press-releases-from-trove","text":"Trove includes more than 370,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them all in Trove by searching for nuc:\"APAR:PR\" in the journals zone. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo. Download from GitHub View using NBViewer Run live on Binder","title":"Harvest parliament press releases from Trove"},{"location":"trove-journals/#harvesting-data-from-the-bulletin","text":"This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Bulletin . It also shows how you can get the front cover images (or any other page). Download from GitHub View using NBViewer Run live on Binder","title":"Harvesting data from the Bulletin"},{"location":"trove-journals/#finding-editorial-cartoons-in-the-bulletin","text":"In another notebook I showed how you could download all the front pages of The Bulletin (and other journals) as images. Amongst the front pages you'll find a number of full page editorial cartoons under The Bulletin's masthead. But you'll also find that many of the front pages are advertising wrap arounds. The full page editorial cartoons were a consistent feature of The Bulletin for many decades, but they moved around between pages one and eleven. That makes them hard to find. I wanted to try and assemble a collection of all the editorial cartoons, but how? Download from GitHub View using NBViewer Run live on Binder","title":"Finding editorial cartoons in the Bulletin"},{"location":"trove-journals/#harvesting-data-from-home","text":"This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Home . It also shows how you can get the front cover images (or any other page). Download from GitHub View using NBViewer Run live on Binder","title":"Harvesting data from Home"},{"location":"trove-journals/#topic-modelling-of-australian-parliamentary-press-releases-by-adel-rahmani","text":"This notebook explores the Politicians talking about 'immigrants' and 'refugees' collection of press releases (see below). Adel notes: 'I was curious about the contents of the press releases, however, at more than 12,000 documents the collection is too overwhelming to read through, so I thought I'd get the computer to do it for me, and use topic modelling to poke aroung the corpus.' View using NBViewer","title":"Topic Modelling of Australian Parliamentary Press Releases by  Adel Rahmani"},{"location":"trove-journals/#data-and-text","text":"","title":"Data and text"},{"location":"trove-journals/#csv-formatted-list-of-journals-available-from-trove-in-digital-form","text":"Harvested: 5 July 2019 This file provides metadata of 2,381 journals that are available from Trove in a digital form. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove","title":"CSV formatted list of journals available from Trove in digital form"},{"location":"trove-journals/#csv-formatted-list-of-journals-with-ocrd-text","text":"Harvested: 5 July 2019 This file provides metadata of 384 digitised journals in Trove that have OCRd text for download. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove issues \u2013 the number of available issues issues_with_text \u2013 the number of issues that OCRd text could be downloaded from directory \u2013 the directory in which the files from this journal have been saved (relative to the output directory)","title":"CSV formatted list of journals with OCRd text"},{"location":"trove-journals/#ocrd-text-from-trove-digitised-journals","text":"Harvested: 5 July 2019 Using the notebook above I harvested metadata and OCRd text from Trove's digitised journals. 384 journals had OCRd text available for download OCRd text was downloaded from 30,462 journal issues About 7gb of text was downloaded The complete collection of text files for all the journals can be browsed and downloaded using this repository on CloudStor .","title":"OCRd text from Trove digitised journals"},{"location":"trove-journals/#editorial-cartoons-from-the-bulletin-1886-to-1952","text":"Harvested: 9 May 2019 Using the notebook above I downloaded at least one full page editorial cartoon for every issue of The Bulletin from 4 September 1886 to 17 September 1952. In total there are 3,471 images (approximately 60gb). The complete collection can be downloaded from CloudStor . The names of each image file provide useful contextual metadata. For example, the file name 19330412-2774-nla.obj-606969767-7.jpg tells you: 19330412 \u2013 the cartoon was published on 12 April 1933 2774 \u2013 it was published in issue number 2774 nla.obj-606969767 \u2013 the Trove identifier for the issue, can be used to make a url eg https://nla.gov.au/nla.obj-606969767 7 \u2013 on page 7 To make it easier to browse the images, I've compiled them into a series of PDFs \u2013 one PDF for each decade. The PDFs include lower resolution versions of the images together with their publication details and a link to Trove. They're all available from DropBox : 1886 to 1889 (45mb PDF) 1890 to 1899 (139mb PDF) 1900 to 1909 (147mb PDF) 1910 to 1919 (153mb PDF) 1920 to 1929 (159mb PDF) 1930 to 1939 (151mb PDF) 1940 to 1949 (146mb PDF) 1950 to 1952 (42mb PDF)","title":"Editorial cartoons from The Bulletin, 1886 to 1952"},{"location":"trove-journals/#politicians-talking-about-immigrants-and-refugees","text":"Using the notebook above I harvested parliamentary press releases that included any of the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', or 'boat arrivals'. A total of 12,619 text files were harvested. You can browse the files on CloudStor , or download the complete dataset as a zip file (43mb) .","title":"Politicians talking about 'immigrants' and 'refugees'"},{"location":"trove-lists/","text":"Trove lists are user created collections of items. The details of public lists are available through the Trove API. Tips, tools, and examples \u00b6 Harvest summary data from Trove lists \u00b6 Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Download from GitHub View using NBViewer Run live on Binder Convert a Trove list into a CSV file \u00b6 Use the Trove API to save the contents of a public list to a CSV file. Download from GitHub View using NBViewer Run live on Binder Data \u00b6 Trove lists metadata \u00b6 Harvested: 20 September 2018 CSV formatted file containing the following fields: created \u2013 date list created id \u2013 list identifier number_items \u2013 number of items in list title \u2013 title of list updated \u2013 date last updated","title":"Trove lists"},{"location":"trove-lists/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-lists/#harvest-summary-data-from-trove-lists","text":"Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Download from GitHub View using NBViewer Run live on Binder","title":"Harvest summary data from Trove lists"},{"location":"trove-lists/#convert-a-trove-list-into-a-csv-file","text":"Use the Trove API to save the contents of a public list to a CSV file. Download from GitHub View using NBViewer Run live on Binder","title":"Convert a Trove list into a CSV file"},{"location":"trove-lists/#data","text":"","title":"Data"},{"location":"trove-lists/#trove-lists-metadata","text":"Harvested: 20 September 2018 CSV formatted file containing the following fields: created \u2013 date list created id \u2013 list identifier number_items \u2013 number of items in list title \u2013 title of list updated \u2013 date last updated","title":"Trove lists metadata"},{"location":"trove-maps/","text":"The Trove 'map' zone includes single maps, as well as map series, atlases, and aerial photographs. You can access metadata from the map zone through the Trove API. Tips, tools, and examples \u00b6 Exploring digitised maps in Trove \u00b6 If you've ever poked around in Trove's 'map' zone, you might have noticed the beautiful deep-zoomable images available for many of the NLA's digitised maps. Even better, in many cases the high-resolution TIFF versions of the digitised maps are available for download. I knew there were lots of great maps you could download from Trove, but how many? And how big were the files? I thought I'd try to quantify this a bit by harvesting and analysing the metadata. Download from GitHub View using NBViewer Run live on Binder Data \u00b6 CSV formatted list of maps with high-resolution downloads \u00b6 Harvested: 26 April 2019 This file provides metadata of 20,158 maps in Trove that have high-resolution downloads. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: copyright_status - a string indicating the copyright status of the map creators \u2013 pipe-separated list of contributors/creators date \u2013 date of the map (format varies) filesize \u2013 size in bytes filesize_string \u2013 size as a human-readable string with unit fulltext_url \u2013 link to the landing page of the digital version height \u2013 height in pixels title \u2013 title of the map trove_id \u2013 digital object identifier trove_url \u2013 link to the metadata record in Trove width \u2013 width in pixels","title":"Trove maps"},{"location":"trove-maps/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-maps/#exploring-digitised-maps-in-trove","text":"If you've ever poked around in Trove's 'map' zone, you might have noticed the beautiful deep-zoomable images available for many of the NLA's digitised maps. Even better, in many cases the high-resolution TIFF versions of the digitised maps are available for download. I knew there were lots of great maps you could download from Trove, but how many? And how big were the files? I thought I'd try to quantify this a bit by harvesting and analysing the metadata. Download from GitHub View using NBViewer Run live on Binder","title":"Exploring digitised maps in Trove"},{"location":"trove-maps/#data","text":"","title":"Data"},{"location":"trove-maps/#csv-formatted-list-of-maps-with-high-resolution-downloads","text":"Harvested: 26 April 2019 This file provides metadata of 20,158 maps in Trove that have high-resolution downloads. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: copyright_status - a string indicating the copyright status of the map creators \u2013 pipe-separated list of contributors/creators date \u2013 date of the map (format varies) filesize \u2013 size in bytes filesize_string \u2013 size as a human-readable string with unit fulltext_url \u2013 link to the landing page of the digital version height \u2013 height in pixels title \u2013 title of the map trove_id \u2013 digital object identifier trove_url \u2013 link to the metadata record in Trove width \u2013 width in pixels","title":"CSV formatted list of maps with high-resolution downloads"},{"location":"trove-newspapers/","text":"Assorted experiments and examples working with Trove\u2019s digitised newspapers Tips, tools, and examples \u00b6 Visualise Trove newspaper searches over time \u00b6 This notebook helps you zoom out and explore how the number of Trove newspaper articles in your search results varies over time by using the decade and year facets . We then combine this approach with other search facets to see how we can slice a set of results up in different ways to investigate historical changes. Download from GitHub View using NBViewer Run live on Binder Visualise the total number of newspaper articles in Trove by year and state \u00b6 Trove currently includes more 200 million digitised newspaper articles published between 1803 and 2015. In this notebook we explore how those newspaper articles are distributed over time, and by state. Download from GitHub View using NBViewer Run live on Binder View an interactive HTML chart Map Trove newspaper results by state \u00b6 Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Download from GitHub View using NBViewer Run live on Binder Map Trove newspaper results by place of publication \u00b6 Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Download from GitHub View using NBViewer Run live on Binder Map Trove newspaper results by place of publication over time \u00b6 Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Download from GitHub View using NBViewer Run live on Binder Today\u2019s news yesterday \u00b6 Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Download from GitHub View using NBViewer Run live on Binder Create a Trove OCR corrections ticker \u00b6 Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds. Download from GitHub View using NBViewer Run live on Binder Save a Trove newspaper article as an image \u00b6 This notebook grabs the page on which an article was published, and then crops the page image to the boundaries of the article. The result is an image which presents the article as it was originally published. Download from GitHub View using NBViewer Run live on Binder Run as an app using Voila Upload Trove newspaper articles to Omeka-S \u00b6 This notebook steps through the process of uploading Trove newspaper articles to your own Omeka-S instance via the API. As well as uploading the article metadata, it attaches image(s) and PDFs of the articles, and creates a linked record for the publishing newspaper. The source of the articles can be a Trove search, a Trove list, a Zotero collection, or just a list of article ids. Download from GitHub View using NBViewer Run live on Binder Beyond the copyright cliff of death \u00b6 Most of the newspaper articles on Trove were published before 1955, but there are some from the later period. Let's find out how many, and which newspapers they were published in. Download from GitHub View using NBViewer Run live on Binder Download a page image \u00b6 The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila Generate an article thumbnail \u00b6 Generate a nice square thumbnail image for a newspaper article. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila Make composite images from lots of Trove newspaper thumbnails \u00b6 This notebook starts with a search in Trove's newspapers. It uses the Trove API to work its way through the search results. For each article it creates a thumbnail image using the code from this notebook. Once this first stage is finished, you have a directory full of lots of thumbnails. The next stage takes all those thumbnails and pastes them one by one into a BIG image to create a composite, or mosaic. Download from GitHub View using NBViewer Run live on Binder QueryPic Deconstructed \u00b6 QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic. Download from GitHub View using NBViewer Run live on Binder in Appmode Get a list of Trove newspapers that doesn't include government gazettes \u00b6 The Trove API includes an option to retrieve details of digitised newspaper titles. Version 2 of the API added a separate option to get details of government gazettes. However the original newspaper/titles requests actually returns both the newspaper and gazette titles, so there's no way of getting just the newspaper titles. This notebook explains the problem and provides a simple workaround. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"Trove newspapers"},{"location":"trove-newspapers/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-newspapers/#visualise-trove-newspaper-searches-over-time","text":"This notebook helps you zoom out and explore how the number of Trove newspaper articles in your search results varies over time by using the decade and year facets . We then combine this approach with other search facets to see how we can slice a set of results up in different ways to investigate historical changes. Download from GitHub View using NBViewer Run live on Binder","title":"Visualise Trove newspaper searches over time"},{"location":"trove-newspapers/#visualise-the-total-number-of-newspaper-articles-in-trove-by-year-and-state","text":"Trove currently includes more 200 million digitised newspaper articles published between 1803 and 2015. In this notebook we explore how those newspaper articles are distributed over time, and by state. Download from GitHub View using NBViewer Run live on Binder View an interactive HTML chart","title":"Visualise the total number of newspaper articles in Trove by year and state"},{"location":"trove-newspapers/#map-trove-newspaper-results-by-state","text":"Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Download from GitHub View using NBViewer Run live on Binder","title":"Map Trove newspaper results by state"},{"location":"trove-newspapers/#map-trove-newspaper-results-by-place-of-publication","text":"Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Download from GitHub View using NBViewer Run live on Binder","title":"Map Trove newspaper results by place of publication"},{"location":"trove-newspapers/#map-trove-newspaper-results-by-place-of-publication-over-time","text":"Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Download from GitHub View using NBViewer Run live on Binder","title":"Map Trove newspaper results by place of publication over time"},{"location":"trove-newspapers/#todays-news-yesterday","text":"Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Download from GitHub View using NBViewer Run live on Binder","title":"Today\u2019s news yesterday"},{"location":"trove-newspapers/#create-a-trove-ocr-corrections-ticker","text":"Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds. Download from GitHub View using NBViewer Run live on Binder","title":"Create a Trove OCR corrections ticker"},{"location":"trove-newspapers/#save-a-trove-newspaper-article-as-an-image","text":"This notebook grabs the page on which an article was published, and then crops the page image to the boundaries of the article. The result is an image which presents the article as it was originally published. Download from GitHub View using NBViewer Run live on Binder Run as an app using Voila","title":"Save a Trove newspaper article as an image"},{"location":"trove-newspapers/#upload-trove-newspaper-articles-to-omeka-s","text":"This notebook steps through the process of uploading Trove newspaper articles to your own Omeka-S instance via the API. As well as uploading the article metadata, it attaches image(s) and PDFs of the articles, and creates a linked record for the publishing newspaper. The source of the articles can be a Trove search, a Trove list, a Zotero collection, or just a list of article ids. Download from GitHub View using NBViewer Run live on Binder","title":"Upload Trove newspaper articles to Omeka-S"},{"location":"trove-newspapers/#beyond-the-copyright-cliff-of-death","text":"Most of the newspaper articles on Trove were published before 1955, but there are some from the later period. Let's find out how many, and which newspapers they were published in. Download from GitHub View using NBViewer Run live on Binder","title":"Beyond the copyright cliff of death"},{"location":"trove-newspapers/#download-a-page-image","text":"The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila","title":"Download a page image"},{"location":"trove-newspapers/#generate-an-article-thumbnail","text":"Generate a nice square thumbnail image for a newspaper article. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila","title":"Generate an article thumbnail"},{"location":"trove-newspapers/#make-composite-images-from-lots-of-trove-newspaper-thumbnails","text":"This notebook starts with a search in Trove's newspapers. It uses the Trove API to work its way through the search results. For each article it creates a thumbnail image using the code from this notebook. Once this first stage is finished, you have a directory full of lots of thumbnails. The next stage takes all those thumbnails and pastes them one by one into a BIG image to create a composite, or mosaic. Download from GitHub View using NBViewer Run live on Binder","title":"Make composite images from lots of Trove newspaper thumbnails"},{"location":"trove-newspapers/#querypic-deconstructed","text":"QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"QueryPic Deconstructed"},{"location":"trove-newspapers/#get-a-list-of-trove-newspapers-that-doesnt-include-government-gazettes","text":"The Trove API includes an option to retrieve details of digitised newspaper titles. Version 2 of the API added a separate option to get details of government gazettes. However the original newspaper/titles requests actually returns both the newspaper and gazette titles, so there's no way of getting just the newspaper titles. This notebook explains the problem and provides a simple workaround. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"Get a list of Trove newspapers that doesn't include government gazettes"},{"location":"trove-unpublished/","text":"Trove unpublished works \u00b6 Experiments and examples relating to Trove's 'Diaries, letters, and archives' zone. Tips, tools, and examples \u00b6 Finding unpublished works that might be entering the public domain on 1 January 2019 \u00b6 Changes to Australian copyright legislation mean that many unpublished resources will be entering the public domain on 1 January 2019. This notebook attempts to harvest the details of some of these resources from Trove. Exploring unpublished works that might be entering the public domain on 1 January 2019 \u00b6 Some ways of exploring the data harvested above. Data \u00b6 Unpublished works that might be entering the public domain on 1 January 2019 \u00b6 Download CSV file (1.8mb) View on Google Sheets","title":"Trove unpublished"},{"location":"trove-unpublished/#trove-unpublished-works","text":"Experiments and examples relating to Trove's 'Diaries, letters, and archives' zone.","title":"Trove unpublished works"},{"location":"trove-unpublished/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-unpublished/#finding-unpublished-works-that-might-be-entering-the-public-domain-on-1-january-2019","text":"Changes to Australian copyright legislation mean that many unpublished resources will be entering the public domain on 1 January 2019. This notebook attempts to harvest the details of some of these resources from Trove.","title":"Finding unpublished works that might be entering the public domain on 1 January 2019"},{"location":"trove-unpublished/#exploring-unpublished-works-that-might-be-entering-the-public-domain-on-1-january-2019","text":"Some ways of exploring the data harvested above.","title":"Exploring unpublished works that might be entering the public domain on 1 January 2019"},{"location":"trove-unpublished/#data","text":"","title":"Data"},{"location":"trove-unpublished/#unpublished-works-that-might-be-entering-the-public-domain-on-1-january-2019","text":"Download CSV file (1.8mb) View on Google Sheets","title":"Unpublished works that might be entering the public domain on 1 January 2019"},{"location":"trove/","text":"Trove provides access to much of it's data through an API (Application Programming Interface). The notebooks in this section provide many examples of using the API to harvest data and analyse the contents of Trove. Before you can use the API you need to obtain a key \u2014 it's free and quick. Just follow these instructions . What's an API? \u00b6 An API is an Application Programming Interface. It's a set of predefined requests and responses that enables computer programs talk to each other. Web APIs are generally used to deliver data. While humans can easily interpret information on a web page, computers need more help. APIs provide data in a form that computers can understand and use (we call this machine-readable data). The Trove API works much like the Trove website. You make queries and you get back results. But instead of going through a nicely-designed web interface, requests to the API are just URLs, and the results are just structured data. While you can just type an API request into the location box of your web browser, most of the time requests and responses will be handled by a computer script or program. APIs don't care what programming language you use as long as you structure requests in the way they expect. In these notebooks we'll be using the programming language Python. No prior knowledge of Python is expected or required -- just follow along! The examples and approaches used could be easily translated into any another programming language. Tools, tips, examples \u00b6 Your first API request \u00b6 In this notebook we're going to learn how to send a request for information to the Trove API. API requests are just like normal urls. However, instead of sending us back a web page, they deliver data in a form that computers can understand. We can then use that data in our own programs. Working with zones \u00b6 Trove's zones are important in constructing API requests and interpreting the results. So let's explore them a bit. Exploring facets \u00b6 Facets aggregate collection data in interesting and useful ways, allowing us to build pictures of the collection. This notebook shows you how to get facet data from Trove.) Useful links \u00b6 Trove API Documentation Trove API console Introduction to using APIs","title":"Trove API introduction"},{"location":"trove/#whats-an-api","text":"An API is an Application Programming Interface. It's a set of predefined requests and responses that enables computer programs talk to each other. Web APIs are generally used to deliver data. While humans can easily interpret information on a web page, computers need more help. APIs provide data in a form that computers can understand and use (we call this machine-readable data). The Trove API works much like the Trove website. You make queries and you get back results. But instead of going through a nicely-designed web interface, requests to the API are just URLs, and the results are just structured data. While you can just type an API request into the location box of your web browser, most of the time requests and responses will be handled by a computer script or program. APIs don't care what programming language you use as long as you structure requests in the way they expect. In these notebooks we'll be using the programming language Python. No prior knowledge of Python is expected or required -- just follow along! The examples and approaches used could be easily translated into any another programming language.","title":"What's an API?"},{"location":"trove/#tools-tips-examples","text":"","title":"Tools, tips, examples"},{"location":"trove/#your-first-api-request","text":"In this notebook we're going to learn how to send a request for information to the Trove API. API requests are just like normal urls. However, instead of sending us back a web page, they deliver data in a form that computers can understand. We can then use that data in our own programs.","title":"Your first API request"},{"location":"trove/#working-with-zones","text":"Trove's zones are important in constructing API requests and interpreting the results. So let's explore them a bit.","title":"Working with zones"},{"location":"trove/#exploring-facets","text":"Facets aggregate collection data in interesting and useful ways, allowing us to build pictures of the collection. This notebook shows you how to get facet data from Trove.)","title":"Exploring facets"},{"location":"trove/#useful-links","text":"Trove API Documentation Trove API console Introduction to using APIs","title":"Useful links"},{"location":"umdigitalstudio2019/","text":"GLAM data workshop \u2013 Digital Studio, University of Melbourne, 15 August 2019 \u00b6 FAQ \u00b6 Where do I find this page? Just go to https://glam-workbench.github.io/ and look under the 'Workshops' menu. What do I need to try the activities? Nothing but a web browser and an Internet connection. We'll be using a cloud-based service called Binder to run the live examples. Oh, but a Trove API key would be handy. Can I try this at home? Absolutely! This page and the linked notebooks will stay online, so feel free to work through the examples at any time. Can I share this workshop/notebook/site? Please do! If you're tweeting please add #glamworkbench, or tag me in (I'm @wragge )! Seeing differently \u00b6 Activity 1: A different perspective on Trove \u00b6 What do you know about Trove\u2019s newspapers? Open up the Newspapers home page . Search for something you\u2019re interested in. What do we know about that search? What\u2019s in the results? How do they change over time? What\u2019s missing? Go to QueryPic Deconstructed . (It might take a little while to load \u2013 just be patient!) Enter your Trove API key. Enter the same search term in the box and click Add query . Click Create chart . What do you see? Anything interesting or unexpected? Try entering multiple search terms. (Just hit Add query to add each term or Clear all to start again.) What has changed from the standard search interface? How might you use this in your own research? Collections as data \u00b6 We already use GLAM collections in our research, why would we want to access and analyse GLAM data directly? Why would we want collections as data ? Shift scales \u2013 zoom in and out, see your searches in a range of contexts Find patterns \u2013 look for shifts in language, the impact of events or policies, changes over time and space Extract features \u2013 find people, places, and events Make connections \u2013 build contexts, identify gaps and overlaps To help people explore these sorts of possibilities, I've been assembling examples based on the collections of a variety of GLAM institutions. The GLAM Workbench is not about specific tools or technologies, it\u2019s about introducing opportunities for seeing GLAM collections differently. It\u2019s a collection of examples for you to use, change, and hopefully learn from. The GLAM Workbench makes heavy use of Jupyter notebooks. Jupyter is a web-based framework that lets you create and share 'computational narratives' \u2013 web pages that blend live code with text, images, examples and more. They break down the distinction between tool and tutorial, enabling you to undertake useful tasks while learning about what's going on underneath the hood. Here\u2019s a 7 minute intro to the GLAM Workbench\u2026 And here's a slightly longer discussion of some of the possibilities of working with collections as data\u2026 But what is GLAM data? \u00b6 GLAM data can take many forms and have a range of characteristics. It's not just a matter of the content of the data, but also how it's controlled, structured, and shared that affects what we can do with it. Working with GLAM data means constantly grappling with questions of access and usability. Here are a few categories of GLAM data you might meet. There are lots of overlaps between these categories, but it's a place to start. Metadata (not content) Structured text / data Unstructured text Images Derived data User generated data Activity data Born digital data Metadata \u00b6 Data about data! In regards to GLAM collections we\u2019re probably talking about the sorts of descriptive information you might find in a catalogue or collection database rather than the digitised objects themselves. Examples: Closed Access \u2013 What files in the National Archives of Australia are closed to public access? What can we know about what we\u2019re not allowed to see? You can harvest your own dataset using this notebook . Datasets from 2017 , 2016 , and 2015 are available on Figshare. Exploring digitised maps in Trove \u2013 I knew the NLA had been had been creating a lot of high-res digital version of historic maps. But how many, and how big? Structured data or text \u00b6 Think data with rows and columns, the sort of thing you might open with a spreadsheet program; although you might also find more complex GLAM data as XML or JSON. The main thing is that the data is organised into fields. This should enable us to do things with the data in those fields. If a field contains dates, for example, we could chart them over time, or extract the years. We sometimes also call this sort of data 'machine readable'. Of course this assumes that the data has been formatted consistently, and as anyone who works with GLAM data will tell you, this is a dangerous assumption to make... Examples: Here's more than 900 GLAM datasets (!) harvested from government data portals. Here\u2019s a big, human-readable list of the datasets. NSW State Archives online indexes \u2013 64 different indexes containing 1,499,259 rows of data! Activity 2: GLAM CSV Explorer \u00b6 What sort of structure does structured data have and how can we use it to help us understand the content? Let's have a little play with the GLAM CSV Explorer which generates automatic previews of more than 500 CSV files harvested from data.gov.au. There's not much to it: Go to the live version . Select a file from the dropdown list and click the button. Rinse and repeat. Do you get a sense of the types of data in the files? Unstructured text \u00b6 Of course text has structure. It has sentences and paragraphs and nouns and proper nouns and verbs, and even punctuation. But most of these structures are implicit, they're not neatly labeled for us to extract and manipulate. We can train computers to find those structures, but it's not a simple process. Examples: Commonwealth Hansard, 1901\u201380 \u2013 Ok, so this is a bit of a structured/unstructured hybrid. The spoken text, as recorded by Hansard, is itself unstructured, but it's embedded in nicely-structured XML files that include information about who spoke the text and when. Whatever it is, there's a lot of it! Trove journals \u2013 As well as newspapers, Trove includes a growing range of digitised journals. Like the newspapers, they've been OCRd and you can download the full text. To save you a lot of pointing and clicking I've harvested the text of 30,462 issues from 384 journals. That's 7gb of text. Get your bulk downloads here! Images \u00b6 Images are data too \u2013 they can be processed and manipulated in all sorts of interesting ways to expose features and patterns. Sometimes we might also want to find particular types of images amongst a large collection. Examples: 'How hard would it be to find the infamous full page editorial cartoons in the Bulletin ?', I foolishly wondered. After several false starts and much trial and error, I ended up with a collection of 3.471 cartoons \u2013 at least one for every issue from 1886 to 1952. The gory details are in this notebook. For more images in bulk, you could try harvesting the covers of a journal in Trove (some of them are really beautiful). Or what about downloading all the digitised files from a series in the National Archives of Australia (NAA). Someone [name redacted] might have already harvested all the publicly available ASIO surveillance files. Derived data \u00b6 Sometimes in processing GLAM data we create new GLAM data that highlights particular features, provides new access points, or adds structure and context. Yay, more data! Examples: Finding faces \u2013 I started playing around with facial detection software about 8 years ago when I created this wall of faces from NAA records used in the administration of the White Australia policy. I still think its an interesting way of changing our perspective on large photographic collections. Here's some more recent experiments with photos from the State Library of NSW's Tribune collection. If you've ever wanted a collection of 200,000 redactions extracted from ASIO surveillance files, have I got a dataset for you \u2013 images , metadata, and even a browsable interface . But watch out for the #redactionart ! User generated data \u00b6 The users of GLAM collections can also create data \u2013 through crowdsourced transcription projects, by adding tags and comments to guide others, or by building and sharing their own collections. We set up the Real Face of White Australia project to transcribe structured data from forms used in the administration of the White Australia Policy. The data itself is shared through its own repository , and used to build tweet-sized stories . I love Trove lists because they're an insight into people's passions (do you know the story of lawnmower man ?). But you can also extract data about Trove lists to explore their contents at scale . Activity data \u00b6 Changes made to collections can also be recorded and shared. Here's a notebook that explores patterns in the OCR corrections volunteers have made to Trove's newspapers. And if you want up-to-date info, here's a live OCR Correction Ticker . Unpacking interfaces, exploring data, & thinking critically \u00b6 It's rarely the case that we can just download and use GLAM data. Even when the data is openly-licensed, well-structured, and easy to access, we still have to spend time getting to know it. What are its limits, and its assumptions? Many of the notebooks in the GLAM Workbench are intended to help with this exploratory phase. Before you can use GLAM data, you have to play with it. From here on we're going to working with live Jupyter notebooks running on Binder. In actual fact, you already have, because QueryPic and the CSV Explorer were also live notebooks, it's just that they were running with all the code hidden. But now you'll get a chance to see and change the code. Jupyter notebooks quick start Don't panic! Jupyter notebooks are meant to encourage experimentation, so don't feel nervous. Just try running a few cells and see what happens! Here's some hints: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it's a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you'll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you've finished editing. Activity 3: Trove newspapers over time \u00b6 Let's go back again to QueryPic and explore the sorts of assumptions we need to unpack when we're exploring Trove's newspapers over time. Just click on the links below to launch the notebooks on Binder. Then just follow the instructions, hitting Shift+Enter on each cell to run it. You'll need to have your Trove API key ready to insert where indicated. Visualise Trove newspaper searches over time Visualise the total number of newspaper articles in Trove by year and state Hopefully these notebooks will give you a sense of how the nature of the digitised newspapers themselves can be critically analysed. Activity 4: What's in the National Museum of Australia? \u00b6 As I mentioned, you have to spend time playing around to understand both the limits and the possibilities of a new dataset. Recently the National Museum of Australia opened up access to its collection data through an API. Here's some notebooks that poke about inside... Note that I've pre-harvested the objects and places data. But before you start working on it, you'll need to follow the link to unzip the data file. Start by exploring object records Click on the 'unzip a pre-harvested dataset' link and run the cell in the linked notebook to prepare the data. Work through the 'Exploring object records' notebook. Think about the sorts of metadata that have been captured. When you've run all the cells, click on the link to the 'time' notebook at the top of the page. This notebook explores the temporal dimensions of the data. When were objects created or collected? There's a few puzzles along the way! When you've run all the cells, click on the link to 'explore places'. This notebook unpacks spatial information and creates some maps. What's missing from the NMA data? What other perspectives might we explore? Activity 5: Exploring the text of Trove's journals \u00b6 What sort of patterns can we observe across the issues of a journal? Let's try working with some unstructured text. Apologies in advance \u2013 this notebook is hot off the presses and is missing a lot of documentation. But just work it using the trusty ol' Shift+Enter and you should see some magic happen. You'll need to select a journal from the dropdown list and then download the OCRd text from the repository on CloudStor. For this exercise, I'd suggest you start by selecting something that doesn't have more than a few hundred issues \u2013 Art in Australia for example. Word frequencies in digitised journals Lunch break \u00b6 Handy hints & hacks \u00b6 Sometimes things that should be easy, aren't. This applies to GLAM collections as much as it does to everything else in life. But at least in the case of GLAM collections we can build and share hacks and hints to overcome at least some of the frustrations. Activity 6: Get an image of a newspaper article in Trove \u00b6 You can download just dowload a newspaper article from the Trove web interface can't you? Well, sort of. You can generate an HTML page which embeds images of the article, although they might be sliced up in strange ways that make them almost unreadable. Sigh... It's often easier just to do a screen capture. But here's an alternative. Find an article in Trove. For bonus points find an illustrated article spread over more than one page. Click here When the page has loaded, paste the article url into the box. Hit the Get Images button! Download your image. You might have guessed that this little app is another Jupyter notebook in disguise. Here's a few other apps/notebooks that you might find handy for getting or moving data. Apps: Create a thumbnail image from a Trove newspaper article Save a Trove newspaper page as a (high-res) image Download the contents of a digitised file from RecordSearch Notebooks: Upload Trove newspaper articles to Omeka-S Convert a Trove list into a CSV file First catch your data \u00b6 One thing we haven't really covered yet is where you find GLAM data and how do you get it in a usable form. As a starting point, I've created a list of GLAM data sources . But as I suggested earlier, access is not a straightforward thing \u2013 just because data is online, doesn't mean it's easy to download or use. Here's some of the real world categories of GLAM data that we often have to deal with: Machine-readable but coding required for download Downloadable but not easily findable Structured but not downloadable Downloadable but in many separate pieces Machine-readable but coding required for download \u00b6 Trove makes a lot (but not all!) of its collection data available via an API (Application Programming Interface). So do other GLAM institutions like the NMA and the Museums Victoria. I love APIs, but to get data out of them you need to be able to code. How do we fill the gap between APIs and the researchers who want to use the data they provide? Here's some notebooks that provide an interface between you and the API, making it easy to explore and download the data you're interested in. Trove newspaper harvester Harvest records from the NMA API DigitalNZ \u2013 New Zealand's Trove! Te Papa collection API Downloadable but not easily findable \u00b6 I've already mentioned that there's lots of GLAM datasets available through the data.gov.au portal, but who would have known it? In total there's 948 datasets from 22 organisations . Some of these are more interesting than others, but there\u2019s some really rich historical data amongst them. However, unless you know where to look they can be hard to find. Datasets are often buried deep in organisational websites, and while data.gov.au has a good search interface, how many people would think to look there for data from libraries? Structured but not downloadable \u00b6 Other than Trove, the collection I\u2019ve spent most time wrestling with is that of the National Archives of Australia. The NAA\u2019s online database, RecordSearch , doesn\u2019t provide an API or download options, so to get data out you have to resort to a process known as screen-scraping \u2014 you have to write some code to extract the data you want from web pages. However, the data in RecordSearch is really rich and complex, so it\u2019s worth the effort. Another example in this category are the indexes to records created by the NSW State Archives and their volunteers. These indexes include things like names, places, and dates, and provide useful entry points into the records. But they\u2019re also rich datasets. The indexes are openly licensed, they\u2019re online, they\u2019re searchable, but there\u2019s no way of downloading them except by screen scraping. So I\u2019ve harvested all the indexes and made them available as CSV files . Screen scrapers are kludgey and prone to breakage, but sometimes it't the only way to get useable data out of GLAM websites. Here's some more examples: Harvest a series from RecordSearch Harvesting functions from the RecordSearch interface Harvesting a records search from Archway (Archives of New Zealand) Harvest the Library and Archives Canada naturalisation database, 1915-1946, by country Downloadable but in many separate pieces \u00b6 As I mentioned early on, when you work with collections as data you can play around with scale, zooming out from a single instance to look for patterns across thousands of files. But this is difficult if you\u2019re only option is to manually download one file at a time. To make useful data our of a collection, sometimes you just have to bring the pieces together. Commonwealth Hansard is an example of this. Sitting underneath the ParlInfo search interface are a lot of well-structured XML files, one for each sitting day in the Senate and House of Reps. But you can only download them manually \u2014 one at a time. I\u2019ve harvested all the XML files from 1901 to 1980 and saved them to a separate repository. All those separate files have become a corpus for large-scale text analysis. The same goes for the transcripts of more than 20,000 speeches, media releases, and interviews by Australian Prime Ministers made available online by the Department of Prime Minister and Cabinet. They're online, they have metadata, and they're downloadable as XML files. But by harvesting them and making them easily downloadable in bulk, we change what's possible. Trove\u2019s API is great, but there\u2019s some important data you can\u2019t get through it \u2013 the content of Trove\u2019s digitised journals , for example, can only be accessed through the web interface. I\u2019ve created a series of notebooks to help you extract metadata, text, and digitised page images from the journals. To do this I've supplemented what's available through the API with extra data from screen scraping, and some reverse engineering of the web interface. I've done the same for Trove's digitised books . Finding pathways \u00b6 How do we start to put all this bits and pieces together in a way that allows researchers to pursue the questions that interest them across collections? Let's experiment with one possible pathway. Activity 7: Zoom out, zoom in \u00b6 First go back once again to the QueryPic visualisation of newspaper searches over time. But this time, I want you to play around with search terms until you find something you think might be worth exploring further. Perhaps there's an unexpected peak, or an interesting shift in usage between particular words. Once you have identified your feature of interest, head to the Trove web interface and construct a search that focuses on that feature. For example, you might use the facets to filter your search by year, state, newspaper, or article type. For the purposes of this exercise, you want to limit the number of results in your search to a few thousand at most. (This is just so you don't spend the rest of the workshop waiting for your 253,000 newspaper articles to download...) Copy the url of your Trove search. Now we're going to feed your search url to the Trove Newspaper Harvester . Once again, you'll need your Trove API key . Just paste your API query and your search url into the boxes where indicated. Run the cell that says %run -m troveharvester -- start $query $api_key --text to start your harvest. The Trove Newspaper Harvester is actually a command line tool, but I've embedded in this notebook to make it easier to use. There's also an app-ified version that makes it even easier! The Newspaper Harvester downloads both the metadata about the articles in your search, and the OCRd text of each individual article. This means that we can explore the content of the articles in depth. It's worth remembering too that the Trove web interface only lets you see the first 2,000 articles in your search. The Newspaper Harvester can download many thousands of articles. I recently harvested more than half a million articles to explore the changing context of the words 'aliens' and 'immigrants' . The notebook includes information on the structure of the metadata file. Once your harvest is finished, click on the link at the bottom of the notebook that says 'Exploring your TroveHarvester data' to load a new notebook. This notebook tries analysing the metadata file you just harvested in a number of different ways. It breaks the results down by newspaper, by time, and by place of publication. It also looks at the frequencies of words in the article titles. It's in need of an overhaul, but it's a useful starting point. Once you've got some sense of the harvested metadata, it's time to look at the text contents of the articles themselves. Click on the link at the bottom of the notebook to explore word frequencies in the text content of the articles. This notebook calculates word frequencies for each article and then aggregates these counts by year. You can also explore how the frequency of particular words change over time. Under 'Visualise word frequencies over time' there's a cell where you can enter words that you want to investigate. Visualise the results as both a facet chart and a bubbleline chart. Once you're ready to move on, click on the link at the bottom to open a notebook that calculates TF-IDF values for words in the articles. TF-IDF scores are calculated by comparing the number of time a word appears in a single document to the number of times it appears in a collection of documents. Word frequencies point us to 'common' words, TF-IDF can indicate 'significant' words. This notebook aggregates the articles by year and then calculates TF-IDF values for each word in each year. Once again you can explore how the TF-IDF values of specific words change over time. So, did you find anything interesting? The point of this exercise was not to define a specific methodology, but to suggest different ways you might pursue a set of research questions through a number of shifts in scale. Of course there are many other things we could do to explore the article text (and more will be added to the GLAM Workbench in time). We could for example look for clusters amongst the articles by using topic modelling. Here's a great notebook by Adel Rahmani that explores topic modelling of Australian parliamentary press releases harvested via Trove. Suggestions welcome \u00b6 If you have suggestions for data sources, tools, or examples to add the GLAM Workbench, feel free to add them via GitHub . Additional readings and resources \u00b6 Here's a few sites and resources that came up in discussion: Explore Trove's digitised journals Hansard interjections as tweets Trove tips & tricks Digital Humanities Slack \u2013 Join me in the #dh-australia channel!","title":"GLAM Workbench workshop, 15 August 2019"},{"location":"umdigitalstudio2019/#glam-data-workshop-digital-studio-university-of-melbourne-15-august-2019","text":"","title":"GLAM data workshop \u2013 Digital Studio, University of Melbourne, 15 August 2019"},{"location":"umdigitalstudio2019/#faq","text":"Where do I find this page? Just go to https://glam-workbench.github.io/ and look under the 'Workshops' menu. What do I need to try the activities? Nothing but a web browser and an Internet connection. We'll be using a cloud-based service called Binder to run the live examples. Oh, but a Trove API key would be handy. Can I try this at home? Absolutely! This page and the linked notebooks will stay online, so feel free to work through the examples at any time. Can I share this workshop/notebook/site? Please do! If you're tweeting please add #glamworkbench, or tag me in (I'm @wragge )!","title":"FAQ"},{"location":"umdigitalstudio2019/#seeing-differently","text":"","title":"Seeing differently"},{"location":"umdigitalstudio2019/#activity-1-a-different-perspective-on-trove","text":"What do you know about Trove\u2019s newspapers? Open up the Newspapers home page . Search for something you\u2019re interested in. What do we know about that search? What\u2019s in the results? How do they change over time? What\u2019s missing? Go to QueryPic Deconstructed . (It might take a little while to load \u2013 just be patient!) Enter your Trove API key. Enter the same search term in the box and click Add query . Click Create chart . What do you see? Anything interesting or unexpected? Try entering multiple search terms. (Just hit Add query to add each term or Clear all to start again.) What has changed from the standard search interface? How might you use this in your own research?","title":"Activity 1: A different perspective on Trove"},{"location":"umdigitalstudio2019/#collections-as-data","text":"We already use GLAM collections in our research, why would we want to access and analyse GLAM data directly? Why would we want collections as data ? Shift scales \u2013 zoom in and out, see your searches in a range of contexts Find patterns \u2013 look for shifts in language, the impact of events or policies, changes over time and space Extract features \u2013 find people, places, and events Make connections \u2013 build contexts, identify gaps and overlaps To help people explore these sorts of possibilities, I've been assembling examples based on the collections of a variety of GLAM institutions. The GLAM Workbench is not about specific tools or technologies, it\u2019s about introducing opportunities for seeing GLAM collections differently. It\u2019s a collection of examples for you to use, change, and hopefully learn from. The GLAM Workbench makes heavy use of Jupyter notebooks. Jupyter is a web-based framework that lets you create and share 'computational narratives' \u2013 web pages that blend live code with text, images, examples and more. They break down the distinction between tool and tutorial, enabling you to undertake useful tasks while learning about what's going on underneath the hood. Here\u2019s a 7 minute intro to the GLAM Workbench\u2026 And here's a slightly longer discussion of some of the possibilities of working with collections as data\u2026","title":"Collections as data"},{"location":"umdigitalstudio2019/#but-what-is-glam-data","text":"GLAM data can take many forms and have a range of characteristics. It's not just a matter of the content of the data, but also how it's controlled, structured, and shared that affects what we can do with it. Working with GLAM data means constantly grappling with questions of access and usability. Here are a few categories of GLAM data you might meet. There are lots of overlaps between these categories, but it's a place to start. Metadata (not content) Structured text / data Unstructured text Images Derived data User generated data Activity data Born digital data","title":"But what is GLAM data?"},{"location":"umdigitalstudio2019/#metadata","text":"Data about data! In regards to GLAM collections we\u2019re probably talking about the sorts of descriptive information you might find in a catalogue or collection database rather than the digitised objects themselves. Examples: Closed Access \u2013 What files in the National Archives of Australia are closed to public access? What can we know about what we\u2019re not allowed to see? You can harvest your own dataset using this notebook . Datasets from 2017 , 2016 , and 2015 are available on Figshare. Exploring digitised maps in Trove \u2013 I knew the NLA had been had been creating a lot of high-res digital version of historic maps. But how many, and how big?","title":"Metadata"},{"location":"umdigitalstudio2019/#structured-data-or-text","text":"Think data with rows and columns, the sort of thing you might open with a spreadsheet program; although you might also find more complex GLAM data as XML or JSON. The main thing is that the data is organised into fields. This should enable us to do things with the data in those fields. If a field contains dates, for example, we could chart them over time, or extract the years. We sometimes also call this sort of data 'machine readable'. Of course this assumes that the data has been formatted consistently, and as anyone who works with GLAM data will tell you, this is a dangerous assumption to make... Examples: Here's more than 900 GLAM datasets (!) harvested from government data portals. Here\u2019s a big, human-readable list of the datasets. NSW State Archives online indexes \u2013 64 different indexes containing 1,499,259 rows of data!","title":"Structured data or text"},{"location":"umdigitalstudio2019/#activity-2-glam-csv-explorer","text":"What sort of structure does structured data have and how can we use it to help us understand the content? Let's have a little play with the GLAM CSV Explorer which generates automatic previews of more than 500 CSV files harvested from data.gov.au. There's not much to it: Go to the live version . Select a file from the dropdown list and click the button. Rinse and repeat. Do you get a sense of the types of data in the files?","title":"Activity 2: GLAM CSV Explorer"},{"location":"umdigitalstudio2019/#unstructured-text","text":"Of course text has structure. It has sentences and paragraphs and nouns and proper nouns and verbs, and even punctuation. But most of these structures are implicit, they're not neatly labeled for us to extract and manipulate. We can train computers to find those structures, but it's not a simple process. Examples: Commonwealth Hansard, 1901\u201380 \u2013 Ok, so this is a bit of a structured/unstructured hybrid. The spoken text, as recorded by Hansard, is itself unstructured, but it's embedded in nicely-structured XML files that include information about who spoke the text and when. Whatever it is, there's a lot of it! Trove journals \u2013 As well as newspapers, Trove includes a growing range of digitised journals. Like the newspapers, they've been OCRd and you can download the full text. To save you a lot of pointing and clicking I've harvested the text of 30,462 issues from 384 journals. That's 7gb of text. Get your bulk downloads here!","title":"Unstructured text"},{"location":"umdigitalstudio2019/#images","text":"Images are data too \u2013 they can be processed and manipulated in all sorts of interesting ways to expose features and patterns. Sometimes we might also want to find particular types of images amongst a large collection. Examples: 'How hard would it be to find the infamous full page editorial cartoons in the Bulletin ?', I foolishly wondered. After several false starts and much trial and error, I ended up with a collection of 3.471 cartoons \u2013 at least one for every issue from 1886 to 1952. The gory details are in this notebook. For more images in bulk, you could try harvesting the covers of a journal in Trove (some of them are really beautiful). Or what about downloading all the digitised files from a series in the National Archives of Australia (NAA). Someone [name redacted] might have already harvested all the publicly available ASIO surveillance files.","title":"Images"},{"location":"umdigitalstudio2019/#derived-data","text":"Sometimes in processing GLAM data we create new GLAM data that highlights particular features, provides new access points, or adds structure and context. Yay, more data! Examples: Finding faces \u2013 I started playing around with facial detection software about 8 years ago when I created this wall of faces from NAA records used in the administration of the White Australia policy. I still think its an interesting way of changing our perspective on large photographic collections. Here's some more recent experiments with photos from the State Library of NSW's Tribune collection. If you've ever wanted a collection of 200,000 redactions extracted from ASIO surveillance files, have I got a dataset for you \u2013 images , metadata, and even a browsable interface . But watch out for the #redactionart !","title":"Derived data"},{"location":"umdigitalstudio2019/#user-generated-data","text":"The users of GLAM collections can also create data \u2013 through crowdsourced transcription projects, by adding tags and comments to guide others, or by building and sharing their own collections. We set up the Real Face of White Australia project to transcribe structured data from forms used in the administration of the White Australia Policy. The data itself is shared through its own repository , and used to build tweet-sized stories . I love Trove lists because they're an insight into people's passions (do you know the story of lawnmower man ?). But you can also extract data about Trove lists to explore their contents at scale .","title":"User generated data"},{"location":"umdigitalstudio2019/#activity-data","text":"Changes made to collections can also be recorded and shared. Here's a notebook that explores patterns in the OCR corrections volunteers have made to Trove's newspapers. And if you want up-to-date info, here's a live OCR Correction Ticker .","title":"Activity data"},{"location":"umdigitalstudio2019/#unpacking-interfaces-exploring-data-thinking-critically","text":"It's rarely the case that we can just download and use GLAM data. Even when the data is openly-licensed, well-structured, and easy to access, we still have to spend time getting to know it. What are its limits, and its assumptions? Many of the notebooks in the GLAM Workbench are intended to help with this exploratory phase. Before you can use GLAM data, you have to play with it. From here on we're going to working with live Jupyter notebooks running on Binder. In actual fact, you already have, because QueryPic and the CSV Explorer were also live notebooks, it's just that they were running with all the code hidden. But now you'll get a chance to see and change the code. Jupyter notebooks quick start Don't panic! Jupyter notebooks are meant to encourage experimentation, so don't feel nervous. Just try running a few cells and see what happens! Here's some hints: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it's a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you'll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you've finished editing.","title":"Unpacking interfaces, exploring data, &amp; thinking critically"},{"location":"umdigitalstudio2019/#activity-3-trove-newspapers-over-time","text":"Let's go back again to QueryPic and explore the sorts of assumptions we need to unpack when we're exploring Trove's newspapers over time. Just click on the links below to launch the notebooks on Binder. Then just follow the instructions, hitting Shift+Enter on each cell to run it. You'll need to have your Trove API key ready to insert where indicated. Visualise Trove newspaper searches over time Visualise the total number of newspaper articles in Trove by year and state Hopefully these notebooks will give you a sense of how the nature of the digitised newspapers themselves can be critically analysed.","title":"Activity 3: Trove newspapers over time"},{"location":"umdigitalstudio2019/#activity-4-whats-in-the-national-museum-of-australia","text":"As I mentioned, you have to spend time playing around to understand both the limits and the possibilities of a new dataset. Recently the National Museum of Australia opened up access to its collection data through an API. Here's some notebooks that poke about inside... Note that I've pre-harvested the objects and places data. But before you start working on it, you'll need to follow the link to unzip the data file. Start by exploring object records Click on the 'unzip a pre-harvested dataset' link and run the cell in the linked notebook to prepare the data. Work through the 'Exploring object records' notebook. Think about the sorts of metadata that have been captured. When you've run all the cells, click on the link to the 'time' notebook at the top of the page. This notebook explores the temporal dimensions of the data. When were objects created or collected? There's a few puzzles along the way! When you've run all the cells, click on the link to 'explore places'. This notebook unpacks spatial information and creates some maps. What's missing from the NMA data? What other perspectives might we explore?","title":"Activity 4: What's in the National Museum of Australia?"},{"location":"umdigitalstudio2019/#activity-5-exploring-the-text-of-troves-journals","text":"What sort of patterns can we observe across the issues of a journal? Let's try working with some unstructured text. Apologies in advance \u2013 this notebook is hot off the presses and is missing a lot of documentation. But just work it using the trusty ol' Shift+Enter and you should see some magic happen. You'll need to select a journal from the dropdown list and then download the OCRd text from the repository on CloudStor. For this exercise, I'd suggest you start by selecting something that doesn't have more than a few hundred issues \u2013 Art in Australia for example. Word frequencies in digitised journals","title":"Activity 5: Exploring the text of Trove's journals"},{"location":"umdigitalstudio2019/#lunch-break","text":"","title":"Lunch break"},{"location":"umdigitalstudio2019/#handy-hints-hacks","text":"Sometimes things that should be easy, aren't. This applies to GLAM collections as much as it does to everything else in life. But at least in the case of GLAM collections we can build and share hacks and hints to overcome at least some of the frustrations.","title":"Handy hints &amp; hacks"},{"location":"umdigitalstudio2019/#activity-6-get-an-image-of-a-newspaper-article-in-trove","text":"You can download just dowload a newspaper article from the Trove web interface can't you? Well, sort of. You can generate an HTML page which embeds images of the article, although they might be sliced up in strange ways that make them almost unreadable. Sigh... It's often easier just to do a screen capture. But here's an alternative. Find an article in Trove. For bonus points find an illustrated article spread over more than one page. Click here When the page has loaded, paste the article url into the box. Hit the Get Images button! Download your image. You might have guessed that this little app is another Jupyter notebook in disguise. Here's a few other apps/notebooks that you might find handy for getting or moving data. Apps: Create a thumbnail image from a Trove newspaper article Save a Trove newspaper page as a (high-res) image Download the contents of a digitised file from RecordSearch Notebooks: Upload Trove newspaper articles to Omeka-S Convert a Trove list into a CSV file","title":"Activity 6: Get an image of a newspaper article in Trove"},{"location":"umdigitalstudio2019/#first-catch-your-data","text":"One thing we haven't really covered yet is where you find GLAM data and how do you get it in a usable form. As a starting point, I've created a list of GLAM data sources . But as I suggested earlier, access is not a straightforward thing \u2013 just because data is online, doesn't mean it's easy to download or use. Here's some of the real world categories of GLAM data that we often have to deal with: Machine-readable but coding required for download Downloadable but not easily findable Structured but not downloadable Downloadable but in many separate pieces","title":"First catch your data"},{"location":"umdigitalstudio2019/#machine-readable-but-coding-required-for-download","text":"Trove makes a lot (but not all!) of its collection data available via an API (Application Programming Interface). So do other GLAM institutions like the NMA and the Museums Victoria. I love APIs, but to get data out of them you need to be able to code. How do we fill the gap between APIs and the researchers who want to use the data they provide? Here's some notebooks that provide an interface between you and the API, making it easy to explore and download the data you're interested in. Trove newspaper harvester Harvest records from the NMA API DigitalNZ \u2013 New Zealand's Trove! Te Papa collection API","title":"Machine-readable but coding required for download"},{"location":"umdigitalstudio2019/#downloadable-but-not-easily-findable","text":"I've already mentioned that there's lots of GLAM datasets available through the data.gov.au portal, but who would have known it? In total there's 948 datasets from 22 organisations . Some of these are more interesting than others, but there\u2019s some really rich historical data amongst them. However, unless you know where to look they can be hard to find. Datasets are often buried deep in organisational websites, and while data.gov.au has a good search interface, how many people would think to look there for data from libraries?","title":"Downloadable but not easily findable"},{"location":"umdigitalstudio2019/#structured-but-not-downloadable","text":"Other than Trove, the collection I\u2019ve spent most time wrestling with is that of the National Archives of Australia. The NAA\u2019s online database, RecordSearch , doesn\u2019t provide an API or download options, so to get data out you have to resort to a process known as screen-scraping \u2014 you have to write some code to extract the data you want from web pages. However, the data in RecordSearch is really rich and complex, so it\u2019s worth the effort. Another example in this category are the indexes to records created by the NSW State Archives and their volunteers. These indexes include things like names, places, and dates, and provide useful entry points into the records. But they\u2019re also rich datasets. The indexes are openly licensed, they\u2019re online, they\u2019re searchable, but there\u2019s no way of downloading them except by screen scraping. So I\u2019ve harvested all the indexes and made them available as CSV files . Screen scrapers are kludgey and prone to breakage, but sometimes it't the only way to get useable data out of GLAM websites. Here's some more examples: Harvest a series from RecordSearch Harvesting functions from the RecordSearch interface Harvesting a records search from Archway (Archives of New Zealand) Harvest the Library and Archives Canada naturalisation database, 1915-1946, by country","title":"Structured but not downloadable"},{"location":"umdigitalstudio2019/#downloadable-but-in-many-separate-pieces","text":"As I mentioned early on, when you work with collections as data you can play around with scale, zooming out from a single instance to look for patterns across thousands of files. But this is difficult if you\u2019re only option is to manually download one file at a time. To make useful data our of a collection, sometimes you just have to bring the pieces together. Commonwealth Hansard is an example of this. Sitting underneath the ParlInfo search interface are a lot of well-structured XML files, one for each sitting day in the Senate and House of Reps. But you can only download them manually \u2014 one at a time. I\u2019ve harvested all the XML files from 1901 to 1980 and saved them to a separate repository. All those separate files have become a corpus for large-scale text analysis. The same goes for the transcripts of more than 20,000 speeches, media releases, and interviews by Australian Prime Ministers made available online by the Department of Prime Minister and Cabinet. They're online, they have metadata, and they're downloadable as XML files. But by harvesting them and making them easily downloadable in bulk, we change what's possible. Trove\u2019s API is great, but there\u2019s some important data you can\u2019t get through it \u2013 the content of Trove\u2019s digitised journals , for example, can only be accessed through the web interface. I\u2019ve created a series of notebooks to help you extract metadata, text, and digitised page images from the journals. To do this I've supplemented what's available through the API with extra data from screen scraping, and some reverse engineering of the web interface. I've done the same for Trove's digitised books .","title":"Downloadable but in many separate pieces"},{"location":"umdigitalstudio2019/#finding-pathways","text":"How do we start to put all this bits and pieces together in a way that allows researchers to pursue the questions that interest them across collections? Let's experiment with one possible pathway.","title":"Finding pathways"},{"location":"umdigitalstudio2019/#activity-7-zoom-out-zoom-in","text":"First go back once again to the QueryPic visualisation of newspaper searches over time. But this time, I want you to play around with search terms until you find something you think might be worth exploring further. Perhaps there's an unexpected peak, or an interesting shift in usage between particular words. Once you have identified your feature of interest, head to the Trove web interface and construct a search that focuses on that feature. For example, you might use the facets to filter your search by year, state, newspaper, or article type. For the purposes of this exercise, you want to limit the number of results in your search to a few thousand at most. (This is just so you don't spend the rest of the workshop waiting for your 253,000 newspaper articles to download...) Copy the url of your Trove search. Now we're going to feed your search url to the Trove Newspaper Harvester . Once again, you'll need your Trove API key . Just paste your API query and your search url into the boxes where indicated. Run the cell that says %run -m troveharvester -- start $query $api_key --text to start your harvest. The Trove Newspaper Harvester is actually a command line tool, but I've embedded in this notebook to make it easier to use. There's also an app-ified version that makes it even easier! The Newspaper Harvester downloads both the metadata about the articles in your search, and the OCRd text of each individual article. This means that we can explore the content of the articles in depth. It's worth remembering too that the Trove web interface only lets you see the first 2,000 articles in your search. The Newspaper Harvester can download many thousands of articles. I recently harvested more than half a million articles to explore the changing context of the words 'aliens' and 'immigrants' . The notebook includes information on the structure of the metadata file. Once your harvest is finished, click on the link at the bottom of the notebook that says 'Exploring your TroveHarvester data' to load a new notebook. This notebook tries analysing the metadata file you just harvested in a number of different ways. It breaks the results down by newspaper, by time, and by place of publication. It also looks at the frequencies of words in the article titles. It's in need of an overhaul, but it's a useful starting point. Once you've got some sense of the harvested metadata, it's time to look at the text contents of the articles themselves. Click on the link at the bottom of the notebook to explore word frequencies in the text content of the articles. This notebook calculates word frequencies for each article and then aggregates these counts by year. You can also explore how the frequency of particular words change over time. Under 'Visualise word frequencies over time' there's a cell where you can enter words that you want to investigate. Visualise the results as both a facet chart and a bubbleline chart. Once you're ready to move on, click on the link at the bottom to open a notebook that calculates TF-IDF values for words in the articles. TF-IDF scores are calculated by comparing the number of time a word appears in a single document to the number of times it appears in a collection of documents. Word frequencies point us to 'common' words, TF-IDF can indicate 'significant' words. This notebook aggregates the articles by year and then calculates TF-IDF values for each word in each year. Once again you can explore how the TF-IDF values of specific words change over time. So, did you find anything interesting? The point of this exercise was not to define a specific methodology, but to suggest different ways you might pursue a set of research questions through a number of shifts in scale. Of course there are many other things we could do to explore the article text (and more will be added to the GLAM Workbench in time). We could for example look for clusters amongst the articles by using topic modelling. Here's a great notebook by Adel Rahmani that explores topic modelling of Australian parliamentary press releases harvested via Trove.","title":"Activity 7: Zoom out, zoom in"},{"location":"umdigitalstudio2019/#suggestions-welcome","text":"If you have suggestions for data sources, tools, or examples to add the GLAM Workbench, feel free to add them via GitHub .","title":"Suggestions welcome"},{"location":"umdigitalstudio2019/#additional-readings-and-resources","text":"Here's a few sites and resources that came up in discussion: Explore Trove's digitised journals Hansard interjections as tweets Trove tips & tricks Digital Humanities Slack \u2013 Join me in the #dh-australia channel!","title":"Additional readings and resources"}]}