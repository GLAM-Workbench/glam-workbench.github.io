{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the wonderful world of GLAM data! \u00b6 Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand. What is GLAM data? \u00b6 When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets. What can I do with GLAM data? \u00b6 Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data. Do I need to be able to code? \u00b6 No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code. What is Jupyter? \u00b6 ~~More detail required~~ Where do I start? \u00b6 ~~More detail required~~ Other GLAM related notebooks \u00b6 Awesome Jupyter GLAM Getting started with ODate","title":"Home"},{"location":"#welcome-to-the-wonderful-world-of-glam-data","text":"Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand.","title":"Welcome to the wonderful world of GLAM data!"},{"location":"#what-is-glam-data","text":"When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets.","title":"What is GLAM data?"},{"location":"#what-can-i-do-with-glam-data","text":"Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data.","title":"What can I do with GLAM data?"},{"location":"#do-i-need-to-be-able-to-code","text":"No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code.","title":"Do I need to be able to code?"},{"location":"#what-is-jupyter","text":"~~More detail required~~","title":"What is Jupyter?"},{"location":"#where-do-i-start","text":"~~More detail required~~","title":"Where do I start?"},{"location":"#other-glam-related-notebooks","text":"Awesome Jupyter GLAM Getting started with ODate","title":"Other GLAM related notebooks"},{"location":"about/","text":"This is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions. It's aimed at researchers in the humanities, but will include examples and tutorials of more general interest. Over the past decade I've created and shared a wide variety of digital tools, examples, tutorials, and datasets. Some like QueryPic and TroveHarvester are fairly polished and well documented. Others are just fragments of code. All of them are intended to support research into our rich cultural collections. But even though something like the TroveHarvester is pretty easy to use, it does require a bit of set-up, and I've been very aware that this can be a barrier to people starting their explorations. I created the dhistory site many years ago to provide the foundation for a digital workbench, but I couldn't quite achieve what I wanted \u2014 tools that were easy to use and required minimal setup, but also tools that exposed their own workings, that inspired novice users to question and to tinker. So here we are. My plan is to use Jupyter , GitHub, and Binder to bring together all those tools, examples, tutorials, and datasets in a way that supports people's explorations through digital GLAM collections. I'm really excited, for example, that I can create a notebook that provides a deconstructed (or perhaps see-through) version of QueryPic \u2014 that enables you to build, step by step, the same sort of visualisations, while learning about how it works. And at the end you can download the results as a CSV for further analysis. I love the way that Jupyter notebooks combine learning with real, live, digital tools and methods. You don't have to read a tutorial then go away and try to follow the instructions on your own. It's all together. Live code. Real research. Active learning. Like most of my projects this is in itself an experiment. I'm still learning what's possible and what works. But I'm hopeful. If you think this project is worthwhile, you might like to support me on Patreon .","title":"Some background"},{"location":"archway/","text":"Archway \u00b6 Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 Harvesting a records search \u00b6 This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 10,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 10,000 \u2014 the notebook provides some examples of how you might do this. Download from GitHub View using NBViewer Run live on Binder Harvesting functions \u00b6 Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset. Download from GitHub View using NBViewer Run live on Binder Who's responsible? \u00b6 Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode","title":"Archway"},{"location":"archway/#archway","text":"Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Archway"},{"location":"archway/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"archway/#harvesting-a-records-search","text":"This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 10,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 10,000 \u2014 the notebook provides some examples of how you might do this. Download from GitHub View using NBViewer Run live on Binder","title":"Harvesting a records search"},{"location":"archway/#harvesting-functions","text":"Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset. Download from GitHub View using NBViewer Run live on Binder","title":"Harvesting functions"},{"location":"archway/#whos-responsible","text":"Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode","title":"Who's responsible?"},{"location":"csv-explorer/","text":"GLAM CSV Explorer \u00b6 Cultural institutions are making collection data available as machine readable downloads. But how can researchers explore the shape and meaning of this data? How do they know what types of questions they can ask? This notebook provides a quick overview of CSV-formatted data files, particularly those created by GLAM institutions (galleries, libraries, archives, and museums). You can select one of more than 400 GLAM datasets shared on data.gov.au or upload your own. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"GLAM CSV Explorer"},{"location":"csv-explorer/#glam-csv-explorer","text":"Cultural institutions are making collection data available as machine readable downloads. But how can researchers explore the shape and meaning of this data? How do they know what types of questions they can ask? This notebook provides a quick overview of CSV-formatted data files, particularly those created by GLAM institutions (galleries, libraries, archives, and museums). You can select one of more than 400 GLAM datasets shared on data.gov.au or upload your own. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"GLAM CSV Explorer"},{"location":"digitalnz/","text":"DigitalNZ aggregates collections from across New Zealand and makes the aggregated metadata available through an API . You'll need an API key to work with DigitalNZ data. Tips, tools, and examples \u00b6 Getting some top-level data from the DigitalNZ API \u00b6 This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Download from GitHub View using NBViewer Run live on Binder Find results by country in DigitalNZ \u00b6 Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Download from GitHub View using NBViewer Run live on Binder Visualise a search in Papers Past \u00b6 Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Download from GitHub View using NBViewer Run live on Binder Harvest data from Papers Past \u00b6 This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles. Download from GitHub View using NBViewer Run live on Binder","title":"DigitalNZ"},{"location":"digitalnz/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"digitalnz/#getting-some-top-level-data-from-the-digitalnz-api","text":"This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Download from GitHub View using NBViewer Run live on Binder","title":"Getting some top-level data from the DigitalNZ API"},{"location":"digitalnz/#find-results-by-country-in-digitalnz","text":"Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Download from GitHub View using NBViewer Run live on Binder","title":"Find results by country in DigitalNZ"},{"location":"digitalnz/#visualise-a-search-in-papers-past","text":"Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Download from GitHub View using NBViewer Run live on Binder","title":"Visualise a search in Papers Past"},{"location":"digitalnz/#harvest-data-from-papers-past","text":"This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles. Download from GitHub View using NBViewer Run live on Binder","title":"Harvest data from Papers Past"},{"location":"facial-detection/","text":"Finding faces in the Tribune negatives \u00b6 Exploring mages from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Finding faces in the Tribune collection \u00b6 Using OpenCV for basic facial detection. Finding all the faces in the Tribune collection \u00b6 This notebook runs a facial detection script across the whole Tribune collection. It saves cropped versions of all the detected faces, and creates a data file recording the number of faces detected per image. Exploring the face data \u00b6 This notebook plays around with the data generated by running the facial detection script over the whole Tribune collection. Make all the faces into one big image \u00b6 Having extracted lots and lots of faces from the Tribune photos, I thought I'd combine them in one big image. Interesting apps \u00b6 Focus on faces \u00b6 A little app that fades from faces to photos...","title":"Facial detection"},{"location":"facial-detection/#finding-faces-in-the-tribune-negatives","text":"Exploring mages from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Finding faces in the Tribune negatives"},{"location":"facial-detection/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"facial-detection/#finding-faces-in-the-tribune-collection","text":"Using OpenCV for basic facial detection.","title":"Finding faces in the Tribune collection"},{"location":"facial-detection/#finding-all-the-faces-in-the-tribune-collection","text":"This notebook runs a facial detection script across the whole Tribune collection. It saves cropped versions of all the detected faces, and creates a data file recording the number of faces detected per image.","title":"Finding all the faces in the Tribune collection"},{"location":"facial-detection/#exploring-the-face-data","text":"This notebook plays around with the data generated by running the facial detection script over the whole Tribune collection.","title":"Exploring the face data"},{"location":"facial-detection/#make-all-the-faces-into-one-big-image","text":"Having extracted lots and lots of faces from the Tribune photos, I thought I'd combine them in one big image.","title":"Make all the faces into one big image"},{"location":"facial-detection/#interesting-apps","text":"","title":"Interesting apps"},{"location":"facial-detection/#focus-on-faces","text":"A little app that fades from faces to photos...","title":"Focus on faces"},{"location":"getting-started/","text":"Using Jupyter notebooks \u00b6 Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing. Viewing notebooks on NBViewer \u00b6 The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar. Running notebooks live on Binder \u00b6 Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session. Running notebooks in \u2018app mode\u2019 \u00b6 Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required. Running notebooks on your own computer \u00b6 If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Getting started"},{"location":"getting-started/#using-jupyter-notebooks","text":"Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing.","title":"Using Jupyter notebooks"},{"location":"getting-started/#viewing-notebooks-on-nbviewer","text":"The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar.","title":"Viewing notebooks on NBViewer"},{"location":"getting-started/#running-notebooks-live-on-binder","text":"Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session.","title":"Running notebooks live on Binder"},{"location":"getting-started/#running-notebooks-in-app-mode","text":"Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required.","title":"Running notebooks in &lsquo;app mode&rsquo;"},{"location":"getting-started/#running-notebooks-on-your-own-computer","text":"If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Running notebooks on your own computer"},{"location":"glam-data-portals/","text":"GLAM data from government portals \u00b6 This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. You can visualise the contents of the CSV datasets I've harvested by using the GLAM CSV Explorer . Tools, tips, and examples \u00b6 Harvesting GLAM data from government portals \u00b6 This notebook attempts to harvest the details of GLAM datasets from state and national data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. It also attempts some analysis of the results. Harvest GLAM datasets from data.gov.au \u00b6 This is a quick attempt to harvest datasets published by GLAM institutions using the new data.gov.au API. To create the list of organisations, I searched the organisations on the data.gov.au site for 'library', 'archives', 'records', and 'museum'. Data \u00b6 Results (July 2019) \u00b6 Human readable list of all GLAM datasets harvested from data.gov.au GLAM datasets from data.gov.au \u2013 all formats (CSV) GLAM datasets from data.gov.au \u2013 CSVs only (CSV) Datasets by format: CSV 586 XML 81 JSON 74 XLSX 61 DOCX 34 HTML 33 PLAIN 14 ZIP 14 API 9 GEOJSON 8 DATA 6 OTHER 4 PDF 4 KML 3 RSS 2 JPEG 2 RDF 1 HMTL 1 WFS 1 TXT 1 APP 1 JAVASCRIPT 1 CSS 1 WMS 1 PAGE 1 Datasets by organisation: State Library of Queensland 204 Queensland State Archives 172 State Library of Western Australia 147 State Library of South Australia 140 Libraries Tasmania 71 State Records 41 PROV Public Record Office 33 South Australian Museum 33 State Library of New South Wales 21 NSW State Archives 19 History Trust of South Australia 19 State Records Office of Western Australia 7 State Library of NSW 6 Western Australian Museum 6 National Library of Australia 5 State Library of Victoria 5 Australian Museum 4 Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS) 3 National Archives of Australia 3 Museum of Applied Arts and Sciences 3 Mount Gambier Library 2 Tasmanian Museum and Art Gallery 2 National Portrait Gallery 2 Datasets by licence: Creative Commons Attribution 250 Creative Commons Attribution 3.0 Australia 244 Creative Commons Attribution 4.0 237 Creative Commons Attribution 4.0 International 146 Creative Commons Attribution 2.5 Australia 32 Creative Commons Attribution-NonCommercial 10 Other (Open) 5 notspecified 5 Creative Commons Attribution Share-Alike 4.0 3 Creative Commons Attribution 3.0 3 Creative Commons Attribution Non-Commercial 4.0 2 Custom (Other) 1 Results (April 2018) \u00b6 Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Gov data portals"},{"location":"glam-data-portals/#glam-data-from-government-portals","text":"This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. You can visualise the contents of the CSV datasets I've harvested by using the GLAM CSV Explorer .","title":"GLAM data from government portals"},{"location":"glam-data-portals/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"glam-data-portals/#harvesting-glam-data-from-government-portals","text":"This notebook attempts to harvest the details of GLAM datasets from state and national data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. It also attempts some analysis of the results.","title":"Harvesting GLAM data from government portals"},{"location":"glam-data-portals/#harvest-glam-datasets-from-datagovau","text":"This is a quick attempt to harvest datasets published by GLAM institutions using the new data.gov.au API. To create the list of organisations, I searched the organisations on the data.gov.au site for 'library', 'archives', 'records', and 'museum'.","title":"Harvest GLAM datasets from data.gov.au"},{"location":"glam-data-portals/#data","text":"","title":"Data"},{"location":"glam-data-portals/#results-july-2019","text":"Human readable list of all GLAM datasets harvested from data.gov.au GLAM datasets from data.gov.au \u2013 all formats (CSV) GLAM datasets from data.gov.au \u2013 CSVs only (CSV) Datasets by format: CSV 586 XML 81 JSON 74 XLSX 61 DOCX 34 HTML 33 PLAIN 14 ZIP 14 API 9 GEOJSON 8 DATA 6 OTHER 4 PDF 4 KML 3 RSS 2 JPEG 2 RDF 1 HMTL 1 WFS 1 TXT 1 APP 1 JAVASCRIPT 1 CSS 1 WMS 1 PAGE 1 Datasets by organisation: State Library of Queensland 204 Queensland State Archives 172 State Library of Western Australia 147 State Library of South Australia 140 Libraries Tasmania 71 State Records 41 PROV Public Record Office 33 South Australian Museum 33 State Library of New South Wales 21 NSW State Archives 19 History Trust of South Australia 19 State Records Office of Western Australia 7 State Library of NSW 6 Western Australian Museum 6 National Library of Australia 5 State Library of Victoria 5 Australian Museum 4 Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS) 3 National Archives of Australia 3 Museum of Applied Arts and Sciences 3 Mount Gambier Library 2 Tasmanian Museum and Art Gallery 2 National Portrait Gallery 2 Datasets by licence: Creative Commons Attribution 250 Creative Commons Attribution 3.0 Australia 244 Creative Commons Attribution 4.0 237 Creative Commons Attribution 4.0 International 146 Creative Commons Attribution 2.5 Australia 32 Creative Commons Attribution-NonCommercial 10 Other (Open) 5 notspecified 5 Creative Commons Attribution Share-Alike 4.0 3 Creative Commons Attribution 3.0 3 Creative Commons Attribution Non-Commercial 4.0 2 Custom (Other) 1","title":"Results (July 2019)"},{"location":"glam-data-portals/#results-april-2018","text":"Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Results (April 2018)"},{"location":"hansard/","text":"The proceedings of Australia's Commonwealth Parliament are recorded in Hansard, which is available online through the Parliamentary Library's ParlInfo database. This repository includes Jupyter notebooks to harvest and explore XML formatted versions of Hansard. You can access a full harvest of the XML files for both houses between 1901 and 1980 from this repository. The XML files are made available on the Australian Parliament website under a CC-BY-NC-ND licence. Tools, tips, and examples \u00b6 Harvesting Commonwealth Hansard \u00b6 Results in ParlInfo are generated from well-structured XML files which can be downloaded individually from the web interface \u2013 one XML file for each sitting day. This notebook shows you how to download all the XML files for large scale analysis. It's an updated version of the code I used to harvest Hansard in 2016.","title":"Commonwealth Hansard"},{"location":"hansard/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"hansard/#harvesting-commonwealth-hansard","text":"Results in ParlInfo are generated from well-structured XML files which can be downloaded individually from the web interface \u2013 one XML file for each sitting day. This notebook shows you how to download all the XML files for large scale analysis. It's an updated version of the code I used to harvest Hansard in 2016.","title":"Harvesting Commonwealth Hansard"},{"location":"image-tagging/","text":"Experimenting with automated image tagging in the Tribune negatives \u00b6 Exploring images from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Basic image recognition with Tribune photos \u00b6 Trying Inception-v3's pre-trained model on some Tribune photos. Tensorflow for poets \u00b6 Notebook to accompany the Tensorflow for poets tutorial Training a classification model for the Tribune \u00b6 Using the 'Tensorflow for poets' example to create our own model. Classifying all the Tribune images \u00b6 Using the simple model we created in the previous notebook, let's attempt to classify all the images in the Tribune collection.","title":"Image tagging"},{"location":"image-tagging/#experimenting-with-automated-image-tagging-in-the-tribune-negatives","text":"Exploring images from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Experimenting with automated image tagging in the Tribune negatives"},{"location":"image-tagging/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"image-tagging/#basic-image-recognition-with-tribune-photos","text":"Trying Inception-v3's pre-trained model on some Tribune photos.","title":"Basic image recognition with Tribune photos"},{"location":"image-tagging/#tensorflow-for-poets","text":"Notebook to accompany the Tensorflow for poets tutorial","title":"Tensorflow for poets"},{"location":"image-tagging/#training-a-classification-model-for-the-tribune","text":"Using the 'Tensorflow for poets' example to create our own model.","title":"Training a classification model for the Tribune"},{"location":"image-tagging/#classifying-all-the-tribune-images","text":"Using the simple model we created in the previous notebook, let's attempt to classify all the images in the Tribune collection.","title":"Classifying all the Tribune images"},{"location":"lac/","text":"Library Archives Canada \u00b6 Unfortunately Library Archives Canada doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 LAC naturalisation database, 1915-1946 \u2014 harvest by country \u00b6 This notebook helps you extract the records of people from a specific country from the LAC 1915-1946 naturalisations database . There are numerous limitations and problems, including the fact that you can only get the first 2000 results. Women and children are not returned in a search by country, so the notebook makes an attempt to find and add them to the initial harvest. Here's an example harvest for 'China' .","title":"Library Archives Canada"},{"location":"lac/#library-archives-canada","text":"Unfortunately Library Archives Canada doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Library Archives Canada"},{"location":"lac/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"lac/#lac-naturalisation-database-1915-1946-harvest-by-country","text":"This notebook helps you extract the records of people from a specific country from the LAC 1915-1946 naturalisations database . There are numerous limitations and problems, including the fact that you can only get the first 2000 results. Women and children are not returned in a search by country, so the notebook makes an attempt to find and add them to the initial harvest. Here's an example harvest for 'China' .","title":"LAC naturalisation database, 1915-1946 \u2014 harvest by country"},{"location":"naa-asio/","text":"Series in the National Archives of Australia with content recorded by the Australian Security Intelligence Organisation (ASIO) \u00b6 This repository includes item-level metadata from all 18 series in the National Archives of Australia that are listed on RecordSearch as including content recorded by ASIO. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory. Tools, tips, and examples \u00b6 Harvesting series \u00b6 Code to harvest item-level data and digitised images from series. Analyse a series \u00b6 Some ways of exploring and visualising the item level data. Summarise series data \u00b6 Summarise the data from all harvested series. Summarise series \u00b6 Summarise the data from a single series. Data \u00b6 Harvest May 2018 \u00b6 To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"ASIO archives"},{"location":"naa-asio/#series-in-the-national-archives-of-australia-with-content-recorded-by-the-australian-security-intelligence-organisation-asio","text":"This repository includes item-level metadata from all 18 series in the National Archives of Australia that are listed on RecordSearch as including content recorded by ASIO. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory.","title":"Series in the National Archives of Australia with content recorded by the Australian Security Intelligence Organisation (ASIO)"},{"location":"naa-asio/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"naa-asio/#harvesting-series","text":"Code to harvest item-level data and digitised images from series.","title":"Harvesting series"},{"location":"naa-asio/#analyse-a-series","text":"Some ways of exploring and visualising the item level data.","title":"Analyse a series"},{"location":"naa-asio/#summarise-series-data","text":"Summarise the data from all harvested series.","title":"Summarise series data"},{"location":"naa-asio/#summarise-series","text":"Summarise the data from a single series.","title":"Summarise series"},{"location":"naa-asio/#data","text":"","title":"Data"},{"location":"naa-asio/#harvest-may-2018","text":"To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"Harvest May 2018"},{"location":"naa-wap/","text":"Series in the National Archives of Australia relating to the White Australia Policy \u00b6 This repository includes item-level metadata from 23 series harvested from the National Archives online database RecordSearch. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory. Tools, tips, and examples \u00b6 Harvesting series \u00b6 Code to harvest item-level data and digitised images from series. Analyse a series \u00b6 Some ways of exploring and visualising the item level data. Summarise series data \u00b6 Summarise the data from all harvested series. Summarise series \u00b6 Summarise the data from a single series. Data \u00b6 Harvest May 2018 \u00b6 To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"White Australia Policy archives"},{"location":"naa-wap/#series-in-the-national-archives-of-australia-relating-to-the-white-australia-policy","text":"This repository includes item-level metadata from 23 series harvested from the National Archives online database RecordSearch. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory.","title":"Series in the National Archives of Australia relating to the White Australia Policy"},{"location":"naa-wap/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"naa-wap/#harvesting-series","text":"Code to harvest item-level data and digitised images from series.","title":"Harvesting series"},{"location":"naa-wap/#analyse-a-series","text":"Some ways of exploring and visualising the item level data.","title":"Analyse a series"},{"location":"naa-wap/#summarise-series-data","text":"Summarise the data from all harvested series.","title":"Summarise series data"},{"location":"naa-wap/#summarise-series","text":"Summarise the data from a single series.","title":"Summarise series"},{"location":"naa-wap/#data","text":"","title":"Data"},{"location":"naa-wap/#harvest-may-2018","text":"To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"Harvest May 2018"},{"location":"nsw-state-archives/","text":"NSW State Archives \u00b6 A collection of notebooks for working with collection data from the NSW State Archives . State Archives content in copyright is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence . See their copyright page for more information. Tools, tips, and examples \u00b6 Get details of online indexes \u00b6 This notebook scrapes details of available indexes from the NSW State Archives A to Z list of online indexes . It saves the results as a CSV formatted file. Download from GitHub View using NBViewer Run live on Binder Harvest online indexes \u00b6 This notebook harvests data from all of NSW State Archives online indexes , saving the data as a collection of easily downloadable CSV files. Download from GitHub View using NBViewer Run live on Binder Summarise index details \u00b6 This notebook counts the number of rows in each index and calculates the total for the whole repository. It formats the results in nice HTML and Markdown tables for easy browsing. Download from GitHub View using NBViewer Run live on Binder NSW State Archives Index Explorer \u00b6 NSW State Archives provides a lot of rich descriptive data in its online indexes. But there's so much data it can be hard to understand what's actually in each index. This notebook tries to help by generating an overview of an index, summarising the contents of each field. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode Data \u00b6 NSW State Archives Online Indexes \u00b6 Harvested: July 2019 This is a repository of data harvested from NSW State Archives online indexes. Data from each index has been extracted from the web interface and saved as a single CSV-formatted file for easy download. View repository on GitHub Currently: 64 indexes harvested with 1,499,259 rows of data. Title Status Number of rows Download data View at NSWSA More info Assisted Immigrants Fully digitised 191688 CSV file Browse index More info Australian Railway Supply Detachment Fully digitised 65 CSV file Browse index More info Bankruptcy Index Not digitised 28880 CSV file Browse index More info Bench of Magistrates cases, 1788-1820 Not digitised 4442 CSV file Browse index More info Botanic Gardens and Government Domains Employees Index Not digitised 916 CSV file Browse index More info Bubonic Plague Index Fully digitised 592 CSV file Browse index More info CSreLand Not digitised 10849 CSV file Browse index More info Child Care and Protection Not digitised 21980 CSV file Browse index More info Closer Settlement Transfer Registers, NRS 8082 Not digitised 4957 CSV file Browse index More info Closer and Soldier Settlement Transfer Files Not digitised 9656 CSV file Browse index More info Colonial Secretary Main series of letters received,1826-1982 Not digitised 7638 CSV file Browse index More info Convict Index Not digitised 141854 CSV file Browse index More info Convicts Applications to Marry 1825-51 Not digitised 8456 CSV file Browse index More info Coroners Inquests 1796-1824 Not digitised 808 CSV file Browse index More info Court of Civil Jurisdiction index Not digitised 2876 CSV file Browse index More info Crew (and Passenger) Lists, 1828-1841 Fully digitised 2560 CSV file Browse index More info Criminal Court Records index 1788-1833 Not digitised 5028 CSV file Browse index More info Criminal Indictments, 1863-1919 Not digitised 15701 CSV file Browse index More info Deceased Estates Not digitised 257524 CSV file Browse index More info Depasturing Licenses Not digitised 7449 CSV file Browse index More info Devonshire Street Cemetery Reinterment Index, 1901 Not digitised 9559 CSV file Browse index More info Divorce Index Not digitised 21239 CSV file Browse index More info Early Convict Index Fully digitised 12933 CSV file Browse index More info FieldBooks Not digitised 813 CSV file Browse index More info Government Architect Not digitised 2373 CSV file Browse index More info Government Asylums for the Infirm and Destitute Not digitised 10264 CSV file Browse index More info Governor\u2019s Court Case Papers, 1815-1824 Not digitised 3789 CSV file Browse index More info Index on Occupants on Aboriginal Reserves, 1875 to 1904 Not digitised 80 CSV file Browse index More info Index to 1841 Census Not digitised 9355 CSV file Browse index More info Index to Closer Settlement Promotion Not digitised 4354 CSV file Browse index More info Index to Court of Claims Not digitised 1051 CSV file Browse index More info Index to Deposition Registers Not digitised 65790 CSV file Browse index More info Index to Early Probate Records Not digitised 1627 CSV file Browse index More info Index to Gaol Photographs Fully digitised 48171 CSV file Browse index More info Index to Intestate Estate Case Papers Not digitised 22520 CSV file Browse index More info Index to Miscellaneous Immigrants Not digitised 8821 CSV file Browse index More info Index to Quarter Sessions cases, 1824-37 Not digitised 6232 CSV file Browse index More info Index to Registers of Firms Not digitised 45683 CSV file Browse index More info Index to Squatters and Graziers Not digitised 9003 CSV file Browse index More info Index to Vessels Arrived, 1837 - 1925 Not digitised 120083 CSV file Browse index More info Index to convict exiles, 1846-50 Not digitised 3036 CSV file Browse index More info Index to the Unassisted Arrivals NSW 1842-1855 Not digitised 135792 CSV file Browse index More info Indigenous Colonial Court Cases 1788-1838 Not digitised 66 CSV file Browse index More info Insolvency Index Not digitised 23108 CSV file Browse index More info King\u2019s and Queen\u2019s Counsel Appointments Fully digitised 2083 CSV file Browse index More info LandGrants Not digitised 5627 CSV file Browse index More info List of Maps and Plans (and Supplement) Not digitised 5455 CSV file Browse index More info NSW Chemists and Druggists Not digitised 2967 CSV file Browse index More info NSW Government Employees Granted Military Leave, 1914-1918 Not digitised 13735 CSV file Browse index More info NSW Govt Railways and Tramways - Roll of Honour - 1914-1919 Not digitised 1214 CSV file Browse index More info Naturalisation Not digitised 9860 CSV file Browse index More info Nominal Roll of the First Railway Section (AIF) Not digitised 417 CSV file Browse index More info Publicans Licenses Not digitised 18457 CSV file Browse index More info Railway Employment Records Not digitised 763 CSV file Browse index More info Register of Auriferous Leases Not digitised 53076 CSV file Browse index More info Registers of Nurses Not digitised 26665 CSV file Browse index More info Registers of Police Not digitised 11319 CSV file Browse index More info Registers of Settlement Purchases Not digitised 9776 CSV file Browse index More info Returned Soldier Settlement Loan Files Not digitised 7642 CSV file Browse index More info Returned Soldiers Settlement Misc files 1916-25 Not digitised 1050 CSV file Browse index More info Schools Not digitised 21246 CSV file Browse index More info Surveyor General - Letters received 1822-55 Not digitised 157 CSV file Browse index More info Teachers Rolls Not digitised 14867 CSV file Browse index More info Unemployed in Sydney 1866 Fully digitised 3222 CSV file Browse index More info","title":"NSW State Archives"},{"location":"nsw-state-archives/#nsw-state-archives","text":"A collection of notebooks for working with collection data from the NSW State Archives . State Archives content in copyright is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence . See their copyright page for more information.","title":"NSW State Archives"},{"location":"nsw-state-archives/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"nsw-state-archives/#get-details-of-online-indexes","text":"This notebook scrapes details of available indexes from the NSW State Archives A to Z list of online indexes . It saves the results as a CSV formatted file. Download from GitHub View using NBViewer Run live on Binder","title":"Get details of online indexes"},{"location":"nsw-state-archives/#harvest-online-indexes","text":"This notebook harvests data from all of NSW State Archives online indexes , saving the data as a collection of easily downloadable CSV files. Download from GitHub View using NBViewer Run live on Binder","title":"Harvest online indexes"},{"location":"nsw-state-archives/#summarise-index-details","text":"This notebook counts the number of rows in each index and calculates the total for the whole repository. It formats the results in nice HTML and Markdown tables for easy browsing. Download from GitHub View using NBViewer Run live on Binder","title":"Summarise index details"},{"location":"nsw-state-archives/#nsw-state-archives-index-explorer","text":"NSW State Archives provides a lot of rich descriptive data in its online indexes. But there's so much data it can be hard to understand what's actually in each index. This notebook tries to help by generating an overview of an index, summarising the contents of each field. Download from GitHub View using NBViewer Run live on Binder Run live on Binder in Appmode","title":"NSW State Archives Index Explorer"},{"location":"nsw-state-archives/#data","text":"","title":"Data"},{"location":"nsw-state-archives/#nsw-state-archives-online-indexes","text":"Harvested: July 2019 This is a repository of data harvested from NSW State Archives online indexes. Data from each index has been extracted from the web interface and saved as a single CSV-formatted file for easy download. View repository on GitHub Currently: 64 indexes harvested with 1,499,259 rows of data. Title Status Number of rows Download data View at NSWSA More info Assisted Immigrants Fully digitised 191688 CSV file Browse index More info Australian Railway Supply Detachment Fully digitised 65 CSV file Browse index More info Bankruptcy Index Not digitised 28880 CSV file Browse index More info Bench of Magistrates cases, 1788-1820 Not digitised 4442 CSV file Browse index More info Botanic Gardens and Government Domains Employees Index Not digitised 916 CSV file Browse index More info Bubonic Plague Index Fully digitised 592 CSV file Browse index More info CSreLand Not digitised 10849 CSV file Browse index More info Child Care and Protection Not digitised 21980 CSV file Browse index More info Closer Settlement Transfer Registers, NRS 8082 Not digitised 4957 CSV file Browse index More info Closer and Soldier Settlement Transfer Files Not digitised 9656 CSV file Browse index More info Colonial Secretary Main series of letters received,1826-1982 Not digitised 7638 CSV file Browse index More info Convict Index Not digitised 141854 CSV file Browse index More info Convicts Applications to Marry 1825-51 Not digitised 8456 CSV file Browse index More info Coroners Inquests 1796-1824 Not digitised 808 CSV file Browse index More info Court of Civil Jurisdiction index Not digitised 2876 CSV file Browse index More info Crew (and Passenger) Lists, 1828-1841 Fully digitised 2560 CSV file Browse index More info Criminal Court Records index 1788-1833 Not digitised 5028 CSV file Browse index More info Criminal Indictments, 1863-1919 Not digitised 15701 CSV file Browse index More info Deceased Estates Not digitised 257524 CSV file Browse index More info Depasturing Licenses Not digitised 7449 CSV file Browse index More info Devonshire Street Cemetery Reinterment Index, 1901 Not digitised 9559 CSV file Browse index More info Divorce Index Not digitised 21239 CSV file Browse index More info Early Convict Index Fully digitised 12933 CSV file Browse index More info FieldBooks Not digitised 813 CSV file Browse index More info Government Architect Not digitised 2373 CSV file Browse index More info Government Asylums for the Infirm and Destitute Not digitised 10264 CSV file Browse index More info Governor\u2019s Court Case Papers, 1815-1824 Not digitised 3789 CSV file Browse index More info Index on Occupants on Aboriginal Reserves, 1875 to 1904 Not digitised 80 CSV file Browse index More info Index to 1841 Census Not digitised 9355 CSV file Browse index More info Index to Closer Settlement Promotion Not digitised 4354 CSV file Browse index More info Index to Court of Claims Not digitised 1051 CSV file Browse index More info Index to Deposition Registers Not digitised 65790 CSV file Browse index More info Index to Early Probate Records Not digitised 1627 CSV file Browse index More info Index to Gaol Photographs Fully digitised 48171 CSV file Browse index More info Index to Intestate Estate Case Papers Not digitised 22520 CSV file Browse index More info Index to Miscellaneous Immigrants Not digitised 8821 CSV file Browse index More info Index to Quarter Sessions cases, 1824-37 Not digitised 6232 CSV file Browse index More info Index to Registers of Firms Not digitised 45683 CSV file Browse index More info Index to Squatters and Graziers Not digitised 9003 CSV file Browse index More info Index to Vessels Arrived, 1837 - 1925 Not digitised 120083 CSV file Browse index More info Index to convict exiles, 1846-50 Not digitised 3036 CSV file Browse index More info Index to the Unassisted Arrivals NSW 1842-1855 Not digitised 135792 CSV file Browse index More info Indigenous Colonial Court Cases 1788-1838 Not digitised 66 CSV file Browse index More info Insolvency Index Not digitised 23108 CSV file Browse index More info King\u2019s and Queen\u2019s Counsel Appointments Fully digitised 2083 CSV file Browse index More info LandGrants Not digitised 5627 CSV file Browse index More info List of Maps and Plans (and Supplement) Not digitised 5455 CSV file Browse index More info NSW Chemists and Druggists Not digitised 2967 CSV file Browse index More info NSW Government Employees Granted Military Leave, 1914-1918 Not digitised 13735 CSV file Browse index More info NSW Govt Railways and Tramways - Roll of Honour - 1914-1919 Not digitised 1214 CSV file Browse index More info Naturalisation Not digitised 9860 CSV file Browse index More info Nominal Roll of the First Railway Section (AIF) Not digitised 417 CSV file Browse index More info Publicans Licenses Not digitised 18457 CSV file Browse index More info Railway Employment Records Not digitised 763 CSV file Browse index More info Register of Auriferous Leases Not digitised 53076 CSV file Browse index More info Registers of Nurses Not digitised 26665 CSV file Browse index More info Registers of Police Not digitised 11319 CSV file Browse index More info Registers of Settlement Purchases Not digitised 9776 CSV file Browse index More info Returned Soldier Settlement Loan Files Not digitised 7642 CSV file Browse index More info Returned Soldiers Settlement Misc files 1916-25 Not digitised 1050 CSV file Browse index More info Schools Not digitised 21246 CSV file Browse index More info Surveyor General - Letters received 1822-55 Not digitised 157 CSV file Browse index More info Teachers Rolls Not digitised 14867 CSV file Browse index More info Unemployed in Sydney 1866 Fully digitised 3222 CSV file Browse index More info","title":"NSW State Archives Online Indexes"},{"location":"pm-transcripts/","text":"The Department of Prime Minister and Cabinet provides transcripts of more than 20,000 speeches, media releases, and interviews by Australian Prime Ministers. These transcripts can be searched online , and the underlying XML files can be downloaded using a simple API. This repository includes Jupyter notebooks for harvesting, indexing, analysing, and aggregating the transcripts. A full harvest of the XML files from the PM Transcripts site is available if you don't want to do it yourself. The XML files are made available by the Department of Prime Minister and Cabinet under a Creative Commons Attribution 3.0 Australia Licence. Tools, tips, and examples \u00b6 Harvest transcripts \u00b6 Harvest all the XML transcripts from the PMs Transcripts site. Create an index to the harvested files \u00b6 The XML files contain embedded metadata that includes the name of the prime minister, and the title and date of the transcript. This notebook extracts that metadata from the harvested files and creates a CSV formatted spreadsheet for easy analysis. It also demonstrates some ways of summarising and visualising the metadata. Aggregate transcripts \u00b6 Depending on how you want to analyse them, it can be useful to group the transcripts by prime minister. This notebook aggregates the transcripts in two ways: by extracting the text content of each XML file and combining them into one big text file, and by zipping up the original XML files. Data \u00b6 Index to transcripts \u00b6 CSV formatted file containing metadata extracted from the XML transcripts. The fields are: id \u2013 transcript id date \u2013 release date title pm \u2013 prime minister's name release_type \u2013 type of transcript (speech, interview, media release etc) subjects \u2013 subjects (not used very often) pdf \u2013 url for PDF version (if there is one) Note that the release_type and subjects fields are not used consistently. See the create an index to the harvested files for more analysis of the metadata. XML repository \u00b6 Harvested: 11 July 2019 All of the harvested XML files are available from this repository . In addition to the original XML files, there is: a single zip file for each prime minister containing all their XML transcripts; a single text file for each prime minister containing the text extracted from all of their XML transcripts.","title":"PMs Transcripts"},{"location":"pm-transcripts/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"pm-transcripts/#harvest-transcripts","text":"Harvest all the XML transcripts from the PMs Transcripts site.","title":"Harvest transcripts"},{"location":"pm-transcripts/#create-an-index-to-the-harvested-files","text":"The XML files contain embedded metadata that includes the name of the prime minister, and the title and date of the transcript. This notebook extracts that metadata from the harvested files and creates a CSV formatted spreadsheet for easy analysis. It also demonstrates some ways of summarising and visualising the metadata.","title":"Create an index to the harvested files"},{"location":"pm-transcripts/#aggregate-transcripts","text":"Depending on how you want to analyse them, it can be useful to group the transcripts by prime minister. This notebook aggregates the transcripts in two ways: by extracting the text content of each XML file and combining them into one big text file, and by zipping up the original XML files.","title":"Aggregate transcripts"},{"location":"pm-transcripts/#data","text":"","title":"Data"},{"location":"pm-transcripts/#index-to-transcripts","text":"CSV formatted file containing metadata extracted from the XML transcripts. The fields are: id \u2013 transcript id date \u2013 release date title pm \u2013 prime minister's name release_type \u2013 type of transcript (speech, interview, media release etc) subjects \u2013 subjects (not used very often) pdf \u2013 url for PDF version (if there is one) Note that the release_type and subjects fields are not used consistently. See the create an index to the harvested files for more analysis of the metadata.","title":"Index to transcripts"},{"location":"pm-transcripts/#xml-repository","text":"Harvested: 11 July 2019 All of the harvested XML files are available from this repository . In addition to the original XML files, there is: a single zip file for each prime minister containing all their XML transcripts; a single text file for each prime minister containing the text extracted from all of their XML transcripts.","title":"XML repository"},{"location":"qsa/","text":"Queensland State Archives \u00b6 Tools, tips, and examples \u00b6 Naturalisations, 1851-1904 \u2014 add series information \u00b6 The Naturalisations, 1851 to 1904 index is available from the Queensland Government data portal. It's not clear, however, that the index collates name entries from a number of different series, with a separate row for each name reference. This means that there can be multiple rows referring to the naturalisation of a single individual. This is obviously important to keep in mind if you're trying to analyse aggregate data relating to naturalisations in Queensland. This notebook adds series information to the original index so that you can filter the data by series.","title":"Queensland State Archives"},{"location":"qsa/#queensland-state-archives","text":"","title":"Queensland State Archives"},{"location":"qsa/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"qsa/#naturalisations-1851-1904-add-series-information","text":"The Naturalisations, 1851 to 1904 index is available from the Queensland Government data portal. It's not clear, however, that the index collates name entries from a number of different series, with a separate row for each name reference. This means that there can be multiple rows referring to the naturalisation of a single individual. This is obviously important to keep in mind if you're trying to analyse aggregate data relating to naturalisations in Queensland. This notebook adds series information to the original index so that you can filter the data by series.","title":"Naturalisations, 1851-1904 \u2014 add series information"},{"location":"quick-start/","text":"Is this thing on? \u00b6 You've followed a link to a Jupyter notebook and want to try running the code. But when you click a cell, or hit Shift+Enter , nothing happens. Why? Notebooks can be viewed as static (read-only) documents, or run as live (interactive) resources.","title":"Quick start"},{"location":"quick-start/#is-this-thing-on","text":"You've followed a link to a Jupyter notebook and want to try running the code. But when you click a cell, or hit Shift+Enter , nothing happens. Why? Notebooks can be viewed as static (read-only) documents, or run as live (interactive) resources.","title":"Is this thing on?"},{"location":"records-of-resistance/","text":"Tribune negatives \u00b6 Exploring metadata harvested from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Tribune collection metadata magic \u00b6 This notebook helps you explore metadata (and images) from the State Library of NSW's Tribune collection. Topics and descriptions \u00b6 Some more examples of analysing text descriptions and topic tags. Playing with places \u00b6 Cleaning and analysing place data created by students.","title":"Tribune metadata"},{"location":"records-of-resistance/#tribune-negatives","text":"Exploring metadata harvested from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Tribune negatives"},{"location":"records-of-resistance/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"records-of-resistance/#tribune-collection-metadata-magic","text":"This notebook helps you explore metadata (and images) from the State Library of NSW's Tribune collection.","title":"Tribune collection metadata magic"},{"location":"records-of-resistance/#topics-and-descriptions","text":"Some more examples of analysing text descriptions and topic tags.","title":"Topics and descriptions"},{"location":"records-of-resistance/#playing-with-places","text":"Cleaning and analysing place data created by students.","title":"Playing with places"},{"location":"recordsearch/","text":"RecordSearch \u00b6 RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping. Tools, tips, and examples \u00b6 Harvesting a series \u00b6 Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well. Harvest files with the access status of 'closed' \u00b6 The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how. Harvesting functions from the RecordSearch interface \u00b6 This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point. How many of the functions are actually used? \u00b6 In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used. Harvest agencies associated with all functions \u00b6 This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy. Useful apps \u00b6 These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button. Download the contents of a digitised file \u00b6 RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link. Get a list of agencies associated with a function \u00b6 RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function. Who's responsible \u00b6 The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function. DFAT Cable Finder \u00b6 If you ever need to find a file in the National Archives of Australia that contains a specific numbered cable from the Department of Foreign Affairs this is the tool for you! Just give it a cable number and it will look in the series listed below for a file that might contain the cable. For each possible match it returns a link to the file as well as a bit of information about it.","title":"RecordSearch"},{"location":"recordsearch/#recordsearch","text":"RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping.","title":"RecordSearch"},{"location":"recordsearch/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"recordsearch/#harvesting-a-series","text":"Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well.","title":"Harvesting a series"},{"location":"recordsearch/#harvest-files-with-the-access-status-of-closed","text":"The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how.","title":"Harvest files with the access status of 'closed'"},{"location":"recordsearch/#harvesting-functions-from-the-recordsearch-interface","text":"This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point.","title":"Harvesting functions from the RecordSearch interface"},{"location":"recordsearch/#how-many-of-the-functions-are-actually-used","text":"In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used.","title":"How many of the functions are actually used?"},{"location":"recordsearch/#harvest-agencies-associated-with-all-functions","text":"This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy.","title":"Harvest agencies associated with all functions"},{"location":"recordsearch/#useful-apps","text":"These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button.","title":"Useful apps"},{"location":"recordsearch/#download-the-contents-of-a-digitised-file","text":"RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link.","title":"Download the contents of a digitised file"},{"location":"recordsearch/#get-a-list-of-agencies-associated-with-a-function","text":"RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function.","title":"Get a list of agencies associated with a function"},{"location":"recordsearch/#whos-responsible","text":"The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function.","title":"Who's responsible"},{"location":"recordsearch/#dfat-cable-finder","text":"If you ever need to find a file in the National Archives of Australia that contains a specific numbered cable from the Department of Foreign Affairs this is the tool for you! Just give it a cable number and it will look in the series listed below for a file that might contain the cable. For each possible match it returns a link to the file as well as a bit of information about it.","title":"DFAT Cable Finder"},{"location":"suggest-a-topic/","text":"Suggest a topic \u00b6 Suggestions for new institutions, data sources, and topics are welcome. Just add your suggestion to the GitHub issue tracker . Suggest a topic","title":"Suggest a topic"},{"location":"suggest-a-topic/#suggest-a-topic","text":"Suggestions for new institutions, data sources, and topics are welcome. Just add your suggestion to the GitHub issue tracker . Suggest a topic","title":"Suggest a topic"},{"location":"tepapa/","text":"Te Papa collections API \u00b6 Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration. Tips, tools, and examples \u00b6 Exploring the Te Papa collection API \u00b6 This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available. Mapping Te Papa's collections \u00b6 This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Te Papa"},{"location":"tepapa/#te-papa-collections-api","text":"Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration.","title":"Te Papa collections API"},{"location":"tepapa/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"tepapa/#exploring-the-te-papa-collection-api","text":"This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available.","title":"Exploring the Te Papa collection API"},{"location":"tepapa/#mapping-te-papas-collections","text":"This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Mapping Te Papa's collections"},{"location":"trove-books/","text":"Trove's 'book' zone includes books (of course), but also ephemera (like pamphlets and leaflets) and theses. You can access metadata from the book zone through the Trove API. Tips, tools, and examples \u00b6 Harvesting the text of digitised books (and ephemera) \u00b6 This notebook harvests metadata and OCRd text from digitised works in Trove's book zone. Results of the harvest are available below. Metadata for Trove digitised works \u00b6 In poking around to try and find a way of automating the download of OCR text from Trove's digitised books, I discovered that there's lots of useful metadata embedded in the page of a digitised work. Most of this metadata isn't available through the Trove API. Getting the text of Trove books from the Internet Archive \u00b6 Previously I've harvested the text of books digitised by the National Library of Australia and made available through Trove. But it occured to me it might be possible to get the full text of other books in Trove by making use of the links to the Open Library. Data and text \u00b6 OCRd text from Trove books (and ephemera) \u00b6 Harvested: 15 April 2019 I've harvested 9,738 text files from digitised books using the notebook above. You can browse the collection in CloudStor , or download the complete set as a zip file (400mb) . CSV formatted list of books with OCRd text \u00b6 Harvested: 15 April 2019 This file provides metadata of 9,738 works in the Trove book zone that have OCRd text for download. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: children \u2013 pipe-separated ids of any child works contributors \u2013 pipe-separated names of contributors date \u2013 publication date form \u2013 work format fulltext_url \u2013 link to the digitised version pages \u2013 number of pages parent \u2013 id of parent work (if any) text_downloaded \u2013 file name of the downloaded OCR text text_file \u2013 True/False is there any OCRd text title \u2013 title of the work trove_id \u2013 unique identifier url \u2013 link to the metadata record in Trove volume \u2013 volume/part number OCRd text from the Internet Archive of 'Australian' books listed in Trove \u00b6 I've harvested 1,513 text files from the Internet Archive of 'Australian' books listed in Trove using the notebook above. Trove's 'Australian content' filter was used to try to limit the results to books published in, or about, Australia. However, this is not always accurate and some of the harvested works don't seem to have an Australian connection. You can browse the collection in CloudStor . CSV formatted list of 'Australian' books in Trove with full text versions in the Internet Archive \u00b6 Harvested: 24 May 2019 This file includes metadata of 1,511 'Australian' books listed in Trove that have freely available text versions in the Internet Archive. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: creators \u2013 pipe-separated list of creators date \u2013 publication date ia_formats \u2013 pipe-separated list of file formats available from the Internet Archive (these can be downloaded from the IA) ia_id \u2013 Internet Archive identifier ia_url \u2013 link to more information in the Internet Archive ol_id \u2013 Open Library identifier publisher \u2013 publisher text_filename \u2013 name of the downloaded text file title \u2013 title of the book trove_url \u2013 link to more information in Trove version_id \u2013 Trove version identifier work_id \u2013 Trove work identifier","title":"Trove books"},{"location":"trove-books/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-books/#harvesting-the-text-of-digitised-books-and-ephemera","text":"This notebook harvests metadata and OCRd text from digitised works in Trove's book zone. Results of the harvest are available below.","title":"Harvesting the text of digitised books (and ephemera)"},{"location":"trove-books/#metadata-for-trove-digitised-works","text":"In poking around to try and find a way of automating the download of OCR text from Trove's digitised books, I discovered that there's lots of useful metadata embedded in the page of a digitised work. Most of this metadata isn't available through the Trove API.","title":"Metadata for Trove digitised works"},{"location":"trove-books/#getting-the-text-of-trove-books-from-the-internet-archive","text":"Previously I've harvested the text of books digitised by the National Library of Australia and made available through Trove. But it occured to me it might be possible to get the full text of other books in Trove by making use of the links to the Open Library.","title":"Getting the text of Trove books from the Internet Archive"},{"location":"trove-books/#data-and-text","text":"","title":"Data and text"},{"location":"trove-books/#ocrd-text-from-trove-books-and-ephemera","text":"Harvested: 15 April 2019 I've harvested 9,738 text files from digitised books using the notebook above. You can browse the collection in CloudStor , or download the complete set as a zip file (400mb) .","title":"OCRd text from Trove books (and ephemera)"},{"location":"trove-books/#csv-formatted-list-of-books-with-ocrd-text","text":"Harvested: 15 April 2019 This file provides metadata of 9,738 works in the Trove book zone that have OCRd text for download. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: children \u2013 pipe-separated ids of any child works contributors \u2013 pipe-separated names of contributors date \u2013 publication date form \u2013 work format fulltext_url \u2013 link to the digitised version pages \u2013 number of pages parent \u2013 id of parent work (if any) text_downloaded \u2013 file name of the downloaded OCR text text_file \u2013 True/False is there any OCRd text title \u2013 title of the work trove_id \u2013 unique identifier url \u2013 link to the metadata record in Trove volume \u2013 volume/part number","title":"CSV formatted list of books with OCRd text"},{"location":"trove-books/#ocrd-text-from-the-internet-archive-of-australian-books-listed-in-trove","text":"I've harvested 1,513 text files from the Internet Archive of 'Australian' books listed in Trove using the notebook above. Trove's 'Australian content' filter was used to try to limit the results to books published in, or about, Australia. However, this is not always accurate and some of the harvested works don't seem to have an Australian connection. You can browse the collection in CloudStor .","title":"OCRd text from the Internet Archive of 'Australian' books listed in Trove"},{"location":"trove-books/#csv-formatted-list-of-australian-books-in-trove-with-full-text-versions-in-the-internet-archive","text":"Harvested: 24 May 2019 This file includes metadata of 1,511 'Australian' books listed in Trove that have freely available text versions in the Internet Archive. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: creators \u2013 pipe-separated list of creators date \u2013 publication date ia_formats \u2013 pipe-separated list of file formats available from the Internet Archive (these can be downloaded from the IA) ia_id \u2013 Internet Archive identifier ia_url \u2013 link to more information in the Internet Archive ol_id \u2013 Open Library identifier publisher \u2013 publisher text_filename \u2013 name of the downloaded text file title \u2013 title of the book trove_url \u2013 link to more information in Trove version_id \u2013 Trove version identifier work_id \u2013 Trove work identifier","title":"CSV formatted list of 'Australian' books in Trove with full text versions in the Internet Archive"},{"location":"trove-harvester/","text":"Download large quantities of digitised newspaper articles from Trove using the Trove Harvester tool. Tools, tips, and examples \u00b6 Using TroveHarvester to get newspaper articles in bulk \u00b6 An easy introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Exploring your TroveHarvester data \u00b6 This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Exploring harvested text files \u00b6 This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction) Useful apps \u00b6 These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Trove Harvester web app \u00b6 A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove.","title":"Trove newspaper harvester"},{"location":"trove-harvester/#tools-tips-and-examples","text":"","title":"Tools, tips, and examples"},{"location":"trove-harvester/#using-troveharvester-to-get-newspaper-articles-in-bulk","text":"An easy introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes.","title":"Using TroveHarvester to get newspaper articles in bulk"},{"location":"trove-harvester/#exploring-your-troveharvester-data","text":"This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction)","title":"Exploring your TroveHarvester data"},{"location":"trove-harvester/#exploring-harvested-text-files","text":"This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction)","title":"Exploring harvested text files"},{"location":"trove-harvester/#useful-apps","text":"These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button.","title":"Useful apps"},{"location":"trove-harvester/#trove-harvester-web-app","text":"A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove.","title":"Trove Harvester web app"},{"location":"trove-journals/","text":"Trove's 'journals' zone includes journals and journal articles, as well as other research outputs and things like press releases. You can access metadata from the book zone through the Trove API. Tips, tools, and examples \u00b6 Create a list of Trove's digitised journals \u00b6 Everyone know's about Trove's newspapers, but there is also a growing collection of digitised journals available in the journals zone. They're not easy to find, however, which is why I created the Trove Titles web app. This notebook uses the Trove API to harvest metadata relating to digitised journals \u2013 or more accurately, journals that are freely available online in a digital form. This includes some born digital publications that are available to view in formats like PDF and MOBI, but excludes some digital journals that have access restrictions. Get OCRd text from a digitised journal in Trove \u00b6 Many of the digitised journals available in Trove make OCRd text available for download \u2013 one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them? This notebook shows how to extract issue data from a digitised journal and download OCRd text for each issue. Get covers (or any other pages) from a digitised journal in Trove \u00b6 In another notebook, I showed how to get issue metadata and OCRd texts from a digitised journal in Trove. It's also possible to download page images and PDFs. This notebook shows how to download all the cover images from a specified journal. With some minor modifications you could download any page, or range of pages. Download the OCRd text for ALL the digitised journals in Trove! \u00b6 Using the code and data from the previous two notebooks, you can download the OCRd text from every digitised journal. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder. Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details. I repeat, you probably don't want to do this yourself. The point of this notebook is really to document the methodology used to create the repository. Harvest parliament press releases from Trove \u00b6 Trove includes more than 370,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them all in Trove by searching for nuc:\"APAR:PR\" in the journals zone. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo. Harvesting data from the Bulletin \u00b6 This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Bulletin . It also shows how you can get the front cover images (or any other page). Finding editorial cartoons in the Bulletin \u00b6 In another notebook I showed how you could download all the front pages of The Bulletin (and other journals) as images. Amongst the front pages you'll find a number of full page editorial cartoons under The Bulletin's masthead. But you'll also find that many of the front pages are advertising wrap arounds. The full page editorial cartoons were a consistent feature of The Bulletin for many decades, but they moved around between pages one and eleven. That makes them hard to find. I wanted to try and assemble a collection of all the editorial cartoons, but how? Harvesting data from Home \u00b6 This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Home . It also shows how you can get the front cover images (or any other page). Topic Modelling of Australian Parliamentary Press Releases by Adel Rahmani \u00b6 This notebook explores the Politicians talking about 'immigrants' and 'refugees' collection of press releases (see below). Adel notes: 'I was curious about the contents of the press releases, however, at more than 12,000 documents the collection is too overwhelming to read through, so I thought I'd get the computer to do it for me, and use topic modelling to poke aroung the corpus.' Data and text \u00b6 CSV formatted list of journals available from Trove in digital form \u00b6 Harvested: 5 July 2019 This file provides metadata of 2,381 journals that are available from Trove in a digital form. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove CSV formatted list of journals with OCRd text \u00b6 Harvested: 5 July 2019 This file provides metadata of 384 digitised journals in Trove that have OCRd text for download. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove issues \u2013 the number of available issues issues_with_text \u2013 the number of issues that OCRd text could be downloaded from directory \u2013 the directory in which the files from this journal have been saved (relative to the output directory) OCRd text from Trove digitised journals \u00b6 Harvested: 5 July 2019 Using the notebook above I harvested metadata and OCRd text from Trove's digitised journals. 384 journals had OCRd text available for download OCRd text was downloaded from 30,462 journal issues About 7gb of text was downloaded The complete collection of text files for all the journals can be browsed and downloaded using this repository on CloudStor . Editorial cartoons from The Bulletin, 1886 to 1952 \u00b6 Harvested: 9 May 2019 Using the notebook above I downloaded at least one full page editorial cartoon for every issue of The Bulletin from 4 September 1886 to 17 September 1952. In total there are 3,471 images (approximately 60gb). The complete collection can be downloaded from CloudStor . The names of each image file provide useful contextual metadata. For example, the file name 19330412-2774-nla.obj-606969767-7.jpg tells you: 19330412 \u2013 the cartoon was published on 12 April 1933 2774 \u2013 it was published in issue number 2774 nla.obj-606969767 \u2013 the Trove identifier for the issue, can be used to make a url eg https://nla.gov.au/nla.obj-606969767 7 \u2013 on page 7 To make it easier to browse the images, I've compiled them into a series of PDFs \u2013 one PDF for each decade. The PDFs include lower resolution versions of the images together with their publication details and a link to Trove. They're all available from DropBox : 1886 to 1889 (45mb PDF) 1890 to 1899 (139mb PDF) 1900 to 1909 (147mb PDF) 1910 to 1919 (153mb PDF) 1920 to 1929 (159mb PDF) 1930 to 1939 (151mb PDF) 1940 to 1949 (146mb PDF) 1950 to 1952 (42mb PDF) Politicians talking about 'immigrants' and 'refugees' \u00b6 Using the notebook above I harvested parliamentary press releases that included any of the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', or 'boat arrivals'. A total of 12,619 text files were harvested. You can browse the files on CloudStor , or download the complete dataset as a zip file (43mb) .","title":"Trove journals"},{"location":"trove-journals/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-journals/#create-a-list-of-troves-digitised-journals","text":"Everyone know's about Trove's newspapers, but there is also a growing collection of digitised journals available in the journals zone. They're not easy to find, however, which is why I created the Trove Titles web app. This notebook uses the Trove API to harvest metadata relating to digitised journals \u2013 or more accurately, journals that are freely available online in a digital form. This includes some born digital publications that are available to view in formats like PDF and MOBI, but excludes some digital journals that have access restrictions.","title":"Create a list of Trove's digitised journals"},{"location":"trove-journals/#get-ocrd-text-from-a-digitised-journal-in-trove","text":"Many of the digitised journals available in Trove make OCRd text available for download \u2013 one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them? This notebook shows how to extract issue data from a digitised journal and download OCRd text for each issue.","title":"Get OCRd text from a digitised journal in Trove"},{"location":"trove-journals/#get-covers-or-any-other-pages-from-a-digitised-journal-in-trove","text":"In another notebook, I showed how to get issue metadata and OCRd texts from a digitised journal in Trove. It's also possible to download page images and PDFs. This notebook shows how to download all the cover images from a specified journal. With some minor modifications you could download any page, or range of pages.","title":"Get covers (or any other pages) from a digitised journal in Trove"},{"location":"trove-journals/#download-the-ocrd-text-for-all-the-digitised-journals-in-trove","text":"Using the code and data from the previous two notebooks, you can download the OCRd text from every digitised journal. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder. Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details. I repeat, you probably don't want to do this yourself. The point of this notebook is really to document the methodology used to create the repository.","title":"Download the OCRd text for ALL the digitised journals in Trove!"},{"location":"trove-journals/#harvest-parliament-press-releases-from-trove","text":"Trove includes more than 370,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them all in Trove by searching for nuc:\"APAR:PR\" in the journals zone. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo.","title":"Harvest parliament press releases from Trove"},{"location":"trove-journals/#harvesting-data-from-the-bulletin","text":"This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Bulletin . It also shows how you can get the front cover images (or any other page).","title":"Harvesting data from the Bulletin"},{"location":"trove-journals/#finding-editorial-cartoons-in-the-bulletin","text":"In another notebook I showed how you could download all the front pages of The Bulletin (and other journals) as images. Amongst the front pages you'll find a number of full page editorial cartoons under The Bulletin's masthead. But you'll also find that many of the front pages are advertising wrap arounds. The full page editorial cartoons were a consistent feature of The Bulletin for many decades, but they moved around between pages one and eleven. That makes them hard to find. I wanted to try and assemble a collection of all the editorial cartoons, but how?","title":"Finding editorial cartoons in the Bulletin"},{"location":"trove-journals/#harvesting-data-from-home","text":"This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Home . It also shows how you can get the front cover images (or any other page).","title":"Harvesting data from Home"},{"location":"trove-journals/#topic-modelling-of-australian-parliamentary-press-releases-by-adel-rahmani","text":"This notebook explores the Politicians talking about 'immigrants' and 'refugees' collection of press releases (see below). Adel notes: 'I was curious about the contents of the press releases, however, at more than 12,000 documents the collection is too overwhelming to read through, so I thought I'd get the computer to do it for me, and use topic modelling to poke aroung the corpus.'","title":"Topic Modelling of Australian Parliamentary Press Releases by  Adel Rahmani"},{"location":"trove-journals/#data-and-text","text":"","title":"Data and text"},{"location":"trove-journals/#csv-formatted-list-of-journals-available-from-trove-in-digital-form","text":"Harvested: 5 July 2019 This file provides metadata of 2,381 journals that are available from Trove in a digital form. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove","title":"CSV formatted list of journals available from Trove in digital form"},{"location":"trove-journals/#csv-formatted-list-of-journals-with-ocrd-text","text":"Harvested: 5 July 2019 This file provides metadata of 384 digitised journals in Trove that have OCRd text for download. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove issues \u2013 the number of available issues issues_with_text \u2013 the number of issues that OCRd text could be downloaded from directory \u2013 the directory in which the files from this journal have been saved (relative to the output directory)","title":"CSV formatted list of journals with OCRd text"},{"location":"trove-journals/#ocrd-text-from-trove-digitised-journals","text":"Harvested: 5 July 2019 Using the notebook above I harvested metadata and OCRd text from Trove's digitised journals. 384 journals had OCRd text available for download OCRd text was downloaded from 30,462 journal issues About 7gb of text was downloaded The complete collection of text files for all the journals can be browsed and downloaded using this repository on CloudStor .","title":"OCRd text from Trove digitised journals"},{"location":"trove-journals/#editorial-cartoons-from-the-bulletin-1886-to-1952","text":"Harvested: 9 May 2019 Using the notebook above I downloaded at least one full page editorial cartoon for every issue of The Bulletin from 4 September 1886 to 17 September 1952. In total there are 3,471 images (approximately 60gb). The complete collection can be downloaded from CloudStor . The names of each image file provide useful contextual metadata. For example, the file name 19330412-2774-nla.obj-606969767-7.jpg tells you: 19330412 \u2013 the cartoon was published on 12 April 1933 2774 \u2013 it was published in issue number 2774 nla.obj-606969767 \u2013 the Trove identifier for the issue, can be used to make a url eg https://nla.gov.au/nla.obj-606969767 7 \u2013 on page 7 To make it easier to browse the images, I've compiled them into a series of PDFs \u2013 one PDF for each decade. The PDFs include lower resolution versions of the images together with their publication details and a link to Trove. They're all available from DropBox : 1886 to 1889 (45mb PDF) 1890 to 1899 (139mb PDF) 1900 to 1909 (147mb PDF) 1910 to 1919 (153mb PDF) 1920 to 1929 (159mb PDF) 1930 to 1939 (151mb PDF) 1940 to 1949 (146mb PDF) 1950 to 1952 (42mb PDF)","title":"Editorial cartoons from The Bulletin, 1886 to 1952"},{"location":"trove-journals/#politicians-talking-about-immigrants-and-refugees","text":"Using the notebook above I harvested parliamentary press releases that included any of the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', or 'boat arrivals'. A total of 12,619 text files were harvested. You can browse the files on CloudStor , or download the complete dataset as a zip file (43mb) .","title":"Politicians talking about 'immigrants' and 'refugees'"},{"location":"trove-lists/","text":"Trove lists are user created collections of items. The details of public lists are available through the Trove API. Tips, tools, and examples \u00b6 Harvest summary data from Trove lists \u00b6 Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Convert a Trove list into a CSV file \u00b6 Use the Trove API to save the contents of a public list to a CSV file. Data \u00b6 Trove lists metadata \u00b6 Harvested: 20 September 2018 CSV formatted file containing the following fields: created \u2013 date list created id \u2013 list identifier number_items \u2013 number of items in list title \u2013 title of list updated \u2013 date last updated","title":"Trove lists"},{"location":"trove-lists/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-lists/#harvest-summary-data-from-trove-lists","text":"Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset.","title":"Harvest summary data from Trove lists"},{"location":"trove-lists/#convert-a-trove-list-into-a-csv-file","text":"Use the Trove API to save the contents of a public list to a CSV file.","title":"Convert a Trove list into a CSV file"},{"location":"trove-lists/#data","text":"","title":"Data"},{"location":"trove-lists/#trove-lists-metadata","text":"Harvested: 20 September 2018 CSV formatted file containing the following fields: created \u2013 date list created id \u2013 list identifier number_items \u2013 number of items in list title \u2013 title of list updated \u2013 date last updated","title":"Trove lists metadata"},{"location":"trove-maps/","text":"The Trove 'map' zone includes single maps, as well as map series, atlases, and aerial photographs. You can access metadata from the map zone through the Trove API. Tips, tools, and examples \u00b6 Exploring digitised maps in Trove \u00b6 If you've ever poked around in Trove's 'map' zone, you might have noticed the beautiful deep-zoomable images available for many of the NLA's digitised maps. Even better, in many cases the high-resolution TIFF versions of the digitised maps are available for download. I knew there were lots of great maps you could download from Trove, but how many? And how big were the files? I thought I'd try to quantify this a bit by harvesting and analysing the metadata. Data \u00b6 CSV formatted list of maps with high-resolution downloads \u00b6 Harvested: 26 April 2019 This file provides metadata of 20,158 maps in Trove that have high-resolution downloads. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: copyright_status - a string indicating the copyright status of the map creators \u2013 pipe-separated list of contributors/creators date \u2013 date of the map (format varies) filesize \u2013 size in bytes filesize_string \u2013 size as a human-readable string with unit fulltext_url \u2013 link to the landing page of the digital version height \u2013 height in pixels title \u2013 title of the map trove_id \u2013 digital object identifier trove_url \u2013 link to the metadata record in Trove width \u2013 width in pixels","title":"Trove maps"},{"location":"trove-maps/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-maps/#exploring-digitised-maps-in-trove","text":"If you've ever poked around in Trove's 'map' zone, you might have noticed the beautiful deep-zoomable images available for many of the NLA's digitised maps. Even better, in many cases the high-resolution TIFF versions of the digitised maps are available for download. I knew there were lots of great maps you could download from Trove, but how many? And how big were the files? I thought I'd try to quantify this a bit by harvesting and analysing the metadata.","title":"Exploring digitised maps in Trove"},{"location":"trove-maps/#data","text":"","title":"Data"},{"location":"trove-maps/#csv-formatted-list-of-maps-with-high-resolution-downloads","text":"Harvested: 26 April 2019 This file provides metadata of 20,158 maps in Trove that have high-resolution downloads. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: copyright_status - a string indicating the copyright status of the map creators \u2013 pipe-separated list of contributors/creators date \u2013 date of the map (format varies) filesize \u2013 size in bytes filesize_string \u2013 size as a human-readable string with unit fulltext_url \u2013 link to the landing page of the digital version height \u2013 height in pixels title \u2013 title of the map trove_id \u2013 digital object identifier trove_url \u2013 link to the metadata record in Trove width \u2013 width in pixels","title":"CSV formatted list of maps with high-resolution downloads"},{"location":"trove-newspapers/","text":"Assorted experiments and examples working with Trove\u2019s digitised newspapers Tips, tools, and examples \u00b6 Visualise Trove newspaper searches over time \u00b6 This notebook helps you zoom out and explore how the number of Trove newspaper articles in your search results varies over time by using the decade and year facets . We then combine this approach with other search facets to see how we can slice a set of results up in different ways to investigate historical changes. Download from GitHub View using NBViewer Run live on Binder Visualise the total number of newspaper articles in Trove by year and state \u00b6 Trove currently includes more 200 million digitised newspaper articles published between 1803 and 2015. In this notebook we explore how those newspaper articles are distributed over time, and by state. Download from GitHub View using NBViewer Run live on Binder View an interactive HTML chart Map Trove newspaper results by state \u00b6 Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Download from GitHub View using NBViewer Run live on Binder Map Trove newspaper results by place of publication \u00b6 Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Download from GitHub View using NBViewer Run live on Binder Map Trove newspaper results by place of publication over time \u00b6 Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Download from GitHub View using NBViewer Run live on Binder Today\u2019s news yesterday \u00b6 Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Download from GitHub View using NBViewer Run live on Binder Create a Trove OCR corrections ticker \u00b6 Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds. Download from GitHub View using NBViewer Run live on Binder Save a Trove newspaper article as an image \u00b6 This notebook grabs the page on which an article was published, and then crops the page image to the boundaries of the article. The result is an image which presents the article as it was originally published. Download from GitHub View using NBViewer Run live on Binder Run as an app using Voila Upload Trove newspaper articles to Omeka-S \u00b6 This notebook steps through the process of uploading Trove newspaper articles to your own Omeka-S instance via the API. As well as uploading the article metadata, it attaches image(s) and PDFs of the articles, and creates a linked record for the publishing newspaper. The source of the articles can be a Trove search, a Trove list, a Zotero collection, or just a list of article ids. Download from GitHub View using NBViewer Run live on Binder Beyond the copyright cliff of death \u00b6 Most of the newspaper articles on Trove were published before 1955, but there are some from the later period. Let's find out how many, and which newspapers they were published in. Download from GitHub View using NBViewer Run live on Binder Download a page image \u00b6 The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila Generate an article thumbnail \u00b6 Generate a nice square thumbnail image for a newspaper article. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila QueryPic Deconstructed \u00b6 QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"Trove newspapers"},{"location":"trove-newspapers/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-newspapers/#visualise-trove-newspaper-searches-over-time","text":"This notebook helps you zoom out and explore how the number of Trove newspaper articles in your search results varies over time by using the decade and year facets . We then combine this approach with other search facets to see how we can slice a set of results up in different ways to investigate historical changes. Download from GitHub View using NBViewer Run live on Binder","title":"Visualise Trove newspaper searches over time"},{"location":"trove-newspapers/#visualise-the-total-number-of-newspaper-articles-in-trove-by-year-and-state","text":"Trove currently includes more 200 million digitised newspaper articles published between 1803 and 2015. In this notebook we explore how those newspaper articles are distributed over time, and by state. Download from GitHub View using NBViewer Run live on Binder View an interactive HTML chart","title":"Visualise the total number of newspaper articles in Trove by year and state"},{"location":"trove-newspapers/#map-trove-newspaper-results-by-state","text":"Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Download from GitHub View using NBViewer Run live on Binder","title":"Map Trove newspaper results by state"},{"location":"trove-newspapers/#map-trove-newspaper-results-by-place-of-publication","text":"Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Download from GitHub View using NBViewer Run live on Binder","title":"Map Trove newspaper results by place of publication"},{"location":"trove-newspapers/#map-trove-newspaper-results-by-place-of-publication-over-time","text":"Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Download from GitHub View using NBViewer Run live on Binder","title":"Map Trove newspaper results by place of publication over time"},{"location":"trove-newspapers/#todays-news-yesterday","text":"Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Download from GitHub View using NBViewer Run live on Binder","title":"Today\u2019s news yesterday"},{"location":"trove-newspapers/#create-a-trove-ocr-corrections-ticker","text":"Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds. Download from GitHub View using NBViewer Run live on Binder","title":"Create a Trove OCR corrections ticker"},{"location":"trove-newspapers/#save-a-trove-newspaper-article-as-an-image","text":"This notebook grabs the page on which an article was published, and then crops the page image to the boundaries of the article. The result is an image which presents the article as it was originally published. Download from GitHub View using NBViewer Run live on Binder Run as an app using Voila","title":"Save a Trove newspaper article as an image"},{"location":"trove-newspapers/#upload-trove-newspaper-articles-to-omeka-s","text":"This notebook steps through the process of uploading Trove newspaper articles to your own Omeka-S instance via the API. As well as uploading the article metadata, it attaches image(s) and PDFs of the articles, and creates a linked record for the publishing newspaper. The source of the articles can be a Trove search, a Trove list, a Zotero collection, or just a list of article ids. Download from GitHub View using NBViewer Run live on Binder","title":"Upload Trove newspaper articles to Omeka-S"},{"location":"trove-newspapers/#beyond-the-copyright-cliff-of-death","text":"Most of the newspaper articles on Trove were published before 1955, but there are some from the later period. Let's find out how many, and which newspapers they were published in. Download from GitHub View using NBViewer Run live on Binder","title":"Beyond the copyright cliff of death"},{"location":"trove-newspapers/#download-a-page-image","text":"The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila","title":"Download a page image"},{"location":"trove-newspapers/#generate-an-article-thumbnail","text":"Generate a nice square thumbnail image for a newspaper article. Download from GitHub View using NBViewer Run live on Binder in Appmode Run as an app using Voila","title":"Generate an article thumbnail"},{"location":"trove-newspapers/#querypic-deconstructed","text":"QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic. Download from GitHub View using NBViewer Run live on Binder in Appmode","title":"QueryPic Deconstructed"},{"location":"trove-unpublished/","text":"Trove unpublished works \u00b6 Experiments and examples relating to Trove's 'Diaries, letters, and archives' zone. Tips, tools, and examples \u00b6 Finding unpublished works that might be entering the public domain on 1 January 2019 \u00b6 Changes to Australian copyright legislation mean that many unpublished resources will be entering the public domain on 1 January 2019. This notebook attempts to harvest the details of some of these resources from Trove. Exploring unpublished works that might be entering the public domain on 1 January 2019 \u00b6 Some ways of exploring the data harvested above. Data \u00b6 Unpublished works that might be entering the public domain on 1 January 2019 \u00b6 Download CSV file (1.8mb) View on Google Sheets","title":"Trove unpublished"},{"location":"trove-unpublished/#trove-unpublished-works","text":"Experiments and examples relating to Trove's 'Diaries, letters, and archives' zone.","title":"Trove unpublished works"},{"location":"trove-unpublished/#tips-tools-and-examples","text":"","title":"Tips, tools, and examples"},{"location":"trove-unpublished/#finding-unpublished-works-that-might-be-entering-the-public-domain-on-1-january-2019","text":"Changes to Australian copyright legislation mean that many unpublished resources will be entering the public domain on 1 January 2019. This notebook attempts to harvest the details of some of these resources from Trove.","title":"Finding unpublished works that might be entering the public domain on 1 January 2019"},{"location":"trove-unpublished/#exploring-unpublished-works-that-might-be-entering-the-public-domain-on-1-january-2019","text":"Some ways of exploring the data harvested above.","title":"Exploring unpublished works that might be entering the public domain on 1 January 2019"},{"location":"trove-unpublished/#data","text":"","title":"Data"},{"location":"trove-unpublished/#unpublished-works-that-might-be-entering-the-public-domain-on-1-january-2019","text":"Download CSV file (1.8mb) View on Google Sheets","title":"Unpublished works that might be entering the public domain on 1 January 2019"},{"location":"trove/","text":"Trove provides access to much of it's data through an API (Application Programming Interface). The notebooks in this section provide many examples of using the API to harvest data and analyse the contents of Trove. Before you can use the API you need to obtain a key \u2014 it's free and quick. Just follow these instructions . What's an API? \u00b6 An API is an Application Programming Interface. It's a set of predefined requests and responses that enables computer programs talk to each other. Web APIs are generally used to deliver data. While humans can easily interpret information on a web page, computers need more help. APIs provide data in a form that computers can understand and use (we call this machine-readable data). The Trove API works much like the Trove website. You make queries and you get back results. But instead of going through a nicely-designed web interface, requests to the API are just URLs, and the results are just structured data. While you can just type an API request into the location box of your web browser, most of the time requests and responses will be handled by a computer script or program. APIs don't care what programming language you use as long as you structure requests in the way they expect. In these notebooks we'll be using the programming language Python. No prior knowledge of Python is expected or required -- just follow along! The examples and approaches used could be easily translated into any another programming language. Tools, tips, examples \u00b6 Your first API request \u00b6 In this notebook we're going to learn how to send a request for information to the Trove API. API requests are just like normal urls. However, instead of sending us back a web page, they deliver data in a form that computers can understand. We can then use that data in our own programs. Working with zones \u00b6 Trove's zones are important in constructing API requests and interpreting the results. So let's explore them a bit. Exploring facets \u00b6 Facets aggregate collection data in interesting and useful ways, allowing us to build pictures of the collection. This notebook shows you how to get facet data from Trove.) Useful links \u00b6 Trove API Documentation Trove API console Introduction to using APIs","title":"Trove API introduction"},{"location":"trove/#whats-an-api","text":"An API is an Application Programming Interface. It's a set of predefined requests and responses that enables computer programs talk to each other. Web APIs are generally used to deliver data. While humans can easily interpret information on a web page, computers need more help. APIs provide data in a form that computers can understand and use (we call this machine-readable data). The Trove API works much like the Trove website. You make queries and you get back results. But instead of going through a nicely-designed web interface, requests to the API are just URLs, and the results are just structured data. While you can just type an API request into the location box of your web browser, most of the time requests and responses will be handled by a computer script or program. APIs don't care what programming language you use as long as you structure requests in the way they expect. In these notebooks we'll be using the programming language Python. No prior knowledge of Python is expected or required -- just follow along! The examples and approaches used could be easily translated into any another programming language.","title":"What's an API?"},{"location":"trove/#tools-tips-examples","text":"","title":"Tools, tips, examples"},{"location":"trove/#your-first-api-request","text":"In this notebook we're going to learn how to send a request for information to the Trove API. API requests are just like normal urls. However, instead of sending us back a web page, they deliver data in a form that computers can understand. We can then use that data in our own programs.","title":"Your first API request"},{"location":"trove/#working-with-zones","text":"Trove's zones are important in constructing API requests and interpreting the results. So let's explore them a bit.","title":"Working with zones"},{"location":"trove/#exploring-facets","text":"Facets aggregate collection data in interesting and useful ways, allowing us to build pictures of the collection. This notebook shows you how to get facet data from Trove.)","title":"Exploring facets"},{"location":"trove/#useful-links","text":"Trove API Documentation Trove API console Introduction to using APIs","title":"Useful links"}]}