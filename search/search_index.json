{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the wonderful world of GLAM data! \u00b6 Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand. What is GLAM data? \u00b6 When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets. What can I do with GLAM data? \u00b6 Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data. Do I need to be able to code? \u00b6 No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code. What is Jupyter? \u00b6 ~~More detail required~~ Where do I start? \u00b6 ~~More detail required~~ Other GLAM related notebooks \u00b6 Awesome Jupyter GLAM Getting started with ODate","title":"Home"},{"location":"#welcome-to-the-wonderful-world-of-glam-data","text":"Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand.","title":"Welcome to the wonderful world of GLAM data!"},{"location":"#what-is-glam-data","text":"When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets.","title":"What is GLAM data?"},{"location":"#what-can-i-do-with-glam-data","text":"Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data.","title":"What can I do with GLAM data?"},{"location":"#do-i-need-to-be-able-to-code","text":"No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code.","title":"Do I need to be able to code?"},{"location":"#what-is-jupyter","text":"~~More detail required~~","title":"What is Jupyter?"},{"location":"#where-do-i-start","text":"~~More detail required~~","title":"Where do I start?"},{"location":"#other-glam-related-notebooks","text":"Awesome Jupyter GLAM Getting started with ODate","title":"Other GLAM related notebooks"},{"location":"about/","text":"This is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions. It's aimed at researchers in the humanities, but will include examples and tutorials of more general interest. Over the past decade I've created and shared a wide variety of digital tools, examples, tutorials, and datasets. Some like QueryPic and TroveHarvester are fairly polished and well documented. Others are just fragments of code. All of them are intended to support research into our rich cultural collections. But even though something like the TroveHarvester is pretty easy to use, it does require a bit of set-up, and I've been very aware that this can be a barrier to people starting their explorations. I created the dhistory site many years ago to provide the foundation for a digital workbench, but I couldn't quite achieve what I wanted \u2014 tools that were easy to use and required minimal setup, but also tools that exposed their own workings, that inspired novice users to question and to tinker. So here we are. My plan is to use Jupyter , GitHub, and Binder to bring together all those tools, examples, tutorials, and datasets in a way that supports people's explorations through digital GLAM collections. I'm really excited, for example, that I can create a notebook that provides a deconstructed (or perhaps see-through) version of QueryPic \u2014 that enables you to build, step by step, the same sort of visualisations, while learning about how it works. And at the end you can download the results as a CSV for further analysis. I love the way that Jupyter notebooks combine learning with real, live, digital tools and methods. You don't have to read a tutorial then go away and try to follow the instructions on your own. It's all together. Live code. Real research. Active learning. Like most of my projects this is in itself an experiment. I'm still learning what's possible and what works. But I'm hopeful. If you think this project is worthwhile, you might like to support me on Patreon .","title":"Some background"},{"location":"archway/","text":"Archway \u00b6 Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 Harvesting a records search This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 10,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 10,000 \u2014 the notebook provides some examples of how you might do this. Harvesting functions Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset. Useful apps \u00b6 Who's responsible? Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function.","title":"Archway"},{"location":"archway/#archway","text":"Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Archway"},{"location":"archway/#tools-tips-and-examples","text":"Harvesting a records search This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 10,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 10,000 \u2014 the notebook provides some examples of how you might do this. Harvesting functions Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset.","title":"Tools, tips, and examples"},{"location":"archway/#useful-apps","text":"Who's responsible? Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function.","title":"Useful apps"},{"location":"csv-explorer/","text":"GLAM CSV Explorer \u00b6 Cultural institutions are making collection data available as machine readable downloads. But how can researchers explore the shape and meaning of this data? How do they know what types of questions they can ask? This notebook provides a quick overview of CSV-formatted data files, particularly those created by GLAM institutions (galleries, libraries, archives, and museums). You can select one of more than 400 GLAM datasets shared on data.gov.au or upload your own. or view notebook using NBViewer .","title":"GLAM CSV Explorer"},{"location":"csv-explorer/#glam-csv-explorer","text":"Cultural institutions are making collection data available as machine readable downloads. But how can researchers explore the shape and meaning of this data? How do they know what types of questions they can ask? This notebook provides a quick overview of CSV-formatted data files, particularly those created by GLAM institutions (galleries, libraries, archives, and museums). You can select one of more than 400 GLAM datasets shared on data.gov.au or upload your own. or view notebook using NBViewer .","title":"GLAM CSV Explorer"},{"location":"digitalnz/","text":"DigitalNZ aggregates collections from across New Zealand and makes the aggregated metadata available through an API . You'll need an API key to work with DigitalNZ data. Tips, tools, and examples \u00b6 Getting some top-level data from the DigitalNZ API This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Find results by country in DigitalNZ Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Visualise a search in Papers Past Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Harvest data from Papers Past This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles.","title":"DigitalNZ"},{"location":"digitalnz/#tips-tools-and-examples","text":"Getting some top-level data from the DigitalNZ API This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Find results by country in DigitalNZ Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Visualise a search in Papers Past Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Harvest data from Papers Past This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles.","title":"Tips, tools, and examples"},{"location":"facial-detection/","text":"Finding faces in the Tribune negatives \u00b6 Exploring mages from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Finding faces in the Tribune collection Using OpenCV for basic facial detection. Finding all the faces in the Tribune collection This notebook runs a facial detection script across the whole Tribune collection. It saves cropped versions of all the detected faces, and creates a data file recording the number of faces detected per image. Exploring the face data This notebook plays around with the data generated by running the facial detection script over the whole Tribune collection. Make all the faces into one big image Having extracted lots and lots of faces from the Tribune photos, I thought I'd combine them in one big image. Interesting apps \u00b6 Focus on faces A little app that fades from faces to photos...","title":"Facial detection"},{"location":"facial-detection/#finding-faces-in-the-tribune-negatives","text":"Exploring mages from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Finding faces in the Tribune negatives"},{"location":"facial-detection/#tools-tips-and-examples","text":"Finding faces in the Tribune collection Using OpenCV for basic facial detection. Finding all the faces in the Tribune collection This notebook runs a facial detection script across the whole Tribune collection. It saves cropped versions of all the detected faces, and creates a data file recording the number of faces detected per image. Exploring the face data This notebook plays around with the data generated by running the facial detection script over the whole Tribune collection. Make all the faces into one big image Having extracted lots and lots of faces from the Tribune photos, I thought I'd combine them in one big image.","title":"Tools, tips, and examples"},{"location":"facial-detection/#interesting-apps","text":"Focus on faces A little app that fades from faces to photos...","title":"Interesting apps"},{"location":"getting-started/","text":"Using Jupyter notebooks \u00b6 Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing. Viewing notebooks on NBViewer \u00b6 The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar. Running notebooks live on Binder \u00b6 Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session. Running notebooks in \u2018app mode\u2019 \u00b6 Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required. Running notebooks on your own computer \u00b6 If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Getting started"},{"location":"getting-started/#using-jupyter-notebooks","text":"Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing.","title":"Using Jupyter notebooks"},{"location":"getting-started/#viewing-notebooks-on-nbviewer","text":"The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar.","title":"Viewing notebooks on NBViewer"},{"location":"getting-started/#running-notebooks-live-on-binder","text":"Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session.","title":"Running notebooks live on Binder"},{"location":"getting-started/#running-notebooks-in-app-mode","text":"Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required.","title":"Running notebooks in &lsquo;app mode&rsquo;"},{"location":"getting-started/#running-notebooks-on-your-own-computer","text":"If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Running notebooks on your own computer"},{"location":"glam-data-portals/","text":"GLAM data from government portals \u00b6 This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. You can visualise the contents of the CSV datasets I've harvested by using the GLAM CSV Explorer . Tools, tips, and examples \u00b6 Harvesting GLAM data from government portals This notebook attempts to harvest the details of GLAM datasets from state and national data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. It also attempts some analysis of the results. Harvest GLAM datasets from data.gov.au This is a quick attempt to harvest datasets published by GLAM institutions using the new data.gov.au API. To create the list of organisations, I searched the organisations on the data.gov.au site for 'library', 'archives', 'records', and 'museum'. Results (March 2019) \u00b6 GLAM datasets from data.gov.au \u2013 all formats (CSV) GLAM datasets from data.gov.au \u2013 CSVs only (CSV) Datasets by format: CSV 447 XML 79 JSON 73 XLSX 54 ESRI REST 41 HTML 34 DOCX 33 PLAIN 16 ZIP 13 GEOJSON 8 API 8 DATA 6 OTHER 4 RSS 2 JPEG 2 KML 2 MPK 2 APP 1 CSS 1 JAVASCRIPT 1 PDF 1 HMTL 1 WFS 1 WMS 1 Datasets by organisation: Queensland State Archives 172 State Library of Western Australia 147 State Library of South Australia 128 State Library of Queensland 101 Libraries Tasmania 71 State Records Office of Western Australia 44 State Records 41 South Australian Museum 33 State Library of New South Wales 21 NSW State Archives 19 History Trust of South Australia 17 Western Australian Museum 14 State Library of Victoria 6 State Library of NSW 6 National Library of Australia 5 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 Tasmanian Museum and Art Gallery 2 Mount Gambier Library 2 Datasets by licence: Creative Commons Attribution 310 Creative Commons Attribution 4.0 168 Creative Commons Attribution 3.0 Australia 156 Creative Commons Attribution 4.0 International 110 License not specified 41 Creative Commons Attribution 2.5 Australia 15 Creative Commons Attribution-NonCommercial 10 notspecified 5 Other (Open) 4 Creative Commons Attribution 3.0 3 Creative Commons Attribution Share-Alike 3 Creative Commons Non-Commercial (Any) 2 Other (Non-Commercial) 1 Creative Commons Attribution Share Alike 4.0 International 1 Results (April 2018) \u00b6 Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Gov data portals"},{"location":"glam-data-portals/#glam-data-from-government-portals","text":"This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. You can visualise the contents of the CSV datasets I've harvested by using the GLAM CSV Explorer .","title":"GLAM data from government portals"},{"location":"glam-data-portals/#tools-tips-and-examples","text":"Harvesting GLAM data from government portals This notebook attempts to harvest the details of GLAM datasets from state and national data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. It also attempts some analysis of the results. Harvest GLAM datasets from data.gov.au This is a quick attempt to harvest datasets published by GLAM institutions using the new data.gov.au API. To create the list of organisations, I searched the organisations on the data.gov.au site for 'library', 'archives', 'records', and 'museum'.","title":"Tools, tips, and examples"},{"location":"glam-data-portals/#results-march-2019","text":"GLAM datasets from data.gov.au \u2013 all formats (CSV) GLAM datasets from data.gov.au \u2013 CSVs only (CSV) Datasets by format: CSV 447 XML 79 JSON 73 XLSX 54 ESRI REST 41 HTML 34 DOCX 33 PLAIN 16 ZIP 13 GEOJSON 8 API 8 DATA 6 OTHER 4 RSS 2 JPEG 2 KML 2 MPK 2 APP 1 CSS 1 JAVASCRIPT 1 PDF 1 HMTL 1 WFS 1 WMS 1 Datasets by organisation: Queensland State Archives 172 State Library of Western Australia 147 State Library of South Australia 128 State Library of Queensland 101 Libraries Tasmania 71 State Records Office of Western Australia 44 State Records 41 South Australian Museum 33 State Library of New South Wales 21 NSW State Archives 19 History Trust of South Australia 17 Western Australian Museum 14 State Library of Victoria 6 State Library of NSW 6 National Library of Australia 5 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 Tasmanian Museum and Art Gallery 2 Mount Gambier Library 2 Datasets by licence: Creative Commons Attribution 310 Creative Commons Attribution 4.0 168 Creative Commons Attribution 3.0 Australia 156 Creative Commons Attribution 4.0 International 110 License not specified 41 Creative Commons Attribution 2.5 Australia 15 Creative Commons Attribution-NonCommercial 10 notspecified 5 Other (Open) 4 Creative Commons Attribution 3.0 3 Creative Commons Attribution Share-Alike 3 Creative Commons Non-Commercial (Any) 2 Other (Non-Commercial) 1 Creative Commons Attribution Share Alike 4.0 International 1","title":"Results (March 2019)"},{"location":"glam-data-portals/#results-april-2018","text":"Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Results (April 2018)"},{"location":"image-tagging/","text":"Experimenting with automated image tagging in the Tribune negatives \u00b6 Exploring images from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Basic image recognition with Tribune photos Trying Inception-v3's pre-trained model on some Tribune photos. Tensorflow for poets Notebook to accompany the Tensorflow for poets tutorial Training a classification model for the Tribune Using the 'Tensorflow for poets' example to create our own model. Classifying all the Tribune images Using the simple model we created in the previous notebook, let's attempt to classify all the images in the Tribune collection.","title":"Image tagging"},{"location":"image-tagging/#experimenting-with-automated-image-tagging-in-the-tribune-negatives","text":"Exploring images from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Experimenting with automated image tagging in the Tribune negatives"},{"location":"image-tagging/#tools-tips-and-examples","text":"Basic image recognition with Tribune photos Trying Inception-v3's pre-trained model on some Tribune photos. Tensorflow for poets Notebook to accompany the Tensorflow for poets tutorial Training a classification model for the Tribune Using the 'Tensorflow for poets' example to create our own model. Classifying all the Tribune images Using the simple model we created in the previous notebook, let's attempt to classify all the images in the Tribune collection.","title":"Tools, tips, and examples"},{"location":"lac/","text":"Library Archives Canada \u00b6 Unfortunately Library Archives Canada doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 LAC naturalisation database, 1915-1946 \u2014 harvest by country This notebook helps you extract the records of people from a specific country from the LAC 1915-1946 naturalisations database . There are numerous limitations and problems, including the fact that you can only get the first 2000 results. Women and children are not returned in a search by country, so the notebook makes an attempt to find and add them to the initial harvest. Here's an example harvest for 'China' .","title":"Library Archives Canada"},{"location":"lac/#library-archives-canada","text":"Unfortunately Library Archives Canada doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Library Archives Canada"},{"location":"lac/#tools-tips-and-examples","text":"LAC naturalisation database, 1915-1946 \u2014 harvest by country This notebook helps you extract the records of people from a specific country from the LAC 1915-1946 naturalisations database . There are numerous limitations and problems, including the fact that you can only get the first 2000 results. Women and children are not returned in a search by country, so the notebook makes an attempt to find and add them to the initial harvest. Here's an example harvest for 'China' .","title":"Tools, tips, and examples"},{"location":"naa-asio/","text":"Series in the National Archives of Australia with content recorded by the Australian Security Intelligence Organisation (ASIO) \u00b6 This repository includes item-level metadata from all 18 series in the National Archives of Australia that are listed on RecordSearch as including content recorded by ASIO. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory. Tools, tips, and examples \u00b6 Harvesting series Code to harvest item-level data and digitised images from series. Analyse a series Some ways of exploring and visualising the item level data. Summarise series data Summarise the data from all harvested series. Summarise series Summarise the data from a single series. Results (May 2018) \u00b6 To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"ASIO archives"},{"location":"naa-asio/#series-in-the-national-archives-of-australia-with-content-recorded-by-the-australian-security-intelligence-organisation-asio","text":"This repository includes item-level metadata from all 18 series in the National Archives of Australia that are listed on RecordSearch as including content recorded by ASIO. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory.","title":"Series in the National Archives of Australia with content recorded by the Australian Security Intelligence Organisation (ASIO)"},{"location":"naa-asio/#tools-tips-and-examples","text":"Harvesting series Code to harvest item-level data and digitised images from series. Analyse a series Some ways of exploring and visualising the item level data. Summarise series data Summarise the data from all harvested series. Summarise series Summarise the data from a single series.","title":"Tools, tips, and examples"},{"location":"naa-asio/#results-may-2018","text":"To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"Results (May 2018)"},{"location":"naa-wap/","text":"Series in the National Archives of Australia relating to the White Australia Policy \u00b6 This repository includes item-level metadata from 23 series harvested from the National Archives online database RecordSearch. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory. Tools, tips, and examples \u00b6 Harvesting series Code to harvest item-level data and digitised images from series. Analyse a series Some ways of exploring and visualising the item level data. Summarise series data Summarise the data from all harvested series. Summarise series Summarise the data from a single series. Results (May 2018) \u00b6 To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"White Australia Policy archives"},{"location":"naa-wap/#series-in-the-national-archives-of-australia-relating-to-the-white-australia-policy","text":"This repository includes item-level metadata from 23 series harvested from the National Archives online database RecordSearch. The complete code for harvesting and processing the data is included, as are some examples of how the data can be analysed. The harvested metadata is available as CSV-formatted files (one for each series) in the RecordSearch/data directory.","title":"Series in the National Archives of Australia relating to the White Australia Policy"},{"location":"naa-wap/#tools-tips-and-examples","text":"Harvesting series Code to harvest item-level data and digitised images from series. Analyse a series Some ways of exploring and visualising the item level data. Summarise series data Summarise the data from all harvested series. Summarise series Summarise the data from a single series.","title":"Tools, tips, and examples"},{"location":"naa-wap/#results-may-2018","text":"To browse information about the harvested series start with the summary of all harvested series notebook . There you'll find a live version of the table below with links to individual series summaries and visualisations. .","title":"Results (May 2018)"},{"location":"qsa/","text":"Queensland State Archives \u00b6 Tools, tips, and examples \u00b6 Naturalisations, 1851-1904 \u2014 add series information The Naturalisations, 1851 to 1904 index is available from the Queensland Government data portal. It's not clear, however, that the index collates name entries from a number of different series, with a separate row for each name reference. This means that there can be multiple rows referring to the naturalisation of a single individual. This is obviously important to keep in mind if you're trying to analyse aggregate data relating to naturalisations in Queensland. This notebook adds series information to the original index so that you can filter the data by series.","title":"Queensland State Archives"},{"location":"qsa/#queensland-state-archives","text":"","title":"Queensland State Archives"},{"location":"qsa/#tools-tips-and-examples","text":"Naturalisations, 1851-1904 \u2014 add series information The Naturalisations, 1851 to 1904 index is available from the Queensland Government data portal. It's not clear, however, that the index collates name entries from a number of different series, with a separate row for each name reference. This means that there can be multiple rows referring to the naturalisation of a single individual. This is obviously important to keep in mind if you're trying to analyse aggregate data relating to naturalisations in Queensland. This notebook adds series information to the original index so that you can filter the data by series.","title":"Tools, tips, and examples"},{"location":"records-of-resistance/","text":"Tribune negatives \u00b6 Exploring metadata harvested from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra. Tools, tips, and examples \u00b6 Tribune collection metadata magic This notebook helps you explore metadata (and images) from the State Library of NSW's Tribune collection. Topics and descriptions Some more examples of analysing text descriptions and topic tags. Playing with places Cleaning and analysing place data created by students.","title":"Tribune negatives"},{"location":"records-of-resistance/#tribune-negatives","text":"Exploring metadata harvested from the Tribune negative collection in the State Library of NSW. These notebooks were created for students in the Exploring Digital Heritage class at the University of Canberra.","title":"Tribune negatives"},{"location":"records-of-resistance/#tools-tips-and-examples","text":"Tribune collection metadata magic This notebook helps you explore metadata (and images) from the State Library of NSW's Tribune collection. Topics and descriptions Some more examples of analysing text descriptions and topic tags. Playing with places Cleaning and analysing place data created by students.","title":"Tools, tips, and examples"},{"location":"recordsearch/","text":"RecordSearch \u00b6 RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping. Tools, tips, and examples \u00b6 Harvesting a series Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well. Harvest files with the access status of 'closed' The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how. Harvesting functions from the RecordSearch interface This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point. How many of the functions are actually used? In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used. Harvest agencies associated with all functions This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy. Useful apps \u00b6 These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button. Download the contents of a digitised file RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link. Get a list of agencies associated with a function RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function. Who's responsible The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function. DFAT Cable Finder If you ever need to find a file in the National Archives of Australia that contains a specific numbered cable from the Department of Foreign Affairs this is the tool for you! Just give it a cable number and it will look in the series listed below for a file that might contain the cable. For each possible match it returns a link to the file as well as a bit of information about it.","title":"RecordSearch"},{"location":"recordsearch/#recordsearch","text":"RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping.","title":"RecordSearch"},{"location":"recordsearch/#tools-tips-and-examples","text":"Harvesting a series Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well. Harvest files with the access status of 'closed' The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how. Harvesting functions from the RecordSearch interface This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point. How many of the functions are actually used? In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used. Harvest agencies associated with all functions This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy.","title":"Tools, tips, and examples"},{"location":"recordsearch/#useful-apps","text":"These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button. Download the contents of a digitised file RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link. Get a list of agencies associated with a function RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function. Who's responsible The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function. DFAT Cable Finder If you ever need to find a file in the National Archives of Australia that contains a specific numbered cable from the Department of Foreign Affairs this is the tool for you! Just give it a cable number and it will look in the series listed below for a file that might contain the cable. For each possible match it returns a link to the file as well as a bit of information about it.","title":"Useful apps"},{"location":"suggest-a-topic/","text":"Suggest a topic \u00b6 Suggestions for new institutions, data sources, and topics are welcome. Just add your suggestion to the GitHub issue tracker . Suggest a topic","title":"Suggest a topic"},{"location":"suggest-a-topic/#suggest-a-topic","text":"Suggestions for new institutions, data sources, and topics are welcome. Just add your suggestion to the GitHub issue tracker . Suggest a topic","title":"Suggest a topic"},{"location":"tepapa/","text":"Te Papa collections API \u00b6 Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration. Tips, tools, and examples \u00b6 Exploring the Te Papa collection API This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available. Mapping Te Papa's collections This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Te Papa"},{"location":"tepapa/#te-papa-collections-api","text":"Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration.","title":"Te Papa collections API"},{"location":"tepapa/#tips-tools-and-examples","text":"Exploring the Te Papa collection API This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available. Mapping Te Papa's collections This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Tips, tools, and examples"},{"location":"trove-books/","text":"Trove's 'book' zone includes books (of course), but also ephemera (like pamphlets and leaflets) and theses. You can access metadata from the book zone through the Trove API. Tips, tools, and examples \u00b6 Harvesting the text of digitised books (and ephemera) This notebook harvests metadata and OCRd text from digitised works in Trove's book zone. Results of the harvest are available below. Metadata for Trove digitised works In poking around to try and find a way of automating the download of OCR text from Trove's digitised books, I discovered that there's lots of useful metadata embedded in the page of a digitised work. Most of this metadata isn't available through the Trove API. Getting the text of Trove books from the Internet Archive Previously I've harvested the text of books digitised by the National Library of Australia and made available through Trove. But it occured to me it might be possible to get the full text of other books in Trove by making use of the links to the Open Library. Data and text \u00b6 OCRd text from Trove books (and ephemera) (15 April 2019) I've harvested 9,738 text files from digitised books using the notebook above. You can browse the collection in CloudStor , or download the complete set as a zip file (400mb) . CSV formatted list of books with OCRd text (15 April 2019) This file provides metadata of 9,738 works in the Trove book zone that have OCRd text for download. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: children \u2013 pipe-separated ids of any child works contributors \u2013 pipe-separated names of contributors date \u2013 publication date form \u2013 work format fulltext_url \u2013 link to the digitised version pages \u2013 number of pages parent \u2013 id of parent work (if any) text_downloaded \u2013 file name of the downloaded OCR text text_file \u2013 True/False is there any OCRd text title \u2013 title of the work trove_id \u2013 unique identifier url \u2013 link to the metadata record in Trove volume \u2013 volume/part number OCRd text from the Internet Archive of 'Australian' books listed in Trove I've harvested 1,513 text files from the Internet Archive of 'Australian' books listed in Trove using the notebook above. Trove's 'Australian content' filter was used to try to limit the results to books published in, or about, Australia. However, this is not always accurate and some of the harvested works don't seem to have an Australian connection. You can browse the collection in CloudStor . CSV formatted list of 'Australian' books in Trove with full text versions in the Internet Archive (24 May 2019) This file includes metadata of 1,511 'Australian' books listed in Trove that have freely available text versions in the Internet Archive. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: creators \u2013 pipe-separated list of creators date \u2013 publication date ia_formats \u2013 pipe-separated list of file formats available from the Internet Archive (these can be downloaded from the IA) ia_id \u2013 Internet Archive identifier ia_url \u2013 link to more information in the Internet Archive ol_id \u2013 Open Library identifier publisher \u2013 publisher text_filename \u2013 name of the downloaded text file title \u2013 title of the book trove_url \u2013 link to more information in Trove version_id \u2013 Trove version identifier work_id \u2013 Trove work identifier","title":"Trove books"},{"location":"trove-books/#tips-tools-and-examples","text":"Harvesting the text of digitised books (and ephemera) This notebook harvests metadata and OCRd text from digitised works in Trove's book zone. Results of the harvest are available below. Metadata for Trove digitised works In poking around to try and find a way of automating the download of OCR text from Trove's digitised books, I discovered that there's lots of useful metadata embedded in the page of a digitised work. Most of this metadata isn't available through the Trove API. Getting the text of Trove books from the Internet Archive Previously I've harvested the text of books digitised by the National Library of Australia and made available through Trove. But it occured to me it might be possible to get the full text of other books in Trove by making use of the links to the Open Library.","title":"Tips, tools, and examples"},{"location":"trove-books/#data-and-text","text":"OCRd text from Trove books (and ephemera) (15 April 2019) I've harvested 9,738 text files from digitised books using the notebook above. You can browse the collection in CloudStor , or download the complete set as a zip file (400mb) . CSV formatted list of books with OCRd text (15 April 2019) This file provides metadata of 9,738 works in the Trove book zone that have OCRd text for download. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: children \u2013 pipe-separated ids of any child works contributors \u2013 pipe-separated names of contributors date \u2013 publication date form \u2013 work format fulltext_url \u2013 link to the digitised version pages \u2013 number of pages parent \u2013 id of parent work (if any) text_downloaded \u2013 file name of the downloaded OCR text text_file \u2013 True/False is there any OCRd text title \u2013 title of the work trove_id \u2013 unique identifier url \u2013 link to the metadata record in Trove volume \u2013 volume/part number OCRd text from the Internet Archive of 'Australian' books listed in Trove I've harvested 1,513 text files from the Internet Archive of 'Australian' books listed in Trove using the notebook above. Trove's 'Australian content' filter was used to try to limit the results to books published in, or about, Australia. However, this is not always accurate and some of the harvested works don't seem to have an Australian connection. You can browse the collection in CloudStor . CSV formatted list of 'Australian' books in Trove with full text versions in the Internet Archive (24 May 2019) This file includes metadata of 1,511 'Australian' books listed in Trove that have freely available text versions in the Internet Archive. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: creators \u2013 pipe-separated list of creators date \u2013 publication date ia_formats \u2013 pipe-separated list of file formats available from the Internet Archive (these can be downloaded from the IA) ia_id \u2013 Internet Archive identifier ia_url \u2013 link to more information in the Internet Archive ol_id \u2013 Open Library identifier publisher \u2013 publisher text_filename \u2013 name of the downloaded text file title \u2013 title of the book trove_url \u2013 link to more information in Trove version_id \u2013 Trove version identifier work_id \u2013 Trove work identifier","title":"Data and text"},{"location":"trove-harvester/","text":"Download large quantities of digitised newspaper articles from Trove using the Trove Harvester tool. Tools, tips, and examples \u00b6 Using TroveHarvester to get newspaper articles in bulk An easy introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Exploring your TroveHarvester data This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Exploring harvested text files This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction) Useful apps \u00b6 These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Trove Harvester web app A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove.","title":"Trove newspaper harvester"},{"location":"trove-harvester/#tools-tips-and-examples","text":"Using TroveHarvester to get newspaper articles in bulk An easy introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Exploring your TroveHarvester data This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Exploring harvested text files This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction)","title":"Tools, tips, and examples"},{"location":"trove-harvester/#useful-apps","text":"These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Trove Harvester web app A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove.","title":"Useful apps"},{"location":"trove-journals/","text":"Trove's 'journals' zone includes journals and journal articles, as well as other research outputs and things like press releases. You can access metadata from the book zone through the Trove API. Tips, tools, and examples \u00b6 Create a list of Trove's digitised journals Everyone know's about Trove's newspapers, but there is also a growing collection of digitised journals available in the journals zone. They're not easy to find, however, which is why I created the Trove Titles web app. This notebook uses the Trove API to harvest metadata relating to digitised journals \u2013 or more accurately, journals that are freely available online in a digital form. This includes some born digital publications that are available to view in formats like PDF and MOBI, but excludes some digital journals that have access restrictions. Get OCRd text from a digitised journal in Trove Many of the digitised journals available in Trove make OCRd text available for download \u2013 one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them? This notebook shows how to extract issue data from a digitised journal and download OCRd text for each issue. Download the OCRd text for ALL the digitised journals in Trove! Using the code and data from the previous two notebooks, you can download the OCRd text from every digitised journal. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder. Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details. I repeat, you probably don't want to do this yourself. The point of this notebook is really to document the methodology used to create the repository. Harvest parliament press releases from Trove Trove includes more than 370,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them all in Trove by searching for nuc:\"APAR:PR\" in the journals zone. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo. Harvesting data from the Bulletin This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Bulletin . It also shows how you can get the front cover images (or any other page). Finding editorial cartoons in the Bulletin In another notebook I showed how you could download all the front pages of The Bulletin (and other journals) as images. Amongst the front pages you'll find a number of full page editorial cartoons under The Bulletin's masthead. But you'll also find that many of the front pages are advertising wrap arounds. The full page editorial cartoons were a consistent feature of The Bulletin for many decades, but they moved around between pages one and eleven. That makes them hard to find. I wanted to try and assemble a collection of all the editorial cartoons, but how? Harvesting data from Home\u00b6 This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Home . It also shows how you can get the front cover images (or any other page). Topic Modelling of Australian Parliamentary Press Releases by Adel Rahmani This notebook explores the Politicians talking about 'immigrants' and 'refugees' collection of press releases (see below). Adel notes: 'I was curious about the contents of the press releases, however, at more than 12,000 documents the collection is too overwhelming to read through, so I thought I'd get the computer to do it for me, and use topic modelling to poke aroung the corpus.' Data and text \u00b6 CSV formatted list of journals freely available from Trove in digital form (21 April 2019) This file provides metadata of 2,024 journals that are freely available from Trove in a digital form. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove CSV formatted list of journals with OCRd text (21 April 2019) This file provides metadata of 358 digitised journals in Trove that have OCRd text for download. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove issues \u2013 the number of available issues issues_with_text \u2013 the number of issues that OCRd text could be downloaded from directory \u2013 the directory in which the files from this journal have been saved (relative to the output directory) OCRd text from Trove digitised journals (21 April 2019) Using the notebook above I harvested metadata and OCRd text from Trove's digitised journals. 358 journals had OCRd text available for download OCRd text was downloaded from 27,426 journal issues About 6.6gb of text was downloaded The complete collection of text files for all the journals can be browsed and downloaded using this repository on CloudStor . Editorial cartoons from The Bulletin, 1886 to 1952 (9 May 2019) Using the notebook above I downloaded at least one full page editorial cartoon for every issue of The Bulletin from 4 September 1886 to 17 September 1952. In total there are 3,471 images (approximately 60gb). The complete collection can be downloaded from CloudStor . The names of each image file provide useful contextual metadata. For example, the file name 19330412-2774-nla.obj-606969767-7.jpg tells you: 19330412 \u2013 the cartoon was published on 12 April 1933 2774 \u2013 it was published in issue number 2774 nla.obj-606969767 \u2013 the Trove identifier for the issue, can be used to make a url eg https://nla.gov.au/nla.obj-606969767 7 \u2013 on page 7 To make it easier to browse the images, I've compiled them into a series of PDFs \u2013 one PDF for each decade. The PDFs include lower resolution versions of the images together with their publication details and a link to Trove. They're all available from DropBox : 1886 to 1889 (45mb PDF) 1890 to 1899 (139mb PDF) 1900 to 1909 (147mb PDF) 1910 to 1919 (153mb PDF) 1920 to 1929 (159mb PDF) 1930 to 1939 (151mb PDF) 1940 to 1949 (146mb PDF) 1950 to 1952 (42mb PDF) Politicians talking about 'immigrants' and 'refugees' Using the notebook above I harvested parliamentary press releases that included any of the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', or 'boat arrivals'. A total of 12,619 text files were harvested. You can browse the files on CloudStor , or download the complete dataset as a zip file (43mb) .","title":"Trove journals"},{"location":"trove-journals/#tips-tools-and-examples","text":"Create a list of Trove's digitised journals Everyone know's about Trove's newspapers, but there is also a growing collection of digitised journals available in the journals zone. They're not easy to find, however, which is why I created the Trove Titles web app. This notebook uses the Trove API to harvest metadata relating to digitised journals \u2013 or more accurately, journals that are freely available online in a digital form. This includes some born digital publications that are available to view in formats like PDF and MOBI, but excludes some digital journals that have access restrictions. Get OCRd text from a digitised journal in Trove Many of the digitised journals available in Trove make OCRd text available for download \u2013 one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them? This notebook shows how to extract issue data from a digitised journal and download OCRd text for each issue. Download the OCRd text for ALL the digitised journals in Trove! Using the code and data from the previous two notebooks, you can download the OCRd text from every digitised journal. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder. Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details. I repeat, you probably don't want to do this yourself. The point of this notebook is really to document the methodology used to create the repository. Harvest parliament press releases from Trove Trove includes more than 370,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them all in Trove by searching for nuc:\"APAR:PR\" in the journals zone. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo. Harvesting data from the Bulletin This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Bulletin . It also shows how you can get the front cover images (or any other page). Finding editorial cartoons in the Bulletin In another notebook I showed how you could download all the front pages of The Bulletin (and other journals) as images. Amongst the front pages you'll find a number of full page editorial cartoons under The Bulletin's masthead. But you'll also find that many of the front pages are advertising wrap arounds. The full page editorial cartoons were a consistent feature of The Bulletin for many decades, but they moved around between pages one and eleven. That makes them hard to find. I wanted to try and assemble a collection of all the editorial cartoons, but how? Harvesting data from Home\u00b6 This is a more specific example of harvesting metadata and OCRd text from a digitised journal, in this case The Home . It also shows how you can get the front cover images (or any other page). Topic Modelling of Australian Parliamentary Press Releases by Adel Rahmani This notebook explores the Politicians talking about 'immigrants' and 'refugees' collection of press releases (see below). Adel notes: 'I was curious about the contents of the press releases, however, at more than 12,000 documents the collection is too overwhelming to read through, so I thought I'd get the computer to do it for me, and use topic modelling to poke aroung the corpus.'","title":"Tips, tools, and examples"},{"location":"trove-journals/#data-and-text","text":"CSV formatted list of journals freely available from Trove in digital form (21 April 2019) This file provides metadata of 2,024 journals that are freely available from Trove in a digital form. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove CSV formatted list of journals with OCRd text (21 April 2019) This file provides metadata of 358 digitised journals in Trove that have OCRd text for download. You can download the CSV file . This file includes the following columns: fulltext_url \u2013 the url of the landing page of the digital version of the journal title \u2013 the title of the journal trove_id \u2013 the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal trove_url \u2013 url of the journal's metadata record in Trove issues \u2013 the number of available issues issues_with_text \u2013 the number of issues that OCRd text could be downloaded from directory \u2013 the directory in which the files from this journal have been saved (relative to the output directory) OCRd text from Trove digitised journals (21 April 2019) Using the notebook above I harvested metadata and OCRd text from Trove's digitised journals. 358 journals had OCRd text available for download OCRd text was downloaded from 27,426 journal issues About 6.6gb of text was downloaded The complete collection of text files for all the journals can be browsed and downloaded using this repository on CloudStor . Editorial cartoons from The Bulletin, 1886 to 1952 (9 May 2019) Using the notebook above I downloaded at least one full page editorial cartoon for every issue of The Bulletin from 4 September 1886 to 17 September 1952. In total there are 3,471 images (approximately 60gb). The complete collection can be downloaded from CloudStor . The names of each image file provide useful contextual metadata. For example, the file name 19330412-2774-nla.obj-606969767-7.jpg tells you: 19330412 \u2013 the cartoon was published on 12 April 1933 2774 \u2013 it was published in issue number 2774 nla.obj-606969767 \u2013 the Trove identifier for the issue, can be used to make a url eg https://nla.gov.au/nla.obj-606969767 7 \u2013 on page 7 To make it easier to browse the images, I've compiled them into a series of PDFs \u2013 one PDF for each decade. The PDFs include lower resolution versions of the images together with their publication details and a link to Trove. They're all available from DropBox : 1886 to 1889 (45mb PDF) 1890 to 1899 (139mb PDF) 1900 to 1909 (147mb PDF) 1910 to 1919 (153mb PDF) 1920 to 1929 (159mb PDF) 1930 to 1939 (151mb PDF) 1940 to 1949 (146mb PDF) 1950 to 1952 (42mb PDF) Politicians talking about 'immigrants' and 'refugees' Using the notebook above I harvested parliamentary press releases that included any of the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', or 'boat arrivals'. A total of 12,619 text files were harvested. You can browse the files on CloudStor , or download the complete dataset as a zip file (43mb) .","title":"Data and text"},{"location":"trove-lists/","text":"Trove lists are user created collections of items. The details of public lists are available through the Trove API. Tips, tools, and examples \u00b6 Harvest summary data from Trove lists Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Convert a Trove list into a CSV file Use the Trove API to save the contents of a public list to a CSV file.","title":"Trove lists"},{"location":"trove-lists/#tips-tools-and-examples","text":"Harvest summary data from Trove lists Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Convert a Trove list into a CSV file Use the Trove API to save the contents of a public list to a CSV file.","title":"Tips, tools, and examples"},{"location":"trove-maps/","text":"The Trove 'map' zone includes single maps, as well as map series, atlases, and aerial photographs. You can access metadata from the map zone through the Trove API. Tips, tools, and examples \u00b6 Exploring digitised maps in Trove If you've ever poked around in Trove's 'map' zone, you might have noticed the beautiful deep-zoomable images available for many of the NLA's digitised maps. Even better, in many cases the high-resolution TIFF versions of the digitised maps are available for download. I knew there were lots of great maps you could download from Trove, but how many? And how big were the files? I thought I'd try to quantify this a bit by harvesting and analysing the metadata. Data \u00b6 CSV formatted list of maps with high-resolution downloads (26 April 2019) This file provides metadata of 20,158 maps in Trove that have high-resolution downloads. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: copyright_status - a string indicating the copyright status of the map creators \u2013 pipe-separated list of contributors/creators date \u2013 date of the map (format varies) filesize \u2013 size in bytes filesize_string \u2013 size as a human-readable string with unit fulltext_url \u2013 link to the landing page of the digital version height \u2013 height in pixels title \u2013 title of the map trove_id \u2013 digital object identifier trove_url \u2013 link to the metadata record in Trove width \u2013 width in pixels","title":"Trove maps"},{"location":"trove-maps/#tips-tools-and-examples","text":"Exploring digitised maps in Trove If you've ever poked around in Trove's 'map' zone, you might have noticed the beautiful deep-zoomable images available for many of the NLA's digitised maps. Even better, in many cases the high-resolution TIFF versions of the digitised maps are available for download. I knew there were lots of great maps you could download from Trove, but how many? And how big were the files? I thought I'd try to quantify this a bit by harvesting and analysing the metadata.","title":"Tips, tools, and examples"},{"location":"trove-maps/#data","text":"CSV formatted list of maps with high-resolution downloads (26 April 2019) This file provides metadata of 20,158 maps in Trove that have high-resolution downloads. You can download the CSV file , or browse the metadata using Google Sheets . This file includes the following columns: copyright_status - a string indicating the copyright status of the map creators \u2013 pipe-separated list of contributors/creators date \u2013 date of the map (format varies) filesize \u2013 size in bytes filesize_string \u2013 size as a human-readable string with unit fulltext_url \u2013 link to the landing page of the digital version height \u2013 height in pixels title \u2013 title of the map trove_id \u2013 digital object identifier trove_url \u2013 link to the metadata record in Trove width \u2013 width in pixels","title":"Data"},{"location":"trove-newspapers/","text":"Assorted experiments and examples working with Trove\u2019s digitised newspapers Tips, tools, and examples \u00b6 Map Trove newspaper results by state Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Map Trove newspaper results by place of publication Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Map Trove newspaper results by place of publication over time Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Today\u2019s news yesterday Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Create a Trove OCR corrections ticker Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds. Useful apps \u00b6 These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Download a page image The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. Generate an article thumbnail Generate a nice square thumbnail image for a newspaper article. QueryPic Deconstructed QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic.","title":"Trove newspapers"},{"location":"trove-newspapers/#tips-tools-and-examples","text":"Map Trove newspaper results by state Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Map Trove newspaper results by place of publication Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Map Trove newspaper results by place of publication over time Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Today\u2019s news yesterday Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Create a Trove OCR corrections ticker Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds.","title":"Tips, tools, and examples"},{"location":"trove-newspapers/#useful-apps","text":"These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Download a page image The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. Generate an article thumbnail Generate a nice square thumbnail image for a newspaper article. QueryPic Deconstructed QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic.","title":"Useful apps"},{"location":"trove-unpublished/","text":"Trove unpublished works \u00b6 Experiments and examples relating to Trove's 'Diaries, letters, and archives' zone. Tips, tools, and examples \u00b6 Finding unpublished works that might be entering the public domain on 1 January 2019 Changes to Australian copyright legislation mean that many unpublished resources will be entering the public domain on 1 January 2019. This notebook attempts to harvest the details of some of these resources from Trove. Exploring unpublished works that might be entering the public domain on 1 January 2019 Some ways of exploring the data harvested above. Results \u00b6 Unpublished works that might be entering the public domain on 1 January 2019 \u2013 Download CSV file (1.8mb) , view on Google Sheets","title":"Trove unpublished"},{"location":"trove-unpublished/#trove-unpublished-works","text":"Experiments and examples relating to Trove's 'Diaries, letters, and archives' zone.","title":"Trove unpublished works"},{"location":"trove-unpublished/#tips-tools-and-examples","text":"Finding unpublished works that might be entering the public domain on 1 January 2019 Changes to Australian copyright legislation mean that many unpublished resources will be entering the public domain on 1 January 2019. This notebook attempts to harvest the details of some of these resources from Trove. Exploring unpublished works that might be entering the public domain on 1 January 2019 Some ways of exploring the data harvested above.","title":"Tips, tools, and examples"},{"location":"trove-unpublished/#results","text":"Unpublished works that might be entering the public domain on 1 January 2019 \u2013 Download CSV file (1.8mb) , view on Google Sheets","title":"Results"},{"location":"trove/","text":"Trove provides access to much of it's data through an API (Application Programming Interface). The notebooks in this section provide many examples of using the API to harvest data and analyse the contents of Trove. Before you can use the API you need to obtain a key \u2014 it's free and quick. Just follow these instructions . What's an API? \u00b6 An API is an Application Programming Interface. It's a set of predefined requests and responses that enables computer programs talk to each other. Web APIs are generally used to deliver data. While humans can easily interpret information on a web page, computers need more help. APIs provide data in a form that computers can understand and use (we call this machine-readable data). The Trove API works much like the Trove website. You make queries and you get back results. But instead of going through a nicely-designed web interface, requests to the API are just URLs, and the results are just structured data. While you can just type an API request into the location box of your web browser, most of the time requests and responses will be handled by a computer script or program. APIs don't care what programming language you use as long as you structure requests in the way they expect. In these notebooks we'll be using the programming language Python. No prior knowledge of Python is expected or required -- just follow along! The examples and approaches used could be easily translated into any another programming language. Tools, tips, examples \u00b6 Your first API request In this notebook we're going to learn how to send a request for information to the Trove API. API requests are just like normal urls. However, instead of sending us back a web page, they deliver data in a form that computers can understand. We can then use that data in our own programs. Working with zones Trove's zones are important in constructing API requests and interpreting the results. So let's explore them a bit. Exploring facets Facets aggregate collection data in interesting and useful ways, allowing us to build pictures of the collection. This notebook shows you how to get facet data from Trove.) Useful links \u00b6 Trove API Documentation Trove API console Introduction to using APIs","title":"Trove API introduction"},{"location":"trove/#whats-an-api","text":"An API is an Application Programming Interface. It's a set of predefined requests and responses that enables computer programs talk to each other. Web APIs are generally used to deliver data. While humans can easily interpret information on a web page, computers need more help. APIs provide data in a form that computers can understand and use (we call this machine-readable data). The Trove API works much like the Trove website. You make queries and you get back results. But instead of going through a nicely-designed web interface, requests to the API are just URLs, and the results are just structured data. While you can just type an API request into the location box of your web browser, most of the time requests and responses will be handled by a computer script or program. APIs don't care what programming language you use as long as you structure requests in the way they expect. In these notebooks we'll be using the programming language Python. No prior knowledge of Python is expected or required -- just follow along! The examples and approaches used could be easily translated into any another programming language.","title":"What's an API?"},{"location":"trove/#tools-tips-examples","text":"Your first API request In this notebook we're going to learn how to send a request for information to the Trove API. API requests are just like normal urls. However, instead of sending us back a web page, they deliver data in a form that computers can understand. We can then use that data in our own programs. Working with zones Trove's zones are important in constructing API requests and interpreting the results. So let's explore them a bit. Exploring facets Facets aggregate collection data in interesting and useful ways, allowing us to build pictures of the collection. This notebook shows you how to get facet data from Trove.)","title":"Tools, tips, examples"},{"location":"trove/#useful-links","text":"Trove API Documentation Trove API console Introduction to using APIs","title":"Useful links"}]}